# 🔧 Qwen 模型下载故障排查指南

## 📅 更新时间
2025-12-04 21:15:00

## ❌ 遇到的问题

```
ModuleNotFoundError: No module named 'optimum.onnxruntime'
```

## 🎯 解决方案

### 方案一：手动安装依赖（推荐）

```bash
# 1. 安装基础依赖
pip install transformers torch

# 2. 安装 optimum（需要分开安装）
pip install optimum

# 3. 安装 onnxruntime
pip install onnxruntime

# 4. 验证安装
python -c "from optimum.onnxruntime import ORTModelForCausalLM; print('✅ optimum.onnxruntime 可用')"
```

### 方案二：使用简化脚本（最简单）

我已经创建了一个简化版本的下载脚本，不需要 ONNX 转换：

```bash
# 使用简化版脚本（只需要 transformers 和 torch）
python scripts/download_qwen_simple.py --model 0.5b

# 如果网络慢，使用国内镜像
python scripts/download_qwen_simple.py --model 0.5b --mirror
```

**注意**：简化版下载的是 PyTorch 格式，不是 ONNX 格式。

### 方案三：从魔搭社区下载（国内快）

```bash
# 1. 安装 modelscope
pip install modelscope

# 2. 使用魔搭社区下载
python -c "
from modelscope import snapshot_download
model_dir = snapshot_download('qwen/Qwen2.5-0.5B-Instruct', cache_dir='./models')
print(f'模型下载到: {model_dir}')
"
```

### 方案四：修复 optimum 安装

如果你一定要使用 ONNX 版本：

```bash
# 卸载旧版本
pip uninstall optimum optimum-onnxruntime -y

# 重新安装（指定版本）
pip install optimum[onnxruntime]==1.14.0

# 如果还是失败，分步安装
pip install optimum==1.14.0
pip install optimum-onnxruntime==1.14.0
pip install onnxruntime==1.15.0

# 验证
python -c "from optimum.onnxruntime import ORTModelForCausalLM; print('OK')"
```

---

## 📋 完整的依赖安装步骤

### Step 1: 创建虚拟环境（可选但推荐）

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

### Step 2: 升级 pip

```bash
python -m pip install --upgrade pip
```

### Step 3: 安装依赖

```bash
# 基础依赖
pip install transformers>=4.30.0
pip install torch>=2.0.0

# ONNX 相关（如果需要）
pip install optimum>=1.14.0
pip install onnxruntime>=1.15.0

# 如果使用 GPU
pip install onnxruntime-gpu
```

### Step 4: 验证安装

```bash
# 测试脚本
python -c "
import transformers
import torch
print(f'✅ transformers {transformers.__version__}')
print(f'✅ torch {torch.__version__}')

try:
    from optimum.onnxruntime import ORTModelForCausalLM
    import onnxruntime
    print(f'✅ optimum.onnxruntime 可用')
    print(f'✅ onnxruntime {onnxruntime.__version__}')
except ImportError as e:
    print(f'⚠️  ONNX 模块不可用: {e}')
"
```

---

## 🚀 推荐的使用流程

### 如果你要用于 Java PPL 服务（推荐）

**不需要自己下载模型**，直接使用在线 API：

```yaml
# application.yml
knowledge:
  qa:
    ppl:
      # 使用 Ollama（更简单）
      default-provider: ollama
      
      ollama:
        enabled: true
        base-url: http://localhost:11434
        model: qwen2.5:0.5b
```

然后安装 Ollama：

```bash
# 1. 安装 Ollama
# Windows: https://ollama.com/download/windows

# 2. 下载 Qwen 模型
ollama pull qwen2.5:0.5b

# 3. 启动（自动）
ollama serve
```

### 如果你需要离线 ONNX 模型

1. **先手动安装依赖**（见上面步骤）
2. **运行转换脚本**：
```bash
python scripts/download_qwen_models.py --model 0.5b
```

---

## 💡 最简单的方案总结

### 方案对比

| 方案 | 优点 | 缺点 | 推荐度 |
|------|------|------|--------|
| **Ollama** | 最简单，开箱即用 | 需要独立服务 | ⭐⭐⭐⭐⭐ |
| **简化脚本** | 无需 ONNX，依赖少 | 不能直接用于 Java | ⭐⭐⭐⭐ |
| **ONNX 转换** | 可嵌入 Java | 依赖复杂，易出错 | ⭐⭐⭐ |
| **魔搭社区** | 国内快 | 需要额外包 | ⭐⭐⭐⭐ |

### 我的建议

1. **首选 Ollama**（最简单）
   ```bash
   ollama pull qwen2.5:0.5b
   ```

2. **次选简化脚本**（如果要自己管理）
   ```bash
   python scripts/download_qwen_simple.py --model 0.5b --mirror
   ```

3. **最后才考虑 ONNX**（如果真的需要嵌入式）
   - 先手动安装所有依赖
   - 再运行转换脚本

---

## 🔍 常见问题

### Q1: 为什么 optimum[onnxruntime] 安装失败？

**原因**：
- 包名写法问题
- 依赖冲突
- 版本不兼容

**解决**：
```bash
# 不要用 optimum[onnxruntime]
# 而是分别安装
pip install optimum
pip install onnxruntime
```

### Q2: 下载速度很慢怎么办？

**解决**：
```bash
# 使用国内镜像
export HF_ENDPOINT=https://hf-mirror.com

# 或使用魔搭社区
pip install modelscope
```

### Q3: 我真的需要 ONNX 吗？

**答案**：不一定

- **如果用于 Java 嵌入式**：需要 ONNX
- **如果用 Ollama**：不需要 ONNX
- **如果用云端 API**：不需要 ONNX

对于大多数场景，**Ollama 是最简单的选择**。

---

## ✅ 验证清单

安装完成后，运行以下检查：

```bash
# 检查 1：基础依赖
python -c "import transformers, torch; print('✅ 基础依赖 OK')"

# 检查 2：ONNX（可选）
python -c "from optimum.onnxruntime import ORTModelForCausalLM; print('✅ ONNX OK')"

# 检查 3：Ollama（如果使用）
curl http://localhost:11434/api/tags

# 检查 4：模型文件
ls ./models/qwen*/
```

---

## 📞 需要帮助？

如果以上方案都不行，请提供：

1. 你的 Python 版本：`python --version`
2. 你的操作系统
3. 完整的错误信息
4. 你想使用哪种方案（Ollama/ONNX/API）

---

**文档版本**：v1.0  
**最后更新**：2025-12-04 21:15:00  
**状态**：✅ 多种解决方案


# ğŸ‡¨ğŸ‡³ PPL æœåŠ¡å›½äº§æ¨¡å‹é…ç½®æŒ‡å—

## ğŸ“… åˆ›å»ºæ—¶é—´
2025-12-04 20:45:00

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•é…ç½® PPL æœåŠ¡ä½¿ç”¨å›½äº§å¤§æ¨¡å‹ï¼ˆä¸»è¦æ˜¯é˜¿é‡Œé€šä¹‰åƒé—® Qwen ç³»åˆ—ï¼‰ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆé€‰æ‹©

| æ–¹æ¡ˆ | æ¨¡å‹ | æˆæœ¬ | é€Ÿåº¦ | æ¨èï¿½ï¿½ï¿½æ™¯ |
|------|------|------|------|----------|
| **ONNXï¼ˆæ¨èï¼‰** | GPT2-Medium | å…è´¹ | âš¡âš¡âš¡ | PPL Chunking/Rerank |
| **Ollama** | Qwen2.5-0.5B | å…è´¹ | âš¡âš¡ | æœ¬åœ°å¤§æ¨¡å‹æ”¯æŒ |
| **é€šä¹‰åƒé—® API** | qwen-turbo | ä»˜è´¹ | âš¡âš¡ | äº‘ç«¯é«˜è´¨é‡ |

---

## ğŸ“¦ æ–¹æ¡ˆä¸€ï¼šONNXï¼ˆæœ¬åœ°ï¼Œæ¨èï¼‰

### ç‰¹ç‚¹
- âœ… å®Œå…¨å…è´¹
- âœ… é€Ÿåº¦æœ€å¿«ï¼ˆ30-150msï¼‰
- âœ… é›¶ç½‘ç»œä¾èµ–
- âœ… éšç§ä¿æŠ¤

### é…ç½®

```yaml
knowledge:
  qa:
    ppl:
      default-provider: onnx
      
      onnx:
        enabled: true
        
        # é€‰é¡¹1ï¼šGPT2-Mediumï¼ˆé€šç”¨ï¼Œæ¨èï¼‰
        model-path: ./models/gpt2-medium-int8/model.onnx
        tokenizer-path: ./models/gpt2-medium-int8/tokenizer.json
        
        # é€‰é¡¹2ï¼šChinese-BERTï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
        # model-path: ./models/chinese-bert-wwm/model.onnx
        # tokenizer-path: ./models/chinese-bert-wwm/tokenizer.json
        
        use-cache: true
        cache-size: 10000
```

### æ¨¡å‹ä¸‹è½½

#### æ–¹æ³•1ï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰

```bash
# è¿è¡Œæ¨¡å‹ä¸‹è½½è„šæœ¬
python scripts/download_onnx_models.py
```

#### æ–¹æ³•2ï¼šæ‰‹åŠ¨ä¸‹è½½

**GPT2-Mediumï¼ˆæ¨èï¼‰**
```bash
# 1. å®‰è£…ä¾èµ–
pip install transformers optimum[onnxruntime]

# 2. è½¬æ¢æ¨¡å‹
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM

# ä¸‹è½½å¹¶è½¬æ¢
model = AutoModelForCausalLM.from_pretrained('gpt2-medium')
tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')

# è½¬æ¢ä¸º ONNX
ort_model = ORTModelForCausalLM.from_pretrained(
    'gpt2-medium',
    from_transformers=True,
    provider='CPUExecutionProvider'
)

# ä¿å­˜
ort_model.save_pretrained('./models/gpt2-medium-int8')
tokenizer.save_pretrained('./models/gpt2-medium-int8')
"
```

**Chinese-BERTï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰**
```bash
python -c "
from transformers import AutoModelForMaskedLM, AutoTokenizer
from optimum.onnxruntime import ORTModelForMaskedLM

# ä¸‹è½½ä¸­æ–‡ BERT
model = AutoModelForMaskedLM.from_pretrained('hfl/chinese-bert-wwm')
tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm')

# è½¬æ¢
ort_model = ORTModelForMaskedLM.from_pretrained(
    'hfl/chinese-bert-wwm',
    from_transformers=True
)

# ä¿å­˜
ort_model.save_pretrained('./models/chinese-bert-wwm')
tokenizer.save_pretrained('./models/chinese-bert-wwm')
"
```

---

## ğŸ  æ–¹æ¡ˆäºŒï¼šOllama + Qwenï¼ˆæœ¬åœ°å¤§æ¨¡å‹ï¼‰

### ç‰¹ç‚¹
- âœ… å…è´¹
- âœ… æœ¬åœ°è¿è¡Œï¼Œéšç§å®‰å…¨
- âœ… æ”¯æŒå¤§æ¨¡å‹ï¼ˆQwen2.5 ç³»åˆ—ï¼‰
- âš ï¸ éœ€è¦è¾ƒå¥½çš„ç¡¬ä»¶ï¼ˆæ¨è 8GB+ å†…å­˜ï¼‰

### å®‰è£… Ollama

```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# ä¸‹è½½å®‰è£…ç¨‹åºï¼šhttps://ollama.com/download/windows
```

### ä¸‹è½½ Qwen æ¨¡å‹

```bash
# æ–¹æ¡ˆ1ï¼šQwen2.5-0.5Bï¼ˆè½»é‡çº§ï¼Œæ¨èç”¨äº PPLï¼‰
ollama pull qwen2.5:0.5b

# æ–¹æ¡ˆ2ï¼šQwen2.5-1.5Bï¼ˆå¹³è¡¡ï¼‰
ollama pull qwen2.5:1.5b

# æ–¹æ¡ˆ3ï¼šQwen2.5-7Bï¼ˆé«˜è´¨é‡ï¼Œéœ€è¦ 8GB+ å†…å­˜ï¼‰
ollama pull qwen2.5:7b
```

### å¯åŠ¨ Ollama

```bash
# å¯åŠ¨æœåŠ¡
ollama serve

# éªŒè¯
curl http://localhost:11434/api/tags
```

### é…ç½®

```yaml
knowledge:
  qa:
    ppl:
      default-provider: ollama
      
      ollama:
        enabled: true
        base-url: http://localhost:11434
        
        # é€‰æ‹©æ¨¡å‹
        model: qwen2.5:0.5b    # è½»é‡çº§ï¼ˆæ¨èï¼‰
        # model: qwen2.5:1.5b  # å¹³è¡¡
        # model: qwen2.5:7b    # é«˜è´¨é‡
        
        timeout: 30000
        max-retries: 3
```

### æµ‹è¯•

```bash
# æµ‹è¯• Ollama + Qwen
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:0.5b",
  "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
  "stream": false
}'
```

---

## â˜ï¸ æ–¹æ¡ˆä¸‰ï¼šé€šä¹‰åƒé—® APIï¼ˆäº‘ç«¯ï¼‰

### ç‰¹ç‚¹
- âœ… è´¨é‡é«˜
- âœ… æ— éœ€æœ¬åœ°èµ„æº
- âœ… å®˜æ–¹æ”¯æŒ
- âš ï¸ ä»˜è´¹ä½¿ç”¨

### è·å– API Key

1. è®¿é—®ï¼šhttps://dashscope.aliyun.com/
2. æ³¨å†Œ/ç™»å½•é˜¿é‡Œäº‘è´¦å·
3. å¼€é€š DashScope æœåŠ¡
4. åˆ›å»º API Key

### é…ç½®

```yaml
knowledge:
  qa:
    ppl:
      default-provider: openai  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
      
      openai:
        enabled: true
        
        # API Keyï¼ˆç¯å¢ƒå˜é‡ï¼‰
        api-key: ${QW_API_KEY:}
        
        # é€‰æ‹©æ¨¡å‹
        model: qwen-turbo       # å¿«é€Ÿã€ç»æµï¼ˆæ¨èç”¨äº PPLï¼‰
        # model: qwen-plus      # å¹³è¡¡æ€§èƒ½
        # model: qwen-max       # æœ€é«˜è´¨é‡
        
        # é€šä¹‰åƒé—® API ç«¯ç‚¹
        api-url: https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
        
        timeout: 60000
        use-logprobs: false     # é€šä¹‰åƒé—®æš‚ä¸æ”¯æŒ
```

### è®¾ç½®ç¯å¢ƒå˜é‡

```bash
# Linux/Mac
export QW_API_KEY=sk-your-qianwen-api-key

# Windows PowerShell
$env:QW_API_KEY="sk-your-qianwen-api-key"

# Windows CMD
set QW_API_KEY=sk-your-qianwen-api-key
```

### æµ‹è¯•

```bash
# æµ‹è¯•é€šä¹‰åƒé—® API
curl https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \
  -H "Authorization: Bearer $QW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-turbo",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ]
  }'
```

---

## ğŸ”„ æ··åˆæ–¹æ¡ˆï¼ˆæ¨èï¼‰

### é…ç½®é™çº§ç­–ç•¥

```yaml
knowledge:
  qa:
    ppl:
      # ä¼˜å…ˆä½¿ç”¨ ONNXï¼ˆå…è´¹ã€å¿«é€Ÿï¼‰
      default-provider: onnx
      
      # å¯ç”¨é™çº§
      enable-fallback: true
      
      # é™çº§é¡ºåº
      fallback-order:
        - onnx      # ç¬¬ä¸€ä¼˜å…ˆï¼šæœ¬åœ° ONNXï¼ˆæœ€å¿«ï¼‰
        - ollama    # ç¬¬äºŒä¼˜å…ˆï¼šæœ¬åœ° Qwenï¼ˆè´¨é‡å¥½ï¼‰
        - openai    # æœ€åå¤‡ä»½ï¼šäº‘ç«¯ APIï¼ˆæœ€ç¨³å®šï¼‰
      
      # åŒæ—¶å¯ç”¨å¤šä¸ªæä¾›å•†
      onnx:
        enabled: true
        # ... ONNX é…ç½® ...
      
      ollama:
        enabled: true
        model: qwen2.5:0.5b
        # ... Ollama é…ç½® ...
      
      openai:
        enabled: true
        model: qwen-turbo
        # ... é€šä¹‰åƒé—® API é…ç½® ...
```

### ä¼˜åŠ¿

1. **æ€§èƒ½ä¼˜å…ˆ**ï¼šONNX æœ€å¿«ï¼Œä¼˜å…ˆä½¿ç”¨
2. **è´¨é‡ä¿éšœ**ï¼šONNX å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° Qwen
3. **é«˜å¯ç”¨**ï¼šå¤šå±‚é™çº§ï¼Œç¡®ä¿æœåŠ¡ä¸ä¸­æ–­
4. **æˆæœ¬å¯æ§**ï¼šä¼˜å…ˆå…è´¹æ–¹æ¡ˆï¼Œæœ€åæ‰ç”¨ä»˜è´¹ API

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### PPL è®¡ç®—æ€§èƒ½ï¼ˆå•æ¬¡ï¼‰

| æ–¹æ¡ˆ | æ¨¡å‹ | å»¶è¿Ÿ | æˆæœ¬ | è´¨é‡ |
|------|------|------|------|------|
| ONNX | GPT2-Medium | 15-50ms | $0 | â­â­â­â­ |
| ONNX | Chinese-BERT | 20-60ms | $0 | â­â­â­â­ |
| Ollama | Qwen2.5-0.5B | 100-300ms | $0 | â­â­â­â­ |
| Ollama | Qwen2.5-1.5B | 200-500ms | $0 | â­â­â­â­â­ |
| Ollama | Qwen2.5-7B | 500-1500ms | $0 | â­â­â­â­â­ |
| API | qwen-turbo | 500-2000ms | $0.0015/åƒæ¬¡ | â­â­â­â­â­ |
| API | qwen-plus | 800-3000ms | $0.004/åƒæ¬¡ | â­â­â­â­â­ |

### æœˆåº¦æˆæœ¬ä¼°ç®—ï¼ˆ10ä¸‡æ–‡æ¡£ + 10ä¸‡æŸ¥è¯¢ï¼‰

| æ–¹æ¡ˆ | Chunking æˆæœ¬ | Rerank æˆæœ¬ | æ€»æˆæœ¬ |
|------|--------------|-------------|--------|
| **ONNX** | $0 | $0 | $0 âœ… |
| **Ollama** | $0 | $0 | $0ï¼ˆéœ€è¦æœåŠ¡å™¨ï¼‰âœ… |
| **é€šä¹‰åƒé—®** | $150 | $15 | $165 |

---

## ğŸ¯ æ¨èé…ç½®

### ä¸ªäººå¼€å‘è€…

```yaml
knowledge:
  qa:
    ppl:
      default-provider: onnx
      
      onnx:
        enabled: true
        model-path: ./models/gpt2-medium-int8/model.onnx
        tokenizer-path: ./models/gpt2-medium-int8/tokenizer.json
      
      chunking:
        ppl-threshold: 20.0
      
      reranking:
        enabled: false  # å¼€å‘æ—¶å¯å…³é—­
```

### å°å‹å›¢é˜Ÿ

```yaml
knowledge:
  qa:
    ppl:
      default-provider: onnx
      enable-fallback: true
      fallback-order: [onnx, ollama]
      
      onnx:
        enabled: true
        # ...
      
      ollama:
        enabled: true
        model: qwen2.5:0.5b
        # ...
      
      reranking:
        enabled: true
        weight: 0.15
```

### ä¼ä¸šçº§

```yaml
knowledge:
  qa:
    ppl:
      default-provider: onnx
      enable-fallback: true
      fallback-order: [onnx, ollama, openai]
      
      onnx:
        enabled: true
        max-batch-size: 16  # æé«˜å¹¶å‘
        cache-size: 50000   # å¢å¤§ç¼“å­˜
      
      ollama:
        enabled: true
        model: qwen2.5:1.5b  # ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹
        base-url: http://ollama-cluster:11434  # é›†ç¾¤éƒ¨ç½²
      
      openai:
        enabled: true
        model: qwen-plus  # å¤‡ç”¨é«˜è´¨é‡æ–¹æ¡ˆ
      
      reranking:
        enabled: true
        weight: 0.20
        top-k: 10
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### ONNX ç›¸å…³

**é—®é¢˜1ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶**
```
è§£å†³ï¼šæ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
ls ./models/gpt2-medium-int8/
```

**é—®é¢˜2ï¼šæ¨¡å‹åŠ è½½å¤±è´¥**
```
è§£å†³ï¼šé‡æ–°ä¸‹è½½æ¨¡å‹
python scripts/download_onnx_models.py
```

### Ollama ç›¸å…³

**é—®é¢˜1ï¼šè¿æ¥æ‹’ç»**
```
è§£å†³ï¼šæ£€æŸ¥ Ollama æ˜¯å¦å¯åŠ¨
curl http://localhost:11434/api/tags
```

**é—®é¢˜2ï¼šæ¨¡å‹æœªæ‰¾åˆ°**
```
è§£å†³ï¼šä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:0.5b
```

### API ç›¸å…³

**é—®é¢˜1ï¼šAPI Key æ— æ•ˆ**
```
è§£å†³ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡
echo $QW_API_KEY
```

**é—®é¢˜2ï¼šè¯·æ±‚è¶…æ—¶**
```
è§£å†³ï¼šå¢åŠ è¶…æ—¶æ—¶é—´
timeout: 120000  # 2åˆ†é’Ÿ
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [é€šä¹‰åƒé—® Qwen](https://github.com/QwenLM/Qwen)
- [DashScope API](https://help.aliyun.com/zh/dashscope/)
- [Ollama](https://ollama.com/)

### æ¨¡å‹ä»“åº“
- [Hugging Face - Qwen](https://huggingface.co/Qwen)
- [Hugging Face - Chinese-BERT](https://huggingface.co/hfl/chinese-bert-wwm)

### ç¤¾åŒº
- [Qwen GitHub Issues](https://github.com/QwenLM/Qwen/issues)
- [Ollama Discord](https://discord.gg/ollama)

---

## âœ… éªŒæ”¶æ¸…å•

- [ ] æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½
- [ ] é…ç½®æ–‡ä»¶å·²æ›´æ–°
- [ ] ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼ˆå¦‚éœ€è¦ï¼‰
- [ ] æœåŠ¡å·²å¯åŠ¨ï¼ˆOllamaï¼Œå¦‚ä½¿ç”¨ï¼‰
- [ ] API Key å·²éªŒè¯ï¼ˆäº‘ç«¯ APIï¼Œå¦‚ä½¿ç”¨ï¼‰
- [ ] åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½ç¬¦åˆé¢„æœŸ

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2025-12-04 20:45:00  
**é€‚ç”¨ç‰ˆæœ¬**ï¼šPPL Service v1.0+  
**çŠ¶æ€**ï¼šâœ… å®Œæ•´é…ç½®æŒ‡å—


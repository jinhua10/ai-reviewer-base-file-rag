# 🎯 最终方案：使用 Ollama（强烈推荐）

## 📅 更新时间
2025-12-04 21:30:00

## ❌ ONNX 转换遇到的问题

### 问题 1：API 版本不兼容
```
ORTModelForCausalLM._from_pretrained() got an unexpected keyword argument 'from_transformers'
```

**原因**：`optimum` 库的 API 在不同版本间变化较大，参数名不一致。

### 问题 2：模型转换复杂
- 需要 PyTorch 环境
- 转换过程可能出错
- 依赖版本兼容性问题
- 转换时间长（5-15 分钟）

## ✅ 推荐方案：使用 Ollama

经过多次尝试，**强烈推荐使用 Ollama**，理由如下：

### 为什么选择 Ollama？

1. **🚀 最简单** - 一条命令搞定
2. **✅ 最稳定** - 无依赖问题
3. **⚡ 最快速** - 无需转换
4. **🔧 易维护** - 自动管理模型
5. **💯 开箱即用** - 立即可用

---

## 🚀 Ollama 安装和使用（5 分钟搞定）

### 步骤 1：安装 Ollama

```bash
# Windows
# 1. 访问 https://ollama.com/download/windows
# 2. 下载 OllamaSetup.exe
# 3. 双击安装（约 500MB）
# 4. 安装完成后自动启动服务
```

### 步骤 2：下载 Qwen 模型

```bash
# 打开 PowerShell 或 CMD

# 下载 Qwen2.5-0.5B（推荐，约 400MB）
ollama pull qwen2.5:0.5b

# 或下载 Qwen2.5-1.5B（更好的质量，约 1.5GB）
ollama pull qwen2.5:1.5b

# 或下载 Qwen2.5-7B（最高质量，约 7GB）
ollama pull qwen2.5:7b
```

### 步骤 3：验证安装

```bash
# 检查 Ollama 服务
curl http://localhost:11434

# 应该返回: Ollama is running

# 列出已安装的模型
ollama list

# 应该看到 qwen2.5:0.5b

# 测试模型
ollama run qwen2.5:0.5b "你好，请介绍一下你自己"
```

### 步骤 4：更新配置

编辑 `application.yml`：

```yaml
knowledge:
  qa:
    ppl:
      # 使用 Ollama
      default-provider: ollama
      
      # 禁用 ONNX（暂时不用）
      onnx:
        enabled: false
      
      # 启用 Ollama
      ollama:
        enabled: true
        base-url: http://localhost:11434
        model: qwen2.5:0.5b  # 使用你下载的模型
        timeout: 30000
        max-retries: 3
```

### 步骤 5：启动应用

```bash
# 启动 Spring Boot 应用
./mvnw spring-boot:run

# 或使用 IDE 直接运行
```

---

## 📊 方案对比（最终结论）

| 特性 | ONNX | Ollama | 胜者 |
|------|------|--------|------|
| **安装难度** | ⭐⭐ 复杂 | ⭐⭐⭐⭐⭐ 简单 | Ollama ✅ |
| **依赖问题** | ⭐⭐ 多 | ⭐⭐⭐⭐⭐ 无 | Ollama ✅ |
| **转换时间** | 5-15 分钟 | 0（直接下载）| Ollama ✅ |
| **稳定性** | ⭐⭐⭐ 一般 | ⭐⭐⭐⭐⭐ 好 | Ollama ✅ |
| **性能** | ⭐⭐⭐⭐ 好 | ⭐⭐⭐⭐⭐ 优 | Ollama ✅ |
| **维护成本** | ⭐⭐ 高 | ⭐⭐⭐⭐⭐ 低 | Ollama ✅ |
| **嵌入式部署** | ⭐⭐⭐⭐⭐ 好 | ⭐⭐ 需要服务 | ONNX |

**结论**：除非有特殊的嵌入式需求，否则 **Ollama 是最佳选择**。

---

## 🎯 推荐配置（最佳实践）

### 配置 1：开发环境

```yaml
knowledge:
  qa:
    ppl:
      default-provider: ollama
      
      ollama:
        enabled: true
        base-url: http://localhost:11434
        model: qwen2.5:0.5b  # 轻量级，启动快
      
      chunking:
        ppl-threshold: 20.0
        max-chunk-size: 2000
      
      reranking:
        enabled: false  # 开发时可关闭
```

### 配置 2：生产环境

```yaml
knowledge:
  qa:
    ppl:
      default-provider: ollama
      enable-fallback: true
      fallback-order: [ollama]  # 单一提供商
      
      ollama:
        enabled: true
        base-url: http://ollama-server:11434  # 独立服务器
        model: qwen2.5:1.5b  # 更好的质量
        timeout: 60000
        max-retries: 5
        connection-pool-size: 20
      
      chunking:
        ppl-threshold: 18.0  # 更细粒度
        max-chunk-size: 2000
      
      reranking:
        enabled: true  # 启用重排序
        weight: 0.2
        top-k: 10
```

---

## 💡 Ollama 高级用法

### 1. 自定义模型配置

```bash
# 创建自定义模型文件
# Modelfile
FROM qwen2.5:0.5b

# 设置温度（控制随机性）
PARAMETER temperature 0.7

# 设置 top_p（控制采样）
PARAMETER top_p 0.9

# 设置上下文长度
PARAMETER num_ctx 4096

# 保存为自定义模型
ollama create my-qwen -f Modelfile
```

### 2. 集群部署

```bash
# 启动多个 Ollama 实例（负载均衡）
ollama serve --host 0.0.0.0:11434  # 实例 1
ollama serve --host 0.0.0.0:11435  # 实例 2

# 配置轮询
# application.yml
ollama:
  base-url: http://load-balancer:11434
```

### 3. GPU 加速

Ollama 自动检测和使用 GPU：
- NVIDIA GPU（CUDA）
- AMD GPU（ROCm）
- Apple Silicon（Metal）

无需额外配置！

---

## 🔍 故障排查

### 问题 1：Ollama 服务未启动

```bash
# 检查服务状态
curl http://localhost:11434

# 手动启动
ollama serve
```

### 问题 2：模型未下载

```bash
# 列出模型
ollama list

# 重新下载
ollama pull qwen2.5:0.5b
```

### 问题 3：连接超时

```yaml
# 增加超时时间
ollama:
  timeout: 60000  # 60 秒
  max-retries: 5
```

---

## 📚 Ollama 资源

### 官方文档
- **官网**: https://ollama.com/
- **GitHub**: https://github.com/ollama/ollama
- **模型库**: https://ollama.com/library

### Qwen 模型
- **qwen2.5:0.5b** - 轻量级（推荐）
- **qwen2.5:1.5b** - 平衡
- **qwen2.5:3b** - 高质量
- **qwen2.5:7b** - 最高质量
- **qwen2.5:14b** - 专业级

### 社区
- Discord: https://discord.gg/ollama
- GitHub Issues: 问题反馈

---

## ✅ 最终检查清单

### Ollama 方案

- [ ] 安装 Ollama
- [ ] 下载 Qwen 模型 (`ollama pull qwen2.5:0.5b`)
- [ ] 验证服务 (`curl http://localhost:11434`)
- [ ] 更新 `application.yml` 配置
- [ ] 启动 Spring Boot 应用
- [ ] 测试 PPL 功能

### 预期时间

- 安装 Ollama: 2 分钟
- 下载模型: 2-5 分钟（取决于网络）
- 配置和测试: 3 分钟
- **总计**: 约 **10 分钟** ✅

---

## 🎉 总结

### 核心建议

1. **放弃 ONNX 转换**（太复杂，易出错）
2. **使用 Ollama**（简单、稳定、快速）
3. **选择 Qwen2.5-0.5B**（轻量级，性能好）

### 为什么这么说？

经过多次尝试和调试：
- ❌ ONNX 转换：依赖问题、API 变化、转换失败
- ✅ Ollama：一次安装，永久使用，零维护

### 行动步骤

```bash
# 1. 安装 Ollama（2 分钟）
# 下载：https://ollama.com/download/windows

# 2. 下载模型（3 分钟）
ollama pull qwen2.5:0.5b

# 3. 更新配置（1 分钟）
# 编辑 application.yml，设置 default-provider: ollama

# 4. 启动应用（1 分钟）
./mvnw spring-boot:run

# 5. 开始使用！🎉
```

---

**最后更新**：2025-12-04 21:30:00  
**推荐方案**：Ollama ⭐⭐⭐⭐⭐  
**状态**：✅ 最终建议


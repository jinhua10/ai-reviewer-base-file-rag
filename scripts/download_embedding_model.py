#!/usr/bin/env python3
"""
å›½äº§å‘é‡åµŒå…¥æ¨¡å‹ä¸‹è½½è„šæœ¬
æ”¯æŒ BGEã€Text2Vec ç­‰å›½äº§æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python download_embedding_model.py --model bge-m3
    python download_embedding_model.py --model bge-base-zh --mirror
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path

def install_package(package_name):
    """å®‰è£… Python åŒ…"""
    print(f"ğŸ“¥ å®‰è£… {package_name}...")
    try:
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--upgrade", package_name],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE
        )
        print(f"âœ… {package_name} å®‰è£…æˆåŠŸ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {package_name} å®‰è£…å¤±è´¥: {e.stderr.decode() if e.stderr else str(e)}")
        return False

def check_dependencies(use_mirror=False):
    """æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ–"""
    print("=" * 70)
    print("ğŸ“¦ æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–...")
    print("=" * 70)

    required_packages = {
        "sentence_transformers": "sentence-transformers>=2.0.0",
        "torch": "torch>=2.0.0",
        "transformers": "transformers>=4.30.0",
        "optimum": "optimum[onnxruntime]>=1.14.0",
        "onnxruntime": "onnxruntime>=1.15.0",
        "onnxscript": "onnxscript>=0.1.0"
    }

    # å¦‚æœä½¿ç”¨é•œåƒï¼Œæ·»åŠ  modelscope
    if use_mirror:
        required_packages["modelscope"] = "modelscope>=1.0.0"

    installed_packages = []
    failed_packages = []

    for package_name, package_spec in required_packages.items():
        try:
            # å°è¯•å¯¼å…¥åŒ…
            __import__(package_name)
            print(f"âœ… {package_name} å·²å®‰è£…")
            installed_packages.append(package_name)
        except ImportError:
            print(f"âš ï¸  {package_name} æœªå®‰è£…ï¼Œå¼€å§‹å®‰è£…...")
            if install_package(package_spec):
                installed_packages.append(package_name)
            else:
                failed_packages.append(package_name)

    print()
    if failed_packages:
        print(f"âŒ ä»¥ä¸‹ä¾èµ–å®‰è£…å¤±è´¥: {', '.join(failed_packages)}")
        print("\nè¯·æ‰‹åŠ¨å®‰è£…:")
        print(f"pip install {' '.join([required_packages[p] for p in failed_packages])}")
        return False

    print(f"âœ… æ‰€æœ‰ä¾èµ–å·²å°±ç»ª ({len(installed_packages)}/{len(required_packages)})")
    print("=" * 70)
    print()
    return True

def download_model_huggingface(model_name, output_dir):
    """ä» Hugging Face ä¸‹è½½æ¨¡å‹"""
    from sentence_transformers import SentenceTransformer

    print(f"ğŸ“¥ ä» Hugging Face ä¸‹è½½æ¨¡å‹: {model_name}")

    try:
        # ä¸‹è½½æ¨¡å‹
        model = SentenceTransformer(model_name)

        # ä¿å­˜æ¨¡å‹
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        model.save(str(output_path))

        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {output_path}")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print("\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - ç»´åº¦: {model.get_sentence_embedding_dimension()}")
        print(f"  - æœ€å¤§é•¿åº¦: {model.max_seq_length}")

        # æµ‹è¯•
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
        embedding = model.encode(test_text)
        print(f"âœ… æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print(f"  è¾“å…¥: {test_text}")
        print(f"  è¾“å‡ºç»´åº¦: {len(embedding)}")

        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_model_modelscope(model_name, output_dir):
    """ä»é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹"""
    try:
        from modelscope import snapshot_download
        from sentence_transformers import SentenceTransformer
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("å®‰è£…: pip install modelscope sentence-transformers")
        return False

    print(f"ğŸ“¥ ä»é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹: {model_name}")

    try:
        # 1. ä½¿ç”¨é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹åˆ°ä¸´æ—¶ç›®å½•
        temp_dir = snapshot_download(model_name)
        print(f"âœ… æ¨¡å‹ä¸‹è½½åˆ°ç¼“å­˜: {temp_dir}")

        # 2. ä½¿ç”¨ sentence-transformers åŠ è½½å¹¶ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
        print(f"ğŸ“¦ è½¬æ¢å¹¶ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
        model = SentenceTransformer(temp_dir)

        # ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        model.save(str(output_path))

        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {output_path}")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print("\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - ç»´åº¦: {model.get_sentence_embedding_dimension()}")
        print(f"  - æœ€å¤§é•¿åº¦: {model.max_seq_length}")

        # æµ‹è¯•
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
        embedding = model.encode(test_text)
        print(f"âœ… æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print(f"  è¾“å…¥: {test_text}")
        print(f"  è¾“å‡ºç»´åº¦: {len(embedding)}")

        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def convert_to_onnx(model_path):
    """
    å°† Sentence-Transformers æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼

    Args:
        model_path: æ¨¡å‹è·¯å¾„

    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    print("\nğŸ”„ è½¬æ¢ä¸º ONNX æ ¼å¼...")

    try:
        from sentence_transformers import SentenceTransformer
        import torch
        import shutil

        # æ–¹æ³•1: å°è¯•ä½¿ç”¨ optimum-cliï¼ˆæ›´å®Œæ•´ï¼‰
        print("ğŸ’¡ æ–¹æ³•1: å°è¯•ä½¿ç”¨ optimum-cli...")
        output_dir = str(Path(model_path).parent / (Path(model_path).name + "-onnx"))

        # é¦–å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰æ­£ç¡®çš„ Hugging Face ç»“æ„
        model = SentenceTransformer(str(model_path))

        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å—ï¼ˆTransformerï¼‰
        if len(model) > 0 and hasattr(model[0], 'auto_model'):
            transformer_model = model[0].auto_model
            tokenizer = model[0].tokenizer

            # ä½¿ç”¨ transformers æ¨¡å‹å¯¼å‡º
            print("ğŸ“¦ ä½¿ç”¨ Transformer æ¨¡å‹ç›´æ¥å¯¼å‡º...")

            result = subprocess.run([
                sys.executable, "-m", "optimum.exporters.onnx",
                "--model", str(model_path),
                output_dir
            ], capture_output=True, text=True)

            if result.returncode == 0:
                print("âœ… optimum-cli è½¬æ¢æˆåŠŸ")
            else:
                print(f"âš ï¸ optimum-cli å¤±è´¥: {result.stderr[:200]}")
                print("\nğŸ’¡ æ–¹æ³•2: ä½¿ç”¨ torch.onnx.exportï¼ˆæ›´ç¨³å®šï¼‰...")

                # æ–¹æ³•2: ä½¿ç”¨ torch ç›´æ¥å¯¼å‡º
                Path(output_dir).mkdir(parents=True, exist_ok=True)

                # åˆ›å»ºç¤ºä¾‹è¾“å…¥
                dummy_text = "This is a sample sentence"
                encoded = tokenizer(
                    dummy_text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )

                # å¯¼å‡º ONNX - ä½¿ç”¨æ›´ç¨³å®šçš„ opset ç‰ˆæœ¬
                onnx_path = Path(output_dir) / "model.onnx"

                # å°è¯•ä¸åŒçš„ opset ç‰ˆæœ¬ï¼ˆä»é«˜åˆ°ä½ï¼‰
                opset_versions = [17, 16, 15, 14, 13]
                export_success = False

                for opset in opset_versions:
                    try:
                        print(f"  å°è¯• opset_version={opset}...")
                        torch.onnx.export(
                            transformer_model,
                            (encoded['input_ids'], encoded['attention_mask']),
                            str(onnx_path),
                            input_names=['input_ids', 'attention_mask'],
                            output_names=['last_hidden_state'],
                            dynamic_axes={
                                'input_ids': {0: 'batch', 1: 'sequence'},
                                'attention_mask': {0: 'batch', 1: 'sequence'},
                                'last_hidden_state': {0: 'batch', 1: 'sequence'}
                            },
                            opset_version=opset,
                            do_constant_folding=True,
                            export_params=True
                        )
                        print(f"âœ… torch.onnx.export è½¬æ¢æˆåŠŸ (opset={opset})")
                        export_success = True
                        break
                    except Exception as e:
                        print(f"  âš ï¸ opset={opset} å¤±è´¥: {str(e)[:100]}")
                        if onnx_path.exists():
                            onnx_path.unlink()  # åˆ é™¤å¤±è´¥çš„æ–‡ä»¶
                        continue

                if not export_success:
                    print("âŒ æ‰€æœ‰ opset ç‰ˆæœ¬è½¬æ¢éƒ½å¤±è´¥")
                    return False

        # å¤åˆ¶ ONNX æ–‡ä»¶åˆ°åŸç›®å½•
        print("\nğŸ“‹ å¤åˆ¶ ONNX æ–‡ä»¶åˆ°æ¨¡å‹ç›®å½•...")
        onnx_file = Path(output_dir) / "model.onnx"
        onnx_data = Path(output_dir) / "model.onnx_data"

        if onnx_file.exists():
            shutil.copy2(onnx_file, Path(model_path) / "model.onnx")
            print(f"âœ… å·²å¤åˆ¶: model.onnx ({onnx_file.stat().st_size / (1024*1024):.1f} MB)")

            if onnx_data.exists():
                shutil.copy2(onnx_data, Path(model_path) / "model.onnx_data")
                print(f"âœ… å·²å¤åˆ¶: model.onnx_data ({onnx_data.stat().st_size / (1024*1024):.1f} MB)")
        else:
            print("âŒ æœªæ‰¾åˆ° ONNX æ–‡ä»¶")
            return False

        # æ¸…ç†ä¸´æ—¶ç›®å½•
        print("\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        try:
            shutil.rmtree(output_dir)
            print(f"âœ… å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {Path(output_dir).name}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

        # éªŒè¯ ONNX æ¨¡å‹
        print("\nğŸ§ª éªŒè¯ ONNX æ¨¡å‹...")
        onnx_model_path = Path(model_path) / "model.onnx"

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not onnx_model_path.exists():
            print("âŒ ONNX æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = onnx_model_path.stat().st_size
        if file_size < 1024:  # å°äº 1KBï¼Œå¯èƒ½æ˜¯æŸåçš„æ–‡ä»¶
            print(f"âŒ ONNX æ¨¡å‹æ–‡ä»¶å¤ªå° ({file_size} bytes)ï¼Œå¯èƒ½å·²æŸå")
            return False

        try:
            import onnxruntime as ort

            # è®¾ç½®ä¼šè¯é€‰é¡¹ï¼Œç¦ç”¨ä¸ç¨³å®šçš„ä¼˜åŒ–
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL

            # å°è¯•åŠ è½½æ¨¡å‹
            session = ort.InferenceSession(
                str(onnx_model_path),
                sess_options=sess_options,
                providers=['CPUExecutionProvider']
            )
            print("âœ… ONNX æ¨¡å‹éªŒè¯æˆåŠŸ")

            print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
            print(f"  è¾“å…¥:")
            for input_meta in session.get_inputs():
                print(f"    - {input_meta.name}: {input_meta.shape}")
            print(f"  è¾“å‡º:")
            for output_meta in session.get_outputs():
                print(f"    - {output_meta.name}: {output_meta.shape}")

        except Exception as e:
            print(f"âš ï¸ éªŒè¯å¤±è´¥: {e}")
            print(f"ğŸ’¡ è¿™å¯èƒ½æ˜¯ç”±äº ONNX Runtime ç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´")
            print(f"   æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥å°è¯•åœ¨ Java åº”ç”¨ä¸­ä½¿ç”¨")
            # ä¸è¿”å› Falseï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½åœ¨ Java ä¸­å¯ç”¨
            return True

        return True

    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½å›½äº§å‘é‡åµŒå…¥æ¨¡å‹"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        choices=["bge-m3", "bge-large-zh", "bge-base-zh", "text2vec-base", "text2vec-large"],
        help="é€‰æ‹©æ¨¡å‹"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./models",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š./modelsï¼‰"
    )
    parser.add_argument(
        "--mirror",
        action="store_true",
        help="ä½¿ç”¨é­”æ­ç¤¾åŒºé•œåƒï¼ˆå›½å†…å¿«ï¼‰"
    )
    parser.add_argument(
        "--convert-onnx",
        action="store_true",
        default=True,
        help="è‡ªåŠ¨è½¬æ¢ä¸º ONNX æ ¼å¼ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--no-convert-onnx",
        dest="convert_onnx",
        action="store_false",
        help="ä¸è½¬æ¢ä¸º ONNX æ ¼å¼"
    )

    args = parser.parse_args()

    # æ¨¡å‹æ˜ å°„
    model_map_hf = {
        "bge-m3": "BAAI/bge-m3",
        "bge-large-zh": "BAAI/bge-large-zh",
        "bge-base-zh": "BAAI/bge-base-zh-v1.5",
        "text2vec-base": "shibing624/text2vec-base-chinese",
        "text2vec-large": "GanymedeNil/text2vec-large-chinese"
    }

    model_map_ms = {
        "bge-m3": "Xorbits/bge-m3",
        "bge-large-zh": "AI-ModelScope/bge-large-zh",
        "bge-base-zh": "AI-ModelScope/bge-base-zh-v1.5",
        "text2vec-base": "damo/nlp_corom_sentence-embedding_chinese-base",
        "text2vec-large": "damo/nlp_corom_sentence-embedding_chinese-large"
    }

    print("=" * 70)
    print("ğŸ‡¨ğŸ‡³ å›½äº§å‘é‡åµŒå…¥æ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 70)
    print()

    # è®¾ç½®é•œåƒ
    if args.mirror:
        print("ğŸŒ ä½¿ç”¨é­”æ­ç¤¾åŒºé•œåƒ...")
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

    # æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
    if not check_dependencies(use_mirror=args.mirror):
        sys.exit(1)

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    model_output = Path(args.output) / args.model

    # ä¸‹è½½æ¨¡å‹
    if args.mirror:
        model_name = model_map_ms.get(args.model)
        success = download_model_modelscope(model_name, str(model_output))
    else:
        model_name = model_map_hf.get(args.model)
        success = download_model_huggingface(model_name, str(model_output))

    if success:
        # è‡ªåŠ¨è½¬æ¢ä¸º ONNXï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if args.convert_onnx:
            onnx_success = convert_to_onnx(str(model_output))
            if not onnx_success:
                print("\nâš ï¸ ONNX è½¬æ¢å¤±è´¥ï¼Œä½† PyTorch æ¨¡å‹å·²ä¸‹è½½")
                print("ğŸ’¡ å¯ä»¥ç¨åæ‰‹åŠ¨è½¬æ¢:")
                print(f"   python {sys.argv[0]} --model {args.model} --convert-onnx")

        print("\n" + "=" * 70)
        print("ğŸ‰ å®Œæˆï¼")
        print("=" * 70)
        print()
        print("ğŸ“ ä¸‹ä¸€æ­¥ï¼š")
        print("1. æ›´æ–° application.yml é…ç½®")
        print(f"   model:")
        print(f"     name: {args.model}")
        print(f"     path: {model_output}/model.onnx")
        print()
        print("2. é‡å»ºå‘é‡ç´¢å¼•")
        print("   è®¿é—®: http://localhost:8080")
        print("   ç‚¹å‡»: é‡å»ºç´¢å¼•")
        print()
        print("3. æµ‹è¯•æ£€ç´¢æ•ˆæœ")
        print("   å¯¹æ¯”æ–°æ—§æ¨¡å‹çš„æ£€ç´¢å‡†ç¡®ç‡")
    else:
        print("\nâŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
        print("\nğŸ’¡ æ•…éšœæ’æŸ¥:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. å°è¯•ä½¿ç”¨é•œåƒ: --mirror")
        print("3. æ‰‹åŠ¨ä¸‹è½½:")
        if args.mirror:
            print(f"   è®¿é—®: https://modelscope.cn/models/{model_map_ms.get(args.model)}")
        else:
            print(f"   è®¿é—®: https://huggingface.co/{model_map_hf.get(args.model)}")
        sys.exit(1)

if __name__ == "__main__":
    main()


#!/usr/bin/env python3
"""
å›½äº§å‘é‡åµŒå…¥æ¨¡å‹ä¸‹è½½è„šæœ¬
æ”¯æŒ BGEã€Text2Vec ç­‰å›½äº§æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python download_embedding_model.py --model bge-m3
    python download_embedding_model.py --model bge-base-zh --mirror
"""

import os
import sys
import argparse
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    print("ğŸ“¦ æ£€æŸ¥ä¾èµ–...")

    try:
        import sentence_transformers
        print(f"âœ… sentence-transformers {sentence_transformers.__version__}")
        return True
    except ImportError:
        print("âŒ sentence-transformers æœªå®‰è£…")
        print("\nè¯·å®‰è£…ä¾èµ–:")
        print("pip install sentence-transformers")
        return False

def download_model_huggingface(model_name, output_dir):
    """ä» Hugging Face ä¸‹è½½æ¨¡å‹"""
    from sentence_transformers import SentenceTransformer

    print(f"ğŸ“¥ ä» Hugging Face ä¸‹è½½æ¨¡å‹: {model_name}")

    try:
        # ä¸‹è½½æ¨¡å‹
        model = SentenceTransformer(model_name)

        # ä¿å­˜æ¨¡å‹
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        model.save(str(output_path))

        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {output_path}")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print("\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - ç»´åº¦: {model.get_sentence_embedding_dimension()}")
        print(f"  - æœ€å¤§é•¿åº¦: {model.max_seq_length}")

        # æµ‹è¯•
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
        embedding = model.encode(test_text)
        print(f"âœ… æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print(f"  è¾“å…¥: {test_text}")
        print(f"  è¾“å‡ºç»´åº¦: {len(embedding)}")

        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_model_modelscope(model_name, output_dir):
    """ä»é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹"""
    try:
        from modelscope import snapshot_download
        from sentence_transformers import SentenceTransformer
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("å®‰è£…: pip install modelscope sentence-transformers")
        return False

    print(f"ğŸ“¥ ä»é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹: {model_name}")

    try:
        # 1. ä½¿ç”¨é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹åˆ°ä¸´æ—¶ç›®å½•
        temp_dir = snapshot_download(model_name)
        print(f"âœ… æ¨¡å‹ä¸‹è½½åˆ°ç¼“å­˜: {temp_dir}")

        # 2. ä½¿ç”¨ sentence-transformers åŠ è½½å¹¶ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
        print(f"ğŸ“¦ è½¬æ¢å¹¶ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
        model = SentenceTransformer(temp_dir)

        # ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        model.save(str(output_path))

        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {output_path}")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print("\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - ç»´åº¦: {model.get_sentence_embedding_dimension()}")
        print(f"  - æœ€å¤§é•¿åº¦: {model.max_seq_length}")

        # æµ‹è¯•
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
        embedding = model.encode(test_text)
        print(f"âœ… æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print(f"  è¾“å…¥: {test_text}")
        print(f"  è¾“å‡ºç»´åº¦: {len(embedding)}")

        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def convert_to_onnx(model_path):
    """
    å°† Sentence-Transformers æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼

    Args:
        model_path: æ¨¡å‹è·¯å¾„

    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    print("\nğŸ”„ è½¬æ¢ä¸º ONNX æ ¼å¼...")

    try:
        from sentence_transformers import SentenceTransformer
        import torch
        import subprocess
        import shutil

        # æ–¹æ³•1: å°è¯•ä½¿ç”¨ optimum-cliï¼ˆæ›´å®Œæ•´ï¼‰
        print("ğŸ’¡ æ–¹æ³•1: å°è¯•ä½¿ç”¨ optimum-cli...")
        output_dir = str(Path(model_path).parent / (Path(model_path).name + "-onnx"))

        # é¦–å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰æ­£ç¡®çš„ Hugging Face ç»“æ„
        model = SentenceTransformer(str(model_path))

        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å—ï¼ˆTransformerï¼‰
        if len(model) > 0 and hasattr(model[0], 'auto_model'):
            transformer_model = model[0].auto_model
            tokenizer = model[0].tokenizer

            # ä½¿ç”¨ transformers æ¨¡å‹å¯¼å‡º
            print("ğŸ“¦ ä½¿ç”¨ Transformer æ¨¡å‹ç›´æ¥å¯¼å‡º...")

            result = subprocess.run([
                sys.executable, "-m", "optimum.exporters.onnx",
                "--model", str(model_path),
                output_dir
            ], capture_output=True, text=True)

            if result.returncode == 0:
                print("âœ… optimum-cli è½¬æ¢æˆåŠŸ")
            else:
                print(f"âš ï¸ optimum-cli å¤±è´¥: {result.stderr[:200]}")
                print("\nğŸ’¡ æ–¹æ³•2: ä½¿ç”¨ torch.onnx.exportï¼ˆæ›´ç¨³å®šï¼‰...")

                # æ–¹æ³•2: ä½¿ç”¨ torch ç›´æ¥å¯¼å‡º
                Path(output_dir).mkdir(parents=True, exist_ok=True)

                # åˆ›å»ºç¤ºä¾‹è¾“å…¥
                dummy_text = "This is a sample sentence"
                encoded = tokenizer(
                    dummy_text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )

                # å¯¼å‡º ONNX
                onnx_path = Path(output_dir) / "model.onnx"
                torch.onnx.export(
                    transformer_model,
                    (encoded['input_ids'], encoded['attention_mask']),
                    str(onnx_path),
                    input_names=['input_ids', 'attention_mask'],
                    output_names=['last_hidden_state'],
                    dynamic_axes={
                        'input_ids': {0: 'batch', 1: 'sequence'},
                        'attention_mask': {0: 'batch', 1: 'sequence'},
                        'last_hidden_state': {0: 'batch', 1: 'sequence'}
                    },
                    opset_version=14,
                    do_constant_folding=True
                )
                print("âœ… torch.onnx.export è½¬æ¢æˆåŠŸ")

        # å¤åˆ¶ ONNX æ–‡ä»¶åˆ°åŸç›®å½•
        print("\nğŸ“‹ å¤åˆ¶ ONNX æ–‡ä»¶åˆ°æ¨¡å‹ç›®å½•...")
        onnx_file = Path(output_dir) / "model.onnx"
        onnx_data = Path(output_dir) / "model.onnx_data"

        if onnx_file.exists():
            shutil.copy2(onnx_file, Path(model_path) / "model.onnx")
            print(f"âœ… å·²å¤åˆ¶: model.onnx ({onnx_file.stat().st_size / (1024*1024):.1f} MB)")

            if onnx_data.exists():
                shutil.copy2(onnx_data, Path(model_path) / "model.onnx_data")
                print(f"âœ… å·²å¤åˆ¶: model.onnx_data ({onnx_data.stat().st_size / (1024*1024):.1f} MB)")
        else:
            print("âŒ æœªæ‰¾åˆ° ONNX æ–‡ä»¶")
            return False

        # éªŒè¯ ONNX æ¨¡å‹
        print("\nğŸ§ª éªŒè¯ ONNX æ¨¡å‹...")
        try:
            import onnxruntime as ort
            session = ort.InferenceSession(str(Path(model_path) / "model.onnx"))
            print("âœ… ONNX æ¨¡å‹éªŒè¯æˆåŠŸ")

            print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
            print(f"  è¾“å…¥:")
            for input_meta in session.get_inputs():
                print(f"    - {input_meta.name}: {input_meta.shape}")
            print(f"  è¾“å‡º:")
            for output_meta in session.get_outputs():
                print(f"    - {output_meta.name}: {output_meta.shape}")

        except Exception as e:
            print(f"âš ï¸ éªŒè¯å¤±è´¥: {e}")
            return False

        return True

    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½å›½äº§å‘é‡åµŒå…¥æ¨¡å‹"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        choices=["bge-m3", "bge-large-zh", "bge-base-zh", "text2vec-base", "text2vec-large"],
        help="é€‰æ‹©æ¨¡å‹"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./models",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š./modelsï¼‰"
    )
    parser.add_argument(
        "--mirror",
        action="store_true",
        help="ä½¿ç”¨é­”æ­ç¤¾åŒºé•œåƒï¼ˆå›½å†…å¿«ï¼‰"
    )
    parser.add_argument(
        "--convert-onnx",
        action="store_true",
        default=True,
        help="è‡ªåŠ¨è½¬æ¢ä¸º ONNX æ ¼å¼ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--no-convert-onnx",
        dest="convert_onnx",
        action="store_false",
        help="ä¸è½¬æ¢ä¸º ONNX æ ¼å¼"
    )

    args = parser.parse_args()

    # æ¨¡å‹æ˜ å°„
    model_map_hf = {
        "bge-m3": "BAAI/bge-m3",
        "bge-large-zh": "BAAI/bge-large-zh",
        "bge-base-zh": "BAAI/bge-base-zh-v1.5",
        "text2vec-base": "shibing624/text2vec-base-chinese",
        "text2vec-large": "GanymedeNil/text2vec-large-chinese"
    }

    model_map_ms = {
        "bge-m3": "Xorbits/bge-m3",
        "bge-large-zh": "AI-ModelScope/bge-large-zh",
        "bge-base-zh": "AI-ModelScope/bge-base-zh-v1.5",
        "text2vec-base": "damo/nlp_corom_sentence-embedding_chinese-base",
        "text2vec-large": "damo/nlp_corom_sentence-embedding_chinese-large"
    }

    print("=" * 70)
    print("ğŸ‡¨ğŸ‡³ å›½äº§å‘é‡åµŒå…¥æ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 70)
    print()

    # è®¾ç½®é•œåƒ
    if args.mirror:
        print("ğŸŒ ä½¿ç”¨é­”æ­ç¤¾åŒºé•œåƒ...")
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    model_output = Path(args.output) / args.model

    # ä¸‹è½½æ¨¡å‹
    if args.mirror:
        model_name = model_map_ms.get(args.model)
        success = download_model_modelscope(model_name, str(model_output))
    else:
        model_name = model_map_hf.get(args.model)
        success = download_model_huggingface(model_name, str(model_output))

    if success:
        # è‡ªåŠ¨è½¬æ¢ä¸º ONNXï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if args.convert_onnx:
            onnx_success = convert_to_onnx(str(model_output))
            if not onnx_success:
                print("\nâš ï¸ ONNX è½¬æ¢å¤±è´¥ï¼Œä½† PyTorch æ¨¡å‹å·²ä¸‹è½½")
                print("ğŸ’¡ å¯ä»¥ç¨åæ‰‹åŠ¨è½¬æ¢:")
                print(f"   python {sys.argv[0]} --model {args.model} --convert-onnx")

        print("\n" + "=" * 70)
        print("ğŸ‰ å®Œæˆï¼")
        print("=" * 70)
        print()
        print("ğŸ“ ä¸‹ä¸€æ­¥ï¼š")
        print("1. æ›´æ–° application.yml é…ç½®")
        print(f"   model:")
        print(f"     name: {args.model}")
        print(f"     path: {model_output}/model.onnx")
        print()
        print("2. é‡å»ºå‘é‡ç´¢å¼•")
        print("   è®¿é—®: http://localhost:8080")
        print("   ç‚¹å‡»: é‡å»ºç´¢å¼•")
        print()
        print("3. æµ‹è¯•æ£€ç´¢æ•ˆæœ")
        print("   å¯¹æ¯”æ–°æ—§æ¨¡å‹çš„æ£€ç´¢å‡†ç¡®ç‡")
    else:
        print("\nâŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
        print("\nğŸ’¡ æ•…éšœæ’æŸ¥:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. å°è¯•ä½¿ç”¨é•œåƒ: --mirror")
        print("3. æ‰‹åŠ¨ä¸‹è½½:")
        if args.mirror:
            print(f"   è®¿é—®: https://modelscope.cn/models/{model_map_ms.get(args.model)}")
        else:
            print(f"   è®¿é—®: https://huggingface.co/{model_map_hf.get(args.model)}")
        sys.exit(1)

if __name__ == "__main__":
    main()


package top.yumbo.ai.rag.ppl.onnx;

import ai.djl.huggingface.tokenizers.Encoding;
import ai.djl.huggingface.tokenizers.HuggingFaceTokenizer;
import ai.onnxruntime.*;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import top.yumbo.ai.rag.chunking.DocumentChunk;
import top.yumbo.ai.rag.i18n.I18N;
import top.yumbo.ai.rag.model.Document;
import top.yumbo.ai.rag.ppl.PPLException;
import top.yumbo.ai.rag.ppl.PPLMetrics;
import top.yumbo.ai.rag.ppl.PPLProviderType;
import top.yumbo.ai.rag.ppl.PPLService;
import top.yumbo.ai.rag.ppl.config.ChunkConfig;
import top.yumbo.ai.rag.ppl.config.PPLConfig;
import top.yumbo.ai.rag.ppl.config.RerankConfig;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.nio.file.Paths;
import java.time.Duration;
import java.util.*;
import java.util.stream.Collectors;

/**
 * åŸºäº ONNX Runtime çš„ PPL æœåŠ¡å®ç°
 * (ONNX Runtime based PPL Service Implementation)
 *
 * ç‰¹ç‚¹ (Features):
 * - æœ¬åœ°åµŒå…¥å¼æ¨ç†ï¼Œæ— ç½‘ç»œå¼€é”€ (Local embedded inference, no network overhead)
 * - é€Ÿåº¦å¿«ï¼ˆ30-150msï¼‰(Fast speed: 30-150ms)
 * - æˆæœ¬ä½ï¼ˆå®Œå…¨å…è´¹ï¼‰(Low cost: completely free)
 * - æ”¯æŒ GPU åŠ é€Ÿ (Supports GPU acceleration)
 * - æ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æ¡£ (Supports Chinese-English mixed documents)
 *
 * @author AI Reviewer Team
 * @since 2025-12-04
 */
@Slf4j
@ConditionalOnProperty(prefix = "knowledge.qa.ppl.onnx", name = "enabled", havingValue = "true", matchIfMissing = true)
public class PPLOnnxService implements PPLService {

    private final PPLConfig config;
    private final PPLMetrics metrics;

    // ONNX Runtime ç»„ä»¶
    private OrtEnvironment env;
    private OrtSession session;
    private HuggingFaceTokenizer tokenizer;

    // PPL ç¼“å­˜
    private Cache<String, Double> pplCache;

    // æ¨¡å‹é…ç½®ï¼ˆä»æ¨¡å‹è¾“å…¥æ¨æ–­ï¼‰(Model config inferred from model inputs)
    private int numLayers = 0;           // transformer å±‚æ•° (number of transformer layers)
    private int numHeads = 0;            // æ³¨æ„åŠ›å¤´æ•° (number of attention heads)
    private int headDim = 0;             // æ¯ä¸ªå¤´çš„ç»´åº¦ (dimension per head)
    private boolean useKVCache = false;  // æ˜¯å¦ä½¿ç”¨ KV Cache (whether to use KV cache)

    public PPLOnnxService(PPLConfig config) {
        this.config = config;
        this.metrics = new PPLMetrics();
    }

    @PostConstruct
    public void init() {
        log.info(I18N.get("ppl_onnx.log.init_start"));

        try {
            PPLConfig.OnnxConfig onnxConfig = config.getOnnx();

            log.info(I18N.get("ppl_onnx.log.model_path", onnxConfig.getModelPath()));
            log.info(I18N.get("ppl_onnx.log.tokenizer_path", onnxConfig.getTokenizerPath()));

            // 1. åˆå§‹åŒ– ONNX Runtime ç¯å¢ƒ
            this.env = OrtEnvironment.getEnvironment();
            log.info(I18N.get("ppl_onnx.log.env_created"));

            // 2. åŠ è½½ ONNX æ¨¡å‹
            OrtSession.SessionOptions sessionOptions = new OrtSession.SessionOptions();
            sessionOptions.setOptimizationLevel(OrtSession.SessionOptions.OptLevel.BASIC_OPT);

            this.session = env.createSession(onnxConfig.getModelPath(), sessionOptions);
            log.info(I18N.get("ppl_onnx.log.model_loaded", onnxConfig.getModelPath()));

            // æ‰“å°æ¨¡å‹è¾“å…¥è¾“å‡ºä¿¡æ¯ï¼Œç”¨äºè¯Šæ–­ (Print model input/output info for diagnosis)
            logModelInfo();

            // 3. åŠ è½½ Tokenizer
            this.tokenizer = HuggingFaceTokenizer.newInstance(Paths.get(onnxConfig.getTokenizerPath()));
            log.info(I18N.get("ppl_onnx.log.tokenizer_loaded", onnxConfig.getTokenizerPath()));

            // 4. åˆå§‹åŒ–ç¼“å­˜
            if (onnxConfig.isUseCache()) {
                this.pplCache = Caffeine.newBuilder()
                        .maximumSize(onnxConfig.getCacheSize())
                        .expireAfterWrite(Duration.ofSeconds(onnxConfig.getCacheTtl()))
                        .recordStats()
                        .build();
                log.info(I18N.get("ppl_onnx.log.cache_init",
                        onnxConfig.getCacheSize(), onnxConfig.getCacheTtl()));
            }

            log.info(I18N.get("ppl_onnx.log.init_success"));

        } catch (Exception e) {
            log.error(I18N.get("ppl_onnx.log.init_failed"), e);
            throw new RuntimeException(I18N.get("ppl_onnx.error.init_failed"), e);
        }
    }

    /**
     * æ‰“å°æ¨¡å‹è¾“å…¥è¾“å‡ºä¿¡æ¯ï¼Œç”¨äºè¯Šæ–­
     * (Print model input/output info for diagnosis)
     */
    private void logModelInfo() {
        try {
            log.info("ğŸ“Š æ¨¡å‹è¾“å…¥ä¿¡æ¯ (Model Input Info):");
            Map<String, NodeInfo> inputInfo = session.getInputInfo();
            for (Map.Entry<String, NodeInfo> entry : inputInfo.entrySet()) {
                String name = entry.getKey();
                NodeInfo info = entry.getValue();
                log.info("  - è¾“å…¥: {} (ç±»å‹: {})", name, info.getInfo());

                // æ£€æµ‹æ˜¯å¦ä½¿ç”¨ KV Cache (Check if using KV cache)
                if (name.startsWith("past_key_values.")) {
                    useKVCache = true;
                    // æå–å±‚æ•° (Extract layer count)
                    try {
                        String[] parts = name.split("\\.");
                        if (parts.length >= 2) {
                            int layerNum = Integer.parseInt(parts[1]);
                            numLayers = Math.max(numLayers, layerNum + 1);
                        }
                    } catch (NumberFormatException ignored) {}

                    // ä»ç¬¬ä¸€ä¸ª KV Cache è¾“å…¥æå– numHeads å’Œ headDim
                    // (Extract numHeads and headDim from first KV Cache input)
                    if (numHeads == 0 && info.getInfo() instanceof TensorInfo) {
                        TensorInfo tensorInfo = (TensorInfo) info.getInfo();
                        long[] shape = tensorInfo.getShape();
                        // KV Cache å½¢çŠ¶: [batch, num_heads, seq_len, head_dim]
                        // ä½†æœ‰äº›æ¨¡å‹å¯èƒ½æ˜¯ [batch, num_kv_heads, seq_len, head_dim]
                        if (shape.length >= 4) {
                            numHeads = (int) shape[1];  // num_heads æˆ– num_kv_heads
                            headDim = (int) shape[3];   // head_dim
                            log.info("  ğŸ“ ä»æ¨¡å‹æå– KV Cache ç»´åº¦: num_heads={}, head_dim={}", numHeads, headDim);
                        }
                    }
                }
            }

            log.info("ğŸ“Š æ¨¡å‹è¾“å‡ºä¿¡æ¯ (Model Output Info):");
            Map<String, NodeInfo> outputInfo = session.getOutputInfo();
            for (Map.Entry<String, NodeInfo> entry : outputInfo.entrySet()) {
                log.info("  - è¾“å‡º: {} (ç±»å‹: {})", entry.getKey(), entry.getValue().getInfo());
            }

            if (useKVCache) {
                log.info("âš ï¸ æ¨¡å‹ä½¿ç”¨ KV Cacheï¼Œå…± {} å±‚, num_heads={}, head_dim={}",
                        numLayers, numHeads, headDim);

                // å¦‚æœæ— æ³•ä»æ¨¡å‹æå–ï¼Œä½¿ç”¨é»˜è®¤å€¼å¹¶è­¦å‘Š
                if (numHeads == 0 || headDim == 0) {
                    log.warn("âš ï¸ æ— æ³•ä»æ¨¡å‹æå– KV Cache ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼");
                    numHeads = 2;   // GQA æ¨¡å¼å¸¸è§å€¼
                    headDim = 64;
                }
            } else {
                log.info("âœ… æ¨¡å‹ä¸ä½¿ç”¨ KV Cacheï¼Œå¯ç›´æ¥æ¨ç†");
            }

        } catch (OrtException e) {
            log.warn("âš ï¸ æ— æ³•è·å–æ¨¡å‹ä¿¡æ¯: {}", e.getMessage());
        }
    }

    /**
     * è®¡ç®—æ–‡æœ¬çš„å›°æƒ‘åº¦
     * (Calculate perplexity for text)
     *
     * å›°æƒ‘åº¦æ˜¯è¡¡é‡è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬é¢„æµ‹èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œå€¼è¶Šä½è¡¨ç¤ºæ–‡æœ¬è¶Š"æµç•…"
     * (Perplexity measures how well the language model predicts the text, lower is more "fluent")
     *
     * @param text æ–‡æœ¬ (text)
     * @return å›°æƒ‘åº¦å€¼ (perplexity value)
     * @throws PPLException è®¡ç®—å¤±è´¥æ—¶æŠ›å‡º (thrown when calculation fails)
     */
    @Override
    public double calculatePerplexity(String text) throws PPLException {
        if (text == null || text.trim().isEmpty()) {
            return Double.MAX_VALUE;
        }

        // æ£€æŸ¥ç¼“å­˜ (Check cache)
        if (pplCache != null) {
            Double cached = pplCache.getIfPresent(text);
            if (cached != null) {
                metrics.recordCacheHit();
                return cached;
            }
            metrics.recordCacheMiss();
        }

        long startTime = System.currentTimeMillis();
        List<OnnxTensor> tensorsToClose = new ArrayList<>();

        try {
            // 1. Tokenize - å°†æ–‡æœ¬è½¬æ¢ä¸º Token IDs (Convert text to Token IDs)
            Encoding encoding = tokenizer.encode(text);
            long[] inputIds = encoding.getIds();
            long[] attentionMask = encoding.getAttentionMask();

            if (inputIds.length == 0) {
                return Double.MAX_VALUE;
            }

            // 2. å‡†å¤‡ ONNX è¾“å…¥ (Prepare ONNX inputs)
            Map<String, OnnxTensor> inputs = new HashMap<>();
            int seqLen = inputIds.length;

            // å°† inputIds è½¬æ¢ä¸º [1, seq_len] çš„å¼ é‡ (Convert to [1, seq_len] tensor)
            long[][] inputIdsArray = new long[1][seqLen];
            inputIdsArray[0] = inputIds;

            long[][] attentionMaskArray = new long[1][seqLen];
            attentionMaskArray[0] = attentionMask;

            // ç”Ÿæˆ position_ids [1, seq_len]ï¼Œå€¼ä¸º 0, 1, 2, ..., seq_len-1
            // (Generate position_ids [1, seq_len], values are 0, 1, 2, ..., seq_len-1)
            long[][] positionIdsArray = new long[1][seqLen];
            for (int i = 0; i < seqLen; i++) {
                positionIdsArray[0][i] = i;
            }

            OnnxTensor inputIdsTensor = OnnxTensor.createTensor(env, inputIdsArray);
            OnnxTensor attentionMaskTensor = OnnxTensor.createTensor(env, attentionMaskArray);
            OnnxTensor positionIdsTensor = OnnxTensor.createTensor(env, positionIdsArray);

            tensorsToClose.add(inputIdsTensor);
            tensorsToClose.add(attentionMaskTensor);
            tensorsToClose.add(positionIdsTensor);

            inputs.put("input_ids", inputIdsTensor);
            inputs.put("attention_mask", attentionMaskTensor);
            inputs.put("position_ids", positionIdsTensor);

            // 3. å¦‚æœæ¨¡å‹ä½¿ç”¨ KV Cacheï¼Œæ·»åŠ ç©ºçš„ past_key_values
            // (If model uses KV Cache, add empty past_key_values)
            if (useKVCache) {
                addEmptyKVCache(inputs, tensorsToClose);
            }

            // 3. æ¨¡å‹æ¨ç† (Model inference)
            try (OrtSession.Result results = session.run(inputs)) {
                // è·å– logitsï¼ˆæ¨¡å‹è¾“å‡ºï¼‰(Get logits - model output)
                OnnxValue logitsValue = results.get(0);
                float[][][] logits = (float[][][]) logitsValue.getValue();

                // 4. è®¡ç®—å›°æƒ‘åº¦ (Calculate perplexity)
                double totalLoss = 0.0;
                int validTokens = 0;

                // å¯¹æ¯ä¸ªä½ç½®è®¡ç®— cross-entropy loss (Calculate cross-entropy loss for each position)
                for (int i = 0; i < inputIds.length - 1; i++) {
                    int targetId = (int) inputIds[i + 1];
                    float[] probs = logits[0][i];

                    // Softmax å½’ä¸€åŒ– (Softmax normalization)
                    float maxLogit = Float.NEGATIVE_INFINITY;
                    for (float logit : probs) {
                        maxLogit = Math.max(maxLogit, logit);
                    }

                    double sumExp = 0.0;
                    for (float logit : probs) {
                        sumExp += Math.exp(logit - maxLogit);
                    }

                    double logProb = probs[targetId] - maxLogit - Math.log(sumExp);
                    totalLoss -= logProb;  // ç­‰ä»·äº += -logProb (equivalent to += -logProb)
                    validTokens++;
                }

                // PPL = exp(average loss)
                double ppl = validTokens > 0 ? Math.exp(totalLoss / validTokens) : Double.MAX_VALUE;

                // æ¸…ç†èµ„æº (Clean up resources)
                for (OnnxTensor tensor : tensorsToClose) {
                    try {
                        tensor.close();
                    } catch (Exception ignored) {}
                }

                // ç¼“å­˜ç»“æœ (Cache result)
                if (pplCache != null) {
                    pplCache.put(text, ppl);
                }

                metrics.recordSuccess(System.currentTimeMillis() - startTime);
                return ppl;
            }

        } catch (Exception e) {
            // ç¡®ä¿æ¸…ç†èµ„æº (Ensure cleanup)
            for (OnnxTensor tensor : tensorsToClose) {
                try {
                    tensor.close();
                } catch (Exception ignored) {}
            }

            metrics.recordFailure(System.currentTimeMillis() - startTime);
            log.error(I18N.get("ppl_onnx.log.calc_ppl_failed",
                    text.substring(0, Math.min(50, text.length()))), e);
            throw new PPLException(PPLProviderType.ONNX,
                    I18N.get("ppl_onnx.error.calc_ppl_failed"), e);
        }
    }

    /**
     * æ·»åŠ ç©ºçš„ KV Cache å¼ é‡åˆ°è¾“å…¥
     * (Add empty KV Cache tensors to inputs)
     *
     * å¯¹äºé¦–æ¬¡æ¨ç†ï¼Œpast_key_values çš„ seq_len ç»´åº¦ä¸º 0
     * (For first inference, the seq_len dimension of past_key_values is 0)
     *
     * ä½¿ç”¨ DirectFloatBuffer åˆ›å»ºå¼ é‡ï¼Œæ”¯æŒé›¶ç»´åº¦
     * (Use DirectFloatBuffer to create tensor, supports zero dimension)
     *
     * @param inputs è¾“å…¥æ˜ å°„ (input map)
     * @param tensorsToClose éœ€è¦æ¸…ç†çš„å¼ é‡åˆ—è¡¨ (list of tensors to close)
     */
    private void addEmptyKVCache(Map<String, OnnxTensor> inputs, List<OnnxTensor> tensorsToClose)
            throws OrtException {
        // ä¸ºæ¯ä¸€å±‚åˆ›å»ºç©ºçš„ key å’Œ value å¼ é‡
        // (Create empty key and value tensors for each layer)
        for (int layer = 0; layer < numLayers; layer++) {
            String keyName = "past_key_values." + layer + ".key";
            String valueName = "past_key_values." + layer + ".value";

            // ä½¿ç”¨ DirectFloatBuffer åˆ›å»ºé›¶ç»´åº¦å¼ é‡
            // å½¢çŠ¶: [batch=1, num_heads, seq_len=0, head_dim]
            // æ€»å…ƒç´ æ•°: 1 * num_heads * 0 * head_dim = 0
            long[] shape = new long[]{1, numHeads, 0, headDim};

            // åˆ›å»ºç›´æ¥ç¼“å†²åŒºï¼ˆå®¹é‡ä¸º 0ï¼‰
            java.nio.FloatBuffer emptyBuffer = java.nio.ByteBuffer
                    .allocateDirect(0)
                    .order(java.nio.ByteOrder.nativeOrder())
                    .asFloatBuffer();

            OnnxTensor keyTensor = OnnxTensor.createTensor(env, emptyBuffer, shape);
            OnnxTensor valueTensor = OnnxTensor.createTensor(env, emptyBuffer, shape);

            tensorsToClose.add(keyTensor);
            tensorsToClose.add(valueTensor);

            inputs.put(keyName, keyTensor);
            inputs.put(valueName, valueTensor);
        }

        log.debug("âœ… å·²æ·»åŠ  {} å±‚ç©º KV Cache (Added {} layers of empty KV Cache)", numLayers, numLayers);
    }

    /**
     * åŸºäº PPL çš„æ–‡æ¡£åˆ†å—
     * (PPL-based document chunking)
     *
     * ä½¿ç”¨å›°æƒ‘åº¦æ£€æµ‹è¯­ä¹‰è¾¹ç•Œï¼Œå®ç°æ™ºèƒ½åˆ†å—
     * (Use perplexity to detect semantic boundaries for intelligent chunking)
     *
     * @param content æ–‡æ¡£å†…å®¹ (document content)
     * @param query æŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œç”¨äºæŸ¥è¯¢æ„ŸçŸ¥åˆ†å—ï¼‰(query, optional, for query-aware chunking)
     * @param config åˆ†å—é…ç½® (chunk configuration)
     * @return æ–‡æ¡£å—åˆ—è¡¨ (list of document chunks)
     * @throws PPLException PPLè®¡ç®—å¼‚å¸¸ (PPLè®¡ç®—å¼‚å¸¸)
     */
    @Override
    public List<DocumentChunk> chunk(String content, String query, ChunkConfig config) throws PPLException {
        if (content == null || content.trim().isEmpty()) {
            return Collections.emptyList();
        }

        long startTime = System.currentTimeMillis();

        try {
            List<DocumentChunk> chunks = new ArrayList<>();

            // 1. åˆ†å¥ - æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            // (Split into sentences by punctuation - supports Chinese and English)
            List<String> sentences = splitIntoSentences(content);

            if (sentences.isEmpty()) {
                return Collections.emptyList();
            }

            // 2. å¦‚æœå¯ç”¨ç²—åˆ†å—ï¼Œå…ˆæŒ‰æ®µè½ç²—åˆ†
            // (If coarse chunking is enabled, first chunk by paragraphs)
            List<List<String>> coarseChunks = new ArrayList<>();
            if (config.isEnableCoarseChunking()) {
                coarseChunks = coarseChunk(sentences, config.getMaxChunkSize());
            } else {
                coarseChunks.add(sentences);
            }

            // 3. å¯¹æ¯ä¸ªç²—å—è¿›è¡Œ PPL ç²¾ç»†åˆ‡åˆ†
            // (Fine-chunk each coarse chunk using PPL)
            int chunkIndex = 0;
            for (List<String> coarseChunk : coarseChunks) {
                List<DocumentChunk> fineChunks = pplBasedChunk(coarseChunk, config);

                // è®¾ç½®ç´¢å¼• (Set index)
                for (DocumentChunk chunk : fineChunks) {
                    chunk.setIndex(chunkIndex++);
                }

                chunks.addAll(fineChunks);
            }

            metrics.recordSuccess(System.currentTimeMillis() - startTime);
            return chunks;

        } catch (Exception e) {
            metrics.recordFailure(System.currentTimeMillis() - startTime);
            throw new PPLException(PPLProviderType.ONNX,
                    I18N.get("ppl_onnx.error.chunk_failed"), e);
        }
    }

    /**
     * åˆ†å¥ - æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
     * (Split into sentences - by punctuation marks, supports Chinese and English)
     *
     * æ”¯æŒçš„åˆ†å¥ç¬¦å· (Supported sentence delimiters):
     * - ä¸­æ–‡ï¼šã€‚ï¼ï¼Ÿ (Chinese: period, exclamation, question)
     * - è‹±æ–‡ï¼š. ! ? (English: period, exclamation, question)
     * - ä¿ç•™å¥æœ«æ ‡ç‚¹ (Preserves trailing punctuation)
     *
     * @param content æ–‡æœ¬å†…å®¹ (text content)
     * @return å¥å­åˆ—è¡¨ (list of sentences)
     */
    private List<String> splitIntoSentences(String content) {
        List<String> sentences = new ArrayList<>();

        // æŒ‰ä¸­è‹±æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²ï¼Œä¿ç•™åˆ†éš”ç¬¦
        // Split by Chinese/English periods, question marks, exclamation marks
        String[] parts = content.split("(?<=[ã€‚ï¼ï¼Ÿ.!?])\\s*");

        for (String part : parts) {
            if (!part.trim().isEmpty()) {
                sentences.add(part.trim());
            }
        }

        return sentences;
    }

    /**
     * ç²—åˆ†å— - è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å‰²ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
     * (Coarse chunking - semantic-aware splitting, optimized version)
     *
     * æ”¹è¿›ç‚¹ (Improvements):
     * 1. åœ¨è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ†ï¼Œè€Œä¸æ˜¯å›ºå®šå­—æ•° (Split at semantic boundaries, not fixed size)
     * 2. æ”¯æŒä¸Šä¸‹æ–‡é‡å ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤± (Support context overlap to avoid information loss)
     * 3. è½¯é™åˆ¶ + ç¡¬é™åˆ¶ï¼Œä¿è¯å—å¤§å°åœ¨åˆç†èŒƒå›´ (Soft + hard limits for reasonable chunk size)
     *
     * @param sentences å¥å­åˆ—è¡¨ (list of sentences)
     * @param maxChunkSize æœ€å¤§å—å¤§å° (maximum chunk size)
     * @return ç²—åˆ†å—åˆ—è¡¨ (list of coarse chunks)
     */
    private List<List<String>> coarseChunk(List<String> sentences, int maxChunkSize) {
        return semanticCoarseChunk(sentences, maxChunkSize, true, 2);
    }

    /**
     * è¯­ä¹‰æ„ŸçŸ¥çš„ç²—åˆ†å—
     * (Semantic-aware coarse chunking)
     *
     * @param sentences å¥å­åˆ—è¡¨ (list of sentences)
     * @param maxChunkSize æœ€å¤§å—å¤§å° (maximum chunk size)
     * @param semanticAware æ˜¯å¦å¯ç”¨è¯­ä¹‰æ„ŸçŸ¥ (whether to enable semantic awareness)
     * @param overlapSentences é‡å å¥å­æ•° (number of overlapping sentences)
     * @return ç²—åˆ†å—åˆ—è¡¨ (list of coarse chunks)
     */
    private List<List<String>> semanticCoarseChunk(List<String> sentences,
            int maxChunkSize, boolean semanticAware, int overlapSentences) {

        List<List<String>> chunks = new ArrayList<>();
        List<String> currentChunk = new ArrayList<>();
        List<String> overlapBuffer = new ArrayList<>();  // é‡å ç¼“å†²åŒº (overlap buffer)
        int currentSize = 0;

        // ç›®æ ‡å¤§å°ä¸ºæœ€å¤§å¤§å°çš„ 60%ï¼Œè½¯é™åˆ¶ (Target size is 60% of max, soft limit)
        int targetSize = (int) (maxChunkSize * 0.6);
        // ç¡¬æ€§ä¸Šé™ä¸ºæœ€å¤§å¤§å°çš„ 125% (Hard limit is 125% of max)
        int hardLimit = (int) (maxChunkSize * 1.25);

        for (int i = 0; i < sentences.size(); i++) {
            String sentence = sentences.get(i);
            int sentenceLength = sentence.length();

            boolean shouldSplit = false;

            if (semanticAware) {
                // è¯­ä¹‰æ„ŸçŸ¥æ¨¡å¼ï¼šåˆ°è¾¾ç›®æ ‡å¤§å°åï¼Œåœ¨è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ†
                String prevSentence = i > 0 ? sentences.get(i - 1) : null;
                boolean isSemanticBoundary = isSemanticBoundary(sentence, prevSentence);

                if (currentSize >= targetSize && isSemanticBoundary) {
                    shouldSplit = true;
                }
            }

            // ç¡¬æ€§ä¸Šé™ï¼šè¶…è¿‡åˆ™å¼ºåˆ¶åˆ‡åˆ†
            if (currentSize + sentenceLength > hardLimit && !currentChunk.isEmpty()) {
                shouldSplit = true;
            }

            if (shouldSplit && !currentChunk.isEmpty()) {
                // æ·»åŠ å½“å‰å—ï¼ˆåŒ…å«å‰ä¸€å—çš„å°¾éƒ¨ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
                List<String> chunkWithContext = new ArrayList<>();
                if (!overlapBuffer.isEmpty()) {
                    chunkWithContext.addAll(overlapBuffer);
                }
                chunkWithContext.addAll(currentChunk);
                chunks.add(chunkWithContext);

                // æ›´æ–°é‡å ç¼“å†²åŒºï¼ˆä¿ç•™æœ€å N ä¸ªå¥å­ï¼‰
                overlapBuffer.clear();
                if (overlapSentences > 0) {
                    int overlapStart = Math.max(0, currentChunk.size() - overlapSentences);
                    for (int j = overlapStart; j < currentChunk.size(); j++) {
                        overlapBuffer.add(currentChunk.get(j));
                    }
                }

                currentChunk.clear();
                currentSize = 0;
            }

            currentChunk.add(sentence);
            currentSize += sentenceLength;
        }

        // å¤„ç†æœ€åä¸€å—
        if (!currentChunk.isEmpty()) {
            List<String> chunkWithContext = new ArrayList<>();
            if (!overlapBuffer.isEmpty()) {
                chunkWithContext.addAll(overlapBuffer);
            }
            chunkWithContext.addAll(currentChunk);
            chunks.add(chunkWithContext);
        }

        return chunks;
    }

    /**
     * æ£€æµ‹è¯­ä¹‰è¾¹ç•Œï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
     * (Detect semantic boundary - supports Chinese and English)
     *
     * æ£€æµ‹è§„åˆ™ (Detection rules):
     * 1. ä¸­æ–‡ç« èŠ‚æ ‡é¢˜ (Chinese chapter titles): ç¬¬ä¸€ç« ã€ç¬¬1èŠ‚ç­‰
     * 2. è‹±æ–‡ç« èŠ‚æ ‡é¢˜ (English chapter titles): Chapter 1, Section 2ç­‰
     * 3. Markdown æ ‡é¢˜ (Markdown headings): # ## ###ç­‰
     * 4. æ•°å­—ç¼–å·æ ‡é¢˜ (Numbered headings): 1. 1.1 ç­‰
     * 5. ä¸­æ–‡è¿‡æ¸¡è¯ (Chinese transition words): é¦–å…ˆã€å…¶æ¬¡ã€æ€»ä¹‹ç­‰
     * 6. è‹±æ–‡è¿‡æ¸¡è¯ (English transition words): First, Second, In conclusionç­‰
     * 7. åˆ—è¡¨ç»“æŸ (End of list)
     * 8. æ®µè½åˆ†éš”æ ‡è®° (Paragraph separators)
     *
     * @param current å½“å‰å¥å­ (current sentence)
     * @param previous å‰ä¸€å¥å­ (previous sentence)
     * @return æ˜¯å¦æ˜¯è¯­ä¹‰è¾¹ç•Œ (whether it's a semantic boundary)
     */
    private boolean isSemanticBoundary(String current, String previous) {
        if (current == null || current.isEmpty()) {
            return false;
        }

        String trimmed = current.trim();
        String lowerTrimmed = trimmed.toLowerCase();

        // 1. ä¸­æ–‡ç« èŠ‚æ ‡é¢˜ (Chinese chapter titles)
        if (trimmed.matches("^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\\d]+[ç« èŠ‚ç¯‡éƒ¨æ¡æ¬¾é¡¹].*")) {
            return true;
        }

        // 2. è‹±æ–‡ç« èŠ‚æ ‡é¢˜ (English chapter titles)
        if (lowerTrimmed.matches("^(chapter|section|part|article|appendix)\\s*\\d+.*")) {
            return true;
        }

        // 3. Markdown æ ‡é¢˜ (Markdown headings)
        if (trimmed.matches("^#{1,6}\\s+.*")) {
            return true;
        }

        // 4. æ•°å­—ç¼–å·æ ‡é¢˜ (Numbered headings): "1. xxx", "1.1 xxx", "(1) xxx"
        if (trimmed.matches("^\\d+(\\.\\d+)*[.ã€\\s].*") && trimmed.length() < 100) {
            return true;
        }
        if (trimmed.matches("^\\([\\d]+\\)\\s+.*") && trimmed.length() < 100) {
            return true;
        }

        // 5. ä¸­æ–‡è¿‡æ¸¡è¯/æ®µè½å¼€å¤´è¯ (Chinese transition words)
        if (trimmed.matches("^(é¦–å…ˆ|å…¶æ¬¡|å†æ¬¡|ç„¶å|æ¥ç€|æœ€å|å¦å¤–|æ­¤å¤–|" +
                "ç»¼ä¸Š|æ€»ä¹‹|å› æ­¤|æ‰€ä»¥|æ€»ç»“|ç»“è®º|æ¦‚è¿°|ç®€ä»‹|èƒŒæ™¯|ç›®çš„|" +
                "ä¸€æ–¹é¢|å¦ä¸€æ–¹é¢|ä¸æ­¤åŒæ—¶|éœ€è¦æ³¨æ„|å€¼å¾—ä¸€æ|ç‰¹åˆ«æ˜¯|" +
                "ç¬¬ä¸€|ç¬¬äºŒ|ç¬¬ä¸‰|ç¬¬å››|ç¬¬äº”|æœ€ç»ˆ|æ€»è€Œè¨€ä¹‹|å½’çº³èµ·æ¥|" +
                "æ¢å¥è¯è¯´|æ¢è¨€ä¹‹|ä¹Ÿå°±æ˜¯è¯´|å…·ä½“æ¥è¯´|ä¾‹å¦‚|æ¯”å¦‚).*")) {
            return true;
        }

        // 6. è‹±æ–‡è¿‡æ¸¡è¯ (English transition words)
        if (lowerTrimmed.matches("^(first(ly)?|second(ly)?|third(ly)?|fourth(ly)?|finally|lastly|" +
                "in conclusion|to summarize|in summary|to sum up|overall|" +
                "furthermore|moreover|additionally|besides|however|nevertheless|" +
                "on the other hand|in contrast|meanwhile|consequently|therefore|" +
                "for example|for instance|specifically|in particular|namely|" +
                "introduction|background|purpose|objective|conclusion|summary|" +
                "note that|it is worth noting|importantly|significantly)[,:\\s].*")) {
            return true;
        }

        // 7. å‰ä¸€å¥æ˜¯åˆ—è¡¨é¡¹ç»“å°¾ï¼Œå½“å‰ä¸æ˜¯åˆ—è¡¨é¡¹ï¼ˆåˆ—è¡¨ç»“æŸï¼‰
        // (Previous sentence is list item, current is not - end of list)
        if (previous != null) {
            // ä¸­æ–‡åˆ—è¡¨é¡¹ (Chinese list items)
            boolean prevIsChineseList = previous.trim().matches("^[\\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[.ã€ï¼‰)].*");
            // è‹±æ–‡åˆ—è¡¨é¡¹ (English list items)
            boolean prevIsEnglishList = previous.trim().matches("^[-*â€¢]\\s.*") ||
                    previous.trim().matches("^\\(?[a-zA-Z\\d]+[.):]\\s.*");
            boolean prevIsList = prevIsChineseList || prevIsEnglishList;

            boolean currIsChineseList = trimmed.matches("^[\\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[.ã€ï¼‰)].*");
            boolean currIsEnglishList = trimmed.matches("^[-*â€¢]\\s.*") ||
                    trimmed.matches("^\\(?[a-zA-Z\\d]+[.):]\\s.*");
            boolean currIsList = currIsChineseList || currIsEnglishList;

            if (prevIsList && !currIsList) {
                return true;
            }
        }

        // 8. æ®µè½åˆ†éš”æ ‡è®° (Paragraph separators)
        if (trimmed.startsWith("[PARA]") || trimmed.startsWith("---") || trimmed.startsWith("***")) {
            return true;
        }

        return false;
    }

    /**
     * åŸºäº PPL çš„ç²¾ç»†åˆ‡åˆ†
     * (PPL-based fine chunking)
     *
     * ä½¿ç”¨å›°æƒ‘åº¦(Perplexity)æ£€æµ‹è¯­ä¹‰çªå˜ç‚¹ï¼Œåœ¨çªå˜ç‚¹å¤„åˆ‡åˆ†
     * (Use perplexity to detect semantic transition points and split at those points)
     *
     * @param sentences å¥å­åˆ—è¡¨ (list of sentences)
     * @param config åˆ†å—é…ç½® (chunk configuration)
     * @return æ–‡æ¡£å—åˆ—è¡¨ (list of document chunks)
     * @throws PPLException PPLè®¡ç®—å¼‚å¸¸ (PPLè®¡ç®—å¼‚å¸¸)
     */
    private List<DocumentChunk> pplBasedChunk(List<String> sentences, ChunkConfig config) throws PPLException {
        List<DocumentChunk> chunks = new ArrayList<>();

        if (sentences.isEmpty()) {
            return chunks;
        }

        // æ£€æµ‹å›¾ç‰‡æ ‡è®°ä½ç½®
        Set<Integer> imagePositions = detectImageMarkers(sentences);

        if (!imagePositions.isEmpty()) {
            log.debug("   ğŸ–¼ï¸ æ£€æµ‹åˆ° {} ä¸ªå›¾ç‰‡ä½ç½®æ ‡è®°", imagePositions.size());
        }

        // è®¡ç®—æ¯ä¸ªå¥å­çš„ PPL
        List<Double> pplScores = new ArrayList<>();
        for (String sentence : sentences) {
            double ppl = calculatePerplexity(sentence);
            pplScores.add(ppl);
        }

        // æ‰¾åˆ° PPL çªå˜ç‚¹ï¼ˆè€ƒè™‘å›¾ç‰‡ä½ç½®ï¼‰
        List<Integer> splitPoints = new ArrayList<>();
        splitPoints.add(0); // èµ·å§‹ç‚¹

        for (int i = 1; i < pplScores.size(); i++) {
            double currentPPL = pplScores.get(i);
            double prevPPL = pplScores.get(i - 1);

            // è®¡ç®— PPL å˜åŒ–
            double pplDelta = Math.abs(currentPPL - prevPPL);

            // å¦‚æœé™„è¿‘æœ‰å›¾ç‰‡æ ‡è®°ï¼Œé™ä½åˆ‡åˆ†æƒé‡
            if (isNearImagePosition(i, imagePositions)) {
                pplDelta *= 0.3;  // å¤§å¹…é™ä½å›¾ç‰‡é™„è¿‘çš„åˆ‡åˆ†æ¦‚ç‡
                log.debug("   ğŸ“ ä½ç½® {} é è¿‘å›¾ç‰‡ï¼ŒPPL æƒé‡é™ä½è‡³ {}", i, pplDelta);
            }

            // PPL å˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œä¸”å½“å‰å—ä¸ä¸ºç©º
            if (pplDelta > config.getPplThreshold()) {
                splitPoints.add(i);
            }
        }

        splitPoints.add(sentences.size()); // ç»“æŸç‚¹

        // æ ¹æ®åˆ‡åˆ†ç‚¹ç”Ÿæˆå—
        for (int i = 0; i < splitPoints.size() - 1; i++) {
            int start = splitPoints.get(i);
            int end = splitPoints.get(i + 1);

            StringBuilder chunkContent = new StringBuilder();
            for (int j = start; j < end; j++) {
                chunkContent.append(sentences.get(j));
                if (j < end - 1) {
                    chunkContent.append(" ");
                }
            }

            String content = chunkContent.toString();

            // åº”ç”¨å¤§å°é™åˆ¶
            if (content.length() >= config.getMinChunkSize() &&
                content.length() <= config.getMaxChunkSize()) {

                DocumentChunk chunk = DocumentChunk.builder()
                        .content(content)
                        .build();
                chunks.add(chunk);
            } else if (content.length() > config.getMaxChunkSize()) {
                // å¤ªå¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                List<DocumentChunk> subChunks = splitLargeChunk(content, config);
                chunks.addAll(subChunks);
            } else if (content.length() < config.getMinChunkSize() && !chunks.isEmpty()) {
                // å¤ªå°ï¼Œåˆå¹¶åˆ°å‰ä¸€ä¸ªå—
                DocumentChunk lastChunk = chunks.getLast();
                lastChunk.setContent(lastChunk.getContent() + " " + content);
            } else if (!content.trim().isEmpty()) {
                // ç¬¬ä¸€ä¸ªå—ï¼Œå³ä½¿å°ä¹Ÿä¿ç•™
                DocumentChunk chunk = DocumentChunk.builder()
                        .content(content)
                        .build();
                chunks.add(chunk);
            }
        }

        return chunks;
    }

    /**
     * åˆ†å‰²è¿‡å¤§çš„å—
     * (Split chunks that are too large)
     *
     * ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹å¼åˆ†å‰²ï¼Œæ”¯æŒé‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
     * (Use sliding window approach with overlap to maintain context continuity)
     *
     * @param content å†…å®¹ (content)
     * @param config åˆ†å—é…ç½® (chunk configuration)
     * @return åˆ†å‰²åçš„å—åˆ—è¡¨ (list of split chunks)
     */
    private List<DocumentChunk> splitLargeChunk(String content, ChunkConfig config) {
        List<DocumentChunk> chunks = new ArrayList<>();
        int maxSize = config.getMaxChunkSize();
        int overlapSize = config.getOverlapSize();

        for (int i = 0; i < content.length(); i += maxSize - overlapSize) {
            int end = Math.min(i + maxSize, content.length());
            String chunkContent = content.substring(i, end);

            DocumentChunk chunk = DocumentChunk.builder()
                    .content(chunkContent)
                    .build();
            chunks.add(chunk);

            if (end >= content.length()) {
                break;
            }
        }

        return chunks;
    }

    /**
     * æ£€æµ‹æ–‡æœ¬ä¸­çš„å›¾ç‰‡æ ‡è®°ä½ç½®
     * Detect image markers in text
     *
     * å›¾ç‰‡æ ‡è®°æ ¼å¼ï¼š[å›¾ç‰‡-xxxï¼šæè¿°] æˆ– [å›¾ç‰‡-xxx.pngï¼šæè¿°]
     *
     * @param sentences å¥å­åˆ—è¡¨
     * @return åŒ…å«å›¾ç‰‡æ ‡è®°çš„å¥å­ç´¢å¼•é›†åˆ
     */
    private Set<Integer> detectImageMarkers(List<String> sentences) {
        Set<Integer> imagePositions = new HashSet<>();

        for (int i = 0; i < sentences.size(); i++) {
            String sentence = sentences.get(i);

            // æ£€æµ‹å›¾ç‰‡æ ‡è®°æ ¼å¼ï¼š[å›¾ç‰‡-xxxï¼š
            if (sentence.contains("[å›¾ç‰‡-") || sentence.contains("[Image-")) {
                imagePositions.add(i);
                log.debug("   ğŸ–¼ï¸ å¥å­ {} åŒ…å«å›¾ç‰‡æ ‡è®°", i);
            }
        }

        return imagePositions;
    }

    /**
     * åˆ¤æ–­ä½ç½®æ˜¯å¦é è¿‘å›¾ç‰‡æ ‡è®°
     * Check if position is near image markers
     *
     * ç­–ç•¥ï¼šå›¾ç‰‡å‰åå„2ä¸ªå¥å­èŒƒå›´å†…éƒ½è®¤ä¸ºæ˜¯"é è¿‘"
     *
     * @param position å½“å‰ä½ç½®
     * @param imagePositions å›¾ç‰‡ä½ç½®é›†åˆ
     * @return æ˜¯å¦é è¿‘å›¾ç‰‡
     */
    private boolean isNearImagePosition(int position, Set<Integer> imagePositions) {
        if (imagePositions.isEmpty()) {
            return false;
        }

        // æ£€æŸ¥å‰å2ä¸ªå¥å­èŒƒå›´
        int range = 2;
        for (int offset = -range; offset <= range; offset++) {
            if (imagePositions.contains(position + offset)) {
                return true;
            }
        }

        return false;
    }

    /**
     * åŸºäº PPL çš„æ–‡æ¡£é‡æ’åº
     * (PPL-based document reranking)
     *
     * ä½¿ç”¨å›°æƒ‘åº¦å¯¹å€™é€‰æ–‡æ¡£é‡æ–°æ’åºï¼ŒPPL è¶Šä½çš„æ–‡æ¡£æ’åè¶Šé å‰
     * (Rerank candidate documents using perplexity, lower PPL ranks higher)
     *
     * @param question ç”¨æˆ·é—®é¢˜ (user question)
     * @param candidates å€™é€‰æ–‡æ¡£åˆ—è¡¨ (list of candidate documents)
     * @param config é‡æ’åºé…ç½® (rerank configuration)
     * @return é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨ (reranked document list)
     * @throws PPLException PPLè®¡ç®—å¼‚å¸¸ (PPLè®¡ç®—å¼‚å¸¸)
     */
    @Override
    public List<Document> rerank(String question, List<Document> candidates, RerankConfig config) throws PPLException {
        if (candidates == null || candidates.isEmpty()) {
            return candidates;
        }

        long startTime = System.currentTimeMillis();

        try {
            // 1. é€‰æ‹©å‰ K ä¸ªæ–‡æ¡£è¿›è¡Œé‡æ’åº (Select top K documents for reranking)
            int topK = Math.min(config.getTopK(), candidates.size());
            List<Document> toRerank = candidates.subList(0, topK);
            List<Document> remaining = candidates.subList(topK, candidates.size());

            // 2. è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ PPL åˆ†æ•° (Calculate PPL score for each document)
            List<DocumentWithScore> scoredDocs = new ArrayList<>();

            for (Document doc : toRerank) {
                String content = doc.getContent();

                // æˆªæ–­å†…å®¹ä»¥æ§åˆ¶æˆæœ¬ (Truncate content to control cost)
                if (content.length() > config.getContentTruncateLength()) {
                    content = content.substring(0, config.getContentTruncateLength());
                }

                // è®¡ç®— PPL (Calculate PPL)
                double ppl = calculatePerplexity(content);

                // PPL è½¬æ¢ä¸ºåˆ†æ•°ï¼šåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼ŒPPL è¶Šä½è¶Šå¥½
                // (Convert PPL to score: higher score is better, lower PPL is better)
                double pplScore = 1.0 / (1.0 + ppl);

                // è·å–åŸå§‹æ£€ç´¢åˆ†æ•°ï¼ˆä»æ··åˆæ£€ç´¢ä¼ é€’è¿‡æ¥ï¼‰
                // (Get original retrieval score, passed from hybrid search)
                double originalScore = doc.getScore() != null ? doc.getScore() : 1.0;

                // æ··åˆè¯„åˆ†ï¼šfinal = (1-weight) * original + weight * ppl_score
                // (Hybrid scoring: final = (1-weight) * original + weight * ppl_score)
                double weight = config.getWeight();
                double finalScore = (1 - weight) * originalScore + weight * pplScore;

                // è®°å½•æ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯• (Log for debugging)
                log.debug(I18N.get("ppl_onnx.log.rerank_detail",
                    doc.getTitle(),
                    String.format("%.3f", originalScore),
                    String.format("%.2f", ppl),
                    String.format("%.3f", pplScore),
                    String.format("%.3f", finalScore)));

                scoredDocs.add(new DocumentWithScore(doc, finalScore));
            }

            // 3. é‡æ–°æ’åº (Re-sort)
            scoredDocs.sort((a, b) -> Double.compare(b.score, a.score));

            // 4. åˆå¹¶ç»“æœ (Merge results)
            List<Document> reranked = scoredDocs.stream()
                    .map(ds -> ds.document)
                    .collect(Collectors.toList());
            reranked.addAll(remaining);

            metrics.recordSuccess(System.currentTimeMillis() - startTime);
            return reranked;

        } catch (Exception e) {
            metrics.recordFailure(System.currentTimeMillis() - startTime);
            throw new PPLException(PPLProviderType.ONNX,
                    I18N.get("ppl_onnx.error.rerank_failed"), e);
        }
    }

    /**
     * æ–‡æ¡£å’Œåˆ†æ•°çš„åŒ…è£…ç±»
     * (Wrapper class for document and score)
     */
    private static class DocumentWithScore {
        final Document document;  // æ–‡æ¡£ (document)
        final double score;       // åˆ†æ•° (score)

        DocumentWithScore(Document document, double score) {
            this.document = document;
            this.score = score;
        }
    }

    /**
     * è·å–æœåŠ¡æä¾›è€…ç±»å‹
     * (Get service provider type)
     */
    @Override
    public PPLProviderType getProviderType() {
        return PPLProviderType.ONNX;
    }

    /**
     * å¥åº·æ£€æŸ¥
     * (Health check)
     *
     * éªŒè¯ ONNX ä¼šè¯å’Œåˆ†è¯å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œ
     * (Verify ONNX session and tokenizer are working properly)
     *
     * @return æ˜¯å¦å¥åº· (whether healthy)
     */
    @Override
    public boolean isHealthy() {
        try {
            // æ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦å·²åˆå§‹åŒ– (Check if key components are initialized)
            if (session == null || tokenizer == null) {
                return false;
            }

            // å°è¯•è®¡ç®—ä¸€ä¸ªç®€å•æ–‡æœ¬çš„ PPL (Try to calculate PPL for simple text)
            String testText = "Hello";
            double ppl = calculatePerplexity(testText);

            // PPL åº”è¯¥æ˜¯ä¸€ä¸ªåˆç†çš„æ­£æ•° (PPL should be a reasonable positive number)
            boolean healthy = ppl > 0 && ppl < 10000;
            if (healthy && useKVCache) {
                log.info("âœ… ONNX æ¨¡å‹ä½¿ç”¨ KV Cacheï¼Œå·²æˆåŠŸæ”¯æŒï¼ˆä½¿ç”¨ DirectBuffer åˆ›å»ºç©ºå¼ é‡ï¼‰");
            }
            return healthy;

        } catch (Exception e) {
            log.warn(I18N.get("ppl_onnx.log.health_check_failed"), e);
            return false;
        }
    }

    /**
     * è·å–æ€§èƒ½æŒ‡æ ‡
     * (Get performance metrics)
     */
    @Override
    public PPLMetrics getMetrics() {
        return metrics;
    }

    /**
     * é”€æ¯æœåŠ¡ï¼Œé‡Šæ”¾èµ„æº
     * (Destroy service and release resources)
     */
    @PreDestroy
    public void destroy() {
        log.info(I18N.get("ppl_onnx.log.shutdown_start"));

        try {
            // é‡Šæ”¾ ONNX Session (Release ONNX Session)
            if (session != null) {
                session.close();
                log.info(I18N.get("ppl_onnx.log.session_closed"));
            }

            // å…³é—­ Tokenizer (Close Tokenizer)
            if (tokenizer != null) {
                tokenizer.close();
                log.info(I18N.get("ppl_onnx.log.tokenizer_closed"));
            }

            // æ¸…ç†ç¼“å­˜ (Clear cache)
            if (pplCache != null) {
                pplCache.invalidateAll();
                log.info(I18N.get("ppl_onnx.log.cache_cleared"));
            }

            log.info(I18N.get("ppl_onnx.log.shutdown_success"));

        } catch (Exception e) {
            log.error(I18N.get("ppl_onnx.log.shutdown_error"), e);
        }
    }
}


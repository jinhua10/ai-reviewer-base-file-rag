log:
  onnx:
    model_input_info: "üìä Model Input Info:"
    input_info: "  - Input: {0} (Type: {1})"
    kv_cache_dimensions: "  üìê Extracted KV Cache dimensions from model: num_heads={0}, head_dim={1}"
    model_output_info: "üìä Model Output Info:"
    output_info: "  - Output: {0} (Type: {1})"
    using_kv_cache: "‚ö†Ô∏è Model uses KV Cache, total {0} layers, num_heads={1}, head_dim={2}"
    cannot_extract_kv_cache_dimensions: "‚ö†Ô∏è Cannot extract KV Cache dimensions from model, using default values"
    not_using_kv_cache: "‚úÖ Model does not use KV Cache, can inference directly"
    cannot_get_model_info: "‚ö†Ô∏è Cannot get model info: {0}"
    added_kv_cache_layers: "‚úÖ Added {0} layers of empty KV Cache"
    detected_image_positions: "   üñºÔ∏è Detected {0} image position markers"
    near_image_position: "   üìç Position {0} near image, PPL weight reduced to {1}"
    sentence_contains_image_marker: "   üñºÔ∏è Sentence {0} contains image marker"
    kv_cache_success: "‚úÖ ONNX model uses KV Cache, successfully supported (using DirectBuffer to create empty tensors)"
  admin:
    cache_cleared: "Cache cleared"
    cache_cleared_success: "Cache cleared successfully"
    clear_cache_failed: "Failed to clear cache"
    clear_cache_failed_detail: "Failed to clear cache: {0}"
    index_optimize_failed: "Index optimization failed"
    index_optimize_failed_detail: "Index optimization failed: {0}"
    index_optimized: "Index optimized"
    index_optimized_success: "Index optimized successfully"
    stats_failed: "Failed to get statistics"
    stats_failed_detail: "Failed to get statistics: {0}"
  app:
    started: "‚úÖ Application started successfully, address: {0}"
    start_failed: "‚ùå Application startup failed"
  api:
    stopped: "API server stopped"
  audit:
    init_failed: "Audit log initialization failed"
    logged: "Audit log recorded: {0}"
    write_failed: "Audit log write failed"
  auth:
    logged_in: "User login: {0}"
    logged_out: "User logout: {0}"
    registered: "User registration: {0}"
  session:
    create: "Creating session: id={0}, total docs={1}, docs per page={2}"
    no_next: "Session has no next page: {0}"
    next: "Session {0} advanced to offset {1}"
    no_prev: "Session has no previous page: {0}"
    prev: "Session {0} back to offset {1}"
    invalid_page: "Invalid page number"
    page_out_of_range: "Requested page number {0} out of range (total pages={1})"
    goto_page: "Go to page {0} (session {1}, offset {2})"
    deleted: "Session deleted: {0}"
    cleaned: "Cleaned expired sessions: {0}"
    cleaned_count: "Cleaned {0} expired sessions"
    not_found: "Session not found: {0}"
    evicted: "üìä Session count exceeded limit, evicted oldest session: {0} (current {1}/{2})"
  cache:
    init: "‚úÖ Retrieval cache initialized (max capacity: {0}, TTL: {1} minutes)"
    hit: "üéØ Cache hit: {0}"
    miss: "üì≠ Cache miss: {0} (retrieval time {1}ms)"
    cleared: "üóëÔ∏è All cache cleared"
    search_cleared: "üóëÔ∏è Search cache cleared"
  ppl:
    rerank_start: "üîÑ Starting PPL Rerank, total {0} documents..."
    rerank_completed: "‚úÖ PPL Rerank completed, time: {0}ms"
    rerank_failed: "‚ö†Ô∏è PPL Rerank failed, using original ranking: {0}"
  hybrid:
    found_docs: "Hybrid search found {0} documents"
    keyword_search: "Executing hybrid keyword search: {0}"
    completed: "Hybrid search completed: time {0}ms"
    detail_item: "{0}. {1} (score={2})"
    extract_keywords: "Extracting keywords: {0}"
    filtered: "Filtered {0} items"
    lucene_found: "Lucene search found {0} documents"
    lucene_top_header: "Lucene _top results:"
    lucene_top_item: "{0}. {1} (luceneScore={2})"
    top5_header: "Combined _top5 results:"
    top5_item: "{0}. {1} (score={2})"
    topk_header: "Top K results:"
    vector_found: "Vector search found {0} documents"
    feedback_weight_applied: "üìä Applied feedback weight adjustments for {0} documents"
    feedback_weight_detail: "üìä Document weight adjustment: {0} | Original score: {1} | Weight: {2} | Adjusted: {3}"
    query_expanded: "üîç Query expansion: {0} -> {1}"
    expand_failed: "‚ö†Ô∏è Query expansion failed, using original query: {0}"
  similar:
    found: "Found {0} similar documents"
    init: "‚úÖ SimilarQAService initialized, loaded {0} stopwords"
    query_empty: "Query question is empty, skipping similar question retrieval"
    service_not_init: "QARecordService not initialized, skipping similar question retrieval"
    no_keywords: "Query question has no valid keywords: {0}"
    keywords: "Query keywords: {0}"
    config_missing: "‚ö†Ô∏è SimilarQAConfig not configured, using default values"
    no_history: "No historical Q&A records"
    failed: "Similar question retrieval failed"
    cache_init: "‚úÖ Similar question cache initialized (max capacity: {0}, TTL: {1} minutes)"
    cache_hit: "üéØ Similar question cache hit: {0}"
    cache_cleared: "üóëÔ∏è Similar question cache cleared"
  query_expansion:
    init: "‚úÖ Query expansion service initialized (synonyms: {0} groups, stopwords: {1} items)"
    reverse_index: "‚úÖ Synonym reverse index built ({0} entries)"
    start: "üîç Starting query expansion: {0}"
  chunk:
    ai_completed: "AI semantic chunking completed: {0} characters -> {1} chunks, time: {2}ms"
    ai_failed: "AI semantic chunking failed, downgrading to smart keyword chunking"
    ai_not_enabled: "AI chunking not enabled in configuration"
    ai_parse_failed: "Failed to parse AI semantic chunking response: {0}"
    ai_start: "Starting AI semantic chunking, content length: {0} characters"
    content_truncate: "Content too large ({0} characters), truncated to {1} characters to prevent memory overflow"
    coverage_low: "Coverage is low, adding sequential chunks"
    delete_failed: "File deletion failed: {0}"
    deleted_all: "All chunks of document deleted: {0}"
    keywords_extracted: "Extracted {0} keywords from query: {1}"
    max_chunks_reached: "Maximum chunk limit reached ({0}), stopping chunking"
    no_keywords_fallback: "No keywords found, downgrading to simple chunking"
    read_meta_failed: "Failed to read chunk metadata: {0}"
    save_failed: "Document chunk save failed: {0} (index: {1})"
    saved: "{0} document chunks saved to document: {1}"
    simple_summary: "Simple chunking: {0} characters -> {1} chunks (size={2}, overlap={3})"
    smart_summary: "Smart keyword chunking: {0} characters -> {1} chunks, {2} keywords"
    storage:
      created: "Document chunk storage directory created: {0}"
      init_failed: "Document chunk storage initialization failed"
    truncate_warning: "Content too long ({0} characters), truncated to {1} characters for AI chunking"
  chunker:
    ai_disabled: "AI chunking not enabled, downgrading to SMART_KEYWORD strategy"
    creating: "Creating document chunker: strategy={0}, chunk size={1}, overlap={2}"
    llm_null: "LLM client is null, downgrading to SMART_KEYWORD strategy"
    unknown_strategy: "Unknown chunking strategy: {0}, using SMART_KEYWORD"
  docs:
    classpath_in_jar: "Classpath is inside JAR package, writing not supported"
    classpath_load_failed: "Failed to load resource from classpath: {0} - {1}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    classpath_realpath: "Using classpath real path: {0}"
    classpath_resource_found: "‚úÖ Found resource from classpath: {0}"
    create:
      failed: "Failed to create document: {0}"
      success: "Document created successfully"
    create_failed: "Failed to create document directory: {0}"
    created: "Document created: {0}"
    delete:
      failed: "Failed to delete document: {0}"
      success: "Document deleted successfully"
    deleted: "Document deleted: {0}"
    directory_ready: "Document directory ready: {0}"
    file_exists_renamed: "File exists, renamed to: {0}"
    get:
      failed: "Failed to get document: {0}"
    invalid_directory: "Invalid directory: {0}"
    list:
      exception: "Exception occurred while listing documents: {0}"
      failed: "Failed to list documents: {0}"
    not_found: "Document not found: {0}"
    process_failed: "File processing failed: {0}"
    saved: "Document saved: {0}"
    scanned_types: "Scanned file types: {0}"
    total: "Total documents: {0}"
    update:
      failed: "Failed to update document: {0}"
      success: "Document updated successfully"
    updated: "Document updated: {0}"
    upload_to_external: "Uploaded documents will be saved to external path: ./data/documents"
    using_default_path: "Using default path: ./data/documents"
    using_filesystem: "Using filesystem path: {0}"
    walk_failed: "Directory traversal failed: {0}"
  factory:
    create_caffeine: "Creating Caffeine cache engine"
    create_filesystem: "Creating filesystem storage engine"
    create_lucene: "Creating Lucene index engine"
  feedback:
    document_failed: "Document feedback processing failed"
    document_received: "Received document feedback {0}: recordId={1}, document={2}"
    get_pending_failed: "Failed to get pending Q&A records"
    get_record_failed: "Failed to get Q&A record"
    get_recent_failed: "Failed to get recent Q&A records"
    get_statistics_failed: "Failed to get Q&A statistics"
    load_failed: "Failed to load document weights"
    overall_failed: "Overall feedback processing failed"
    overall_received: "Received overall feedback: recordId={0}, rating={1}"
    overall_rating_submitted: "Submitted overall rating {0} [{1}]: {2} stars"
    rating_failed: "Rating processing failed"
    rating_submitted: "Submitted rating {0} [{1}]: {2} - {3} stars (impact: {4})"
    rating_updated: "üìä Document weight updated (via rating): {0} -> weight={1} ({2} stars, adjustment {3}, üëç{4} üëé{5})"
    save_failed: "Failed to save document weights"
    weight_disabled: "Dynamic weight adjustment disabled"
    weight_reset: "üîÑ Document weight reset: {0} -> {1}"
    weight_updated: "üìä Document weight updated: {0} -> weight={1} (üëç{2} üëé{3})"
    weights_cleared: "üßπ All document weights cleared"
    weights_loaded: "üìÇ Loaded document weights: {0} documents"
    weights_saved: "üíæ Saved document weights: {0} documents"
    weights_file_not_exists: "üìÇ Document weights file not found, using default weights"

  imageproc:
    llm_fallback_vision: "‚ö†Ô∏è  Main LLM does not support image input, falling back to vision-llm configuration"
    check_llm_support: "Checking if LLM client supports images: model={0}, supports images={1}"
    tessdata_default: "System default"
    activated: "‚úÖ Image processing strategy activated: {0}"
    add_ocr: "   Adding OCR strategy:"
    add_vision: "   Adding Vision LLM strategy:"
    add_llm_vision: "‚úÖ Adding LLM Vision strategy (reusing main LLM client)"
    init: "üñºÔ∏è  Initializing image processing functionality..."
    extraction_mode: "üì∏ Image extraction mode: {0}"
    language: "      - Recognition language: {0}"
    llm_vision_client_type: "   - Client type: {0}"
    llm_vision_model: "   - Model: {0}"
    llm_vision_no_client: "‚ö†Ô∏è  Cannot add LLM Vision strategy: LLMClient not initialized"
    llm_vision_no_image_support: "‚ö†Ô∏è  Cannot add LLM Vision strategy: model {0} does not support image input"
    llm_vision_available: "‚úÖ LLM Vision strategy available"
    llm_vision_unavailable: "‚ö†Ô∏è  LLM Vision strategy unavailable"
    ocr_available: "   ‚úÖ OCR strategy available"
    ocr_hint: "   üí° Hint: Add dependency net.sourceforge.tess4j:tess4j:5.9.0"
    ocr_unavailable: "   ‚ö†Ô∏è OCR strategy unavailable: missing tess4j dependency"
    placeholder: "   Using placeholder strategy (default)"
    strategy: "   Configured strategy: {0}"
    strategy_selected: "‚úÖ Selected image processing strategy: {0}"
    strategy_none: "‚ö†Ô∏è  No available image processing strategies, using placeholder"
    extract_failed: "Image content extraction failed: {0}"
    extract_error: "[Image: {0} - Extraction failed]"
    image_placeholder: "[Image: {0}]"
    tessdata: "      - Tessdata path: {0}"
    vision_apikey_hint: "   üí° Hint: Set environment variable VISION_LLM_API_KEY or configure knowledge.qa.image-processing.vision-llm.api-key"
    vision_available: "   ‚úÖ Vision LLM strategy available"
    vision_endpoint: "      - Endpoint: {0}"
    vision_model: "      - Model: {0}"
    vision_no_apikey: "   ‚ö†Ô∏è Vision LLM enabled but API Key not configured"
    vision_unavailable: "   ‚ö†Ô∏è Vision LLM strategy unavailable"
    position:
      simple_desc: "Image {0}: {1}"
      full_desc: "Image {0}: {1} (Position: {2}, Coordinates: {3},{4}, Size: {5}x{6})"
      top: "Top"
      bottom: "Bottom"
      middle: "Middle"
      left: "Left"
      right: "Right"
      center: "Center"
  office:
    pptx_start: "Starting PPTX file processing: {0}, total {1} slides"
    pptx_complete: "‚úÖ PPTX processing complete: {0}"
    pptx_failed: "Failed to process PPTX file: {0}"
    docx_start: "Starting DOCX file processing: {0}"
    docx_complete: "‚úÖ DOCX processing complete: {0}"
    docx_failed: "Failed to process DOCX file: {0}"
    xlsx_start: "Starting XLSX file processing: {0}, total {1} worksheets"
    xlsx_complete: "‚úÖ XLSX processing complete: {0}"
    xlsx_failed: "Failed to process XLSX file: {0}"
    slide_title: "\n\n========== Slide {0} ==========\n"
    slide_text: "„ÄêText Content„Äë\n"
    sheet_title: "\n\n========== Worksheet: {0} ==========\n"
    image_section: "\n\n========== Document Images ==========\n"
    image_content: "\n„ÄêImage Content„Äë\n"
    extract_image: "üì∑ Extracting image: {0} ({1}KB)"
    extract_success: "‚úÖ Image content extracted successfully: {0} -> {1} characters"
    extract_empty: "‚ö†Ô∏è  Image content is empty: {0}"
    slide_images: "Slide {0} contains {1} images"
    sheet_images: "Worksheet {0} contains {1} images"
    docx_images: "‚úÖ Extracted {0} images from DOCX"
    xlsx_extract_failed: "Failed to extract XLSX images"
    process_failed: "\n[Processing failed: {0}]\n"
    batch_config: "Batch processing configuration: processing {0} slides per batch"
    processing_slides: "üì¶ Processing slides {0}-{1}/{2}"
    use_cache: "üíæ Using cache: slide {0} ({1} images)"
    need_process: "üì∏ Need to process {0} images (from {1} slides)"
    batch_complete: "‚úÖ Batch analysis complete: {0} images -> {1} characters"
    cache_stats: "üíæ Cache stats: cached {0} images, newly processed {1} images, total {2} images"
    save_image: "üíæ Saving image: {0} -> {1}/{2}"
    save_image_failed: "Failed to save image: {0} - {1}"
  kb:
    vector_enabled: "Vector retrieval enabled."
    progress: "Processing progress:"
    path_not_exists: "Path does not exist: {0}"
    scan_classpath: "Scanning classpath resources: {0}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    resource_found: "Resource found: {0}"
    resource_file_not_exists: "Resource file does not exist: {0}"
    resource_path: "Resource path: {0}"
    scan_directory: "Scanning files in directory."
    files_found: "Total {0} files found."
    add_file: "Adding file: {0}"
    scan_classpath_failed: "Failed to scan classpath resources: {0}"
    file_not_exists: "File does not exist: {0}"
    index_single_file: "Starting to index single file: {0}"
    file_indexed: "File indexed successfully: {0}"
    index_file_failed: "Failed to index file: {0}"
    file_too_large: "File too large, size: {0} MB, max allowed size: {1} MB"
    content_empty: "Document content is empty."
    image_extraction_failed: "Image extraction failed: {0}"
    force_chunk: "Document content too large, forced chunking, size: {0} MB"
    auto_chunk: "Document content large, auto chunking, size: {0} KB"
    chunked: "Document chunked, chunk count: {0}"
    processing_failed: "Document processing failed."
    file_process_failed: "Failed to process file: {0}"
    batch_task_failed: "Batch task execution failed."
    vector_generation_failed: "Vector generation failed: {0}"
    incremental_start: "Starting incremental indexing."
    incremental_complete: "Incremental indexing complete."
    incremental_files: "Incremental indexing file count: {0}"
    build_complete: "Knowledge base build complete"
    build_failed: "Knowledge base build failed"
    build_memory: "Knowledge base build memory usage"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build successful"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total processed items: {0}"
    memory_after: "Memory usage after build: {0} MB"
    memory_before: "Memory usage before build: {0} MB"
    old_kb_cleared: "Old knowledge base cleared"
    rebuild_prepare: "Preparing knowledge base rebuild..."
    tracking_cleared: "Knowledge base tracking info cleared"
    tracking_saved: "Knowledge base tracking info saved: {0}"
    batch_processing: "üì¶ Batch processing: {0} documents ({1} / {2})"
    batch_commit: "üìù Committing batch index"
    content_extracted: "‚úÖ Content extraction complete: {0}"
    content_too_large: "‚ö†Ô∏è  Content too large ({0} characters)"
    content_truncated: "‚úÇÔ∏è  Content truncated to {0} characters"
    exists: "üìö Detected existing knowledge base ({0} documents)"
    files_to_index: "üìù Files to index: {0}"
    final_batch: "üì¶ Processing final batch: {0} documents"
    first_create: "üìö First time creating knowledge base"
    found_files: "‚úÖ Found {0} document files"
    gc_before: "üóëÔ∏è  Executing garbage collection to release memory"
    hint_put_docs: "üí° Hint: Please place documents in {0} directory"
    images_added: "‚ûï {0} images added to index"
    images_extracted: "üñºÔ∏è  Extracted {0} images"
    incremental_done: "\n‚úÖ Incremental indexing complete!"
    incremental_failed: "‚ùå Incremental indexing failed"
    incremental_stats: "   - Processed files: {0}/{1}, failed: {2}, total documents: {3}, time: {4} seconds"
    indexation_complete: "‚úÖ Indexing complete: {0}"
    indexing_complete: "‚úÖ Knowledge base indexing complete: {0}"
    no_documents: "‚ö†Ô∏è  No supported document files found"
    parallel_mode: "üöÄ Using parallel processing mode ({0} threads)"
    parallel_progress: "üìä Parallel progress: {0}%"
    parallel_memory: "üíæ Memory usage: {0} MB"
    processing_file: "üìÑ Processing file: {0}, size: {0} KB"
    processing_start: "\nüìù Starting document processing..."
    scanning: "üìÇ Scanning documents: {0}"
    serial_mode: "üìù Using serial processing mode"
    source_path: "   - Document path: {0}"
    supported_formats: "      Supported formats: {0}"
    up_to_date: "‚úÖ All files are up to date, no updates needed"
    vector_init_failed: "‚ùå Vector search engine initialization failed"
    files_to_update: "üìù Files to update: {0}"
    saved_chunks: "‚úÖ Saved {0} chunks for document {1}"
    save_chunks_failed: "‚ö†Ô∏è  Failed to save chunks (document: {0}): {1}"
    preprocess_start: "üîÑ Starting document preprocessing (image extraction + text conversion)..."
    preprocess_complete: "‚úÖ Document preprocessing complete, final content length: {0}"
    preprocess_failed: "‚ö†Ô∏è Document preprocessing failed: {0}"
    ppl_chunking_start: "üß† Using PPL intelligent chunking..."
    ppl_chunking_complete: "‚úÖ PPL chunking complete: {0} chunks"
    ppl_chunking_failed: "‚ö†Ô∏è PPL chunking failed, falling back to traditional chunking: {0}"
    file_item: "   - {0}"
  kqa:
    response_time: "\n‚è±Ô∏è  Response time: {0}ms"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build successful"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total processed items: {0}"
    memory_after: "Memory usage after build: {0} MB"
    memory_before: "Memory usage before build: {0} MB"
    tracking_saved: "Knowledge base tracking info saved: {0}"
    vector_enabled: "Vector indexing enabled: {0}"
    sep: "================================================================================="
    # Supplement missing kb/log keys
    build_complete: "Knowledge base build complete: {0}"
    answer_header: "\nüí° Answer:"
    build_failed: "Knowledge base build failed: {0}"
    docs_dir_missing: "‚ö†Ô∏è Documents directory does not exist: {0}"
    incremental_complete: "Incremental indexing complete"
    incremental_error: "Incremental indexing error"
    incremental_failed: "Incremental indexing failed: {0}"
    incremental_mode: "   üîÑ Starting knowledge base incremental indexing..."
    init_done: "‚úÖ Knowledge base Q&A system initialized successfully!"
    init_failed: "‚ùå Knowledge Q&A service initialization failed"
    init_kb: "Initializing knowledge base"
    init_start: "üöÄ Initializing knowledge Q&A service"
    kb_not_initialized: "Knowledge base not initialized"
    load_chunks_images_failed: "Failed to load chunks/images info"
    more_docs_available: "‚ÑπÔ∏è Another {0} related documents not included in this answer"
    rebuild_complete: "Knowledge base rebuild successful"
    rebuild_error: "Knowledge base rebuild error"
    rebuild_failed: "Knowledge base rebuild failed: {0}"
    rebuild_mode: "   üöÄ Starting knowledge base full rebuild..."
    recover_failed: "Knowledge base recovery failed"
    recover_kb: "Recovering knowledge base"
    reinit_complete: "Knowledge base re-initialization successful"
    reinit_kb: "Re-initializing knowledge base"
    scanned_files_count: "üìÇ File system scan complete, found {0} supported documents"
    scan_failed: "‚ùå File system scan failed"
    source_path: "   - Source path: {0}"
    sources_header: "\nüìö Data sources (total {0} documents):"
    step: "\nüöÄ Step {0}: {1}"
    storage_path: "   - Storage path: {0}"
    system_not_initialized: "Knowledge Q&A system not initialized"
    used_docs_count: "üìö Used {0} documents to generate this answer"
  llm:
    api_key_deepseek: "      - DeepSeek: export AI_API_KEY=your-deepseek-key"
    api_key_hint: "üí° Hint: Set environment variable:"
    api_key_missing: "‚ö†Ô∏è LLM API Key not configured"
    api_key_openai: "      - OpenAI: export OPENAI_API_KEY=your-openai-key"
    client_created: "ü§ñ Created {0} LLM client"
    fallback_mock: "üí° Downgraded to Mock mode"
    model: "   - Model: {0}"
    api_url: "   - API: {0}"
    mock_created: "ü§ñ Created Mock LLM client (for testing only)"
    mock_hint: "üí° To use real LLM, please configure:"
    mock_provider: "      knowledge.qa.llm.provider=openai"
    mock_apikey: "      And set corresponding API Key and URL"
    mock_warning: "‚ö†Ô∏è Mock mode will return fixed simulated answers"
  memory:
    usage: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}%)"
    usage_phase: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}%)"
    warning: "Phase {0} - Memory usage too high: {1}%"
    gc_trigger: "‚ö†Ô∏è  Memory usage too high ({0}%), triggering garbage collection"
    gc_done: "‚úÖ Garbage collection complete: {0}% -> {1}%"
  optimization:
    chunker:
      batch_completed: "Batch chunking complete: {0} documents -> {1} chunks"
      chunked: "Document {0} chunked into {1} parts"
      initialized: "DocumentChunker initialized - chunk size: {0}, overlap: {1}, smart split: {2}, max content length: {3}, max chunks: {4}"
    context:
      built: "Smart context built: {0} characters from {1} documents ({2}% of max limit)"
      initialized_with_chunker: "SmartContextBuilder initialized (with chunker): strategy={0}, max context={1} characters, max doc={2} characters, storage={3}"
      remaining_chars: "\n[... Another {0} characters not displayed, content extracted by keyword priority]"
    memory:
      gc_freed: "Garbage collection complete, approximately freed {0}MB memory"
      gc_no_freed: "Garbage collection complete, no significant memory freed"
      suggest_gc: "Suggest garbage collection, current memory usage: {0}MB"
  search_dispatcher:
    init: "‚úÖ Retrieval strategy dispatcher initialized, total {0} strategies"
    no_strategies: "‚ö†Ô∏è No retrieval strategies registered"
    strategy_registered: "  ‚úÖ Registered strategy: {0} - {1} (Priority: {2})"
    selected_strategy: "üéØ Selected retrieval strategy: {0} ({1})"
    search_completed: "‚úÖ Strategy {0} retrieval complete, found {1} documents, time {2}ms"
    search_failed: "‚ùå Strategy {0} retrieval failed"
    no_suitable_strategy: "‚ö†Ô∏è No suitable retrieval strategy found"
    strategy_not_found: "‚ö†Ô∏è Strategy {0} not found"
    using_strategy: "üìç Using specified strategy: {0}"
    fallback: "‚ö†Ô∏è  Falling back to default strategy: {0}"
    fallback_failed: "‚ùå Fallback strategy also failed"
    strategy_score: "   Strategy {0} applicability score: {1}"
    evaluate_failed: "‚ö†Ô∏è  Failed to evaluate strategy {0}"
    default_changed: "üìç Default strategy changed to: {0}"
    threshold_changed: "üìç Applicability threshold changed to: {0}"
    strategy_added: "‚úÖ Dynamically registered strategy: {0}"
  score_fusion:
    init: "‚úÖ Score fusion service initialized, total {0} contributors"
    no_contributors: "‚ö†Ô∏è No score contributors registered"
    contributor_registered: "  ‚úÖ Registered contributor: {0} (Weight: {1}, Priority: {2})"
    no_enabled_contributors: "‚ö†Ô∏è No enabled score contributors"
    zero_weight: "‚ö†Ô∏è All contributors have zero weight"
    contributor_done: "   Contributor {0} complete, {1} scores, time {2}ms"
    contributor_failed: "‚ùå Contributor {0} execution failed"
    contributor_added: "‚úÖ Dynamically added contributor: {0}"
    contributor_removed: "‚úÖ Removed contributor: {0}"
  score_contributor:
    lucene:
      done: "   Lucene scoring complete, {0} documents"
    vector:
      no_engine: "Vector engine unavailable, skipping vector scoring"
      done: "   Vector scoring complete, {0} documents"
    feedback:
      disabled: "Feedback service unavailable, skipping feedback scoring"
      done: "   Feedback scoring complete, {0} documents"
  strategy:
    hybrid:
      completed: "‚úÖ Hybrid strategy retrieval complete, {0} documents"
    keyword:
      completed: "‚úÖ Keyword strategy retrieval complete, {0} documents"
    vector:
      no_engine: "Vector engine unavailable"
      completed: "‚úÖ Vector strategy retrieval complete, {0} documents"
  qa:
    archive:
      init: "Initializing Q&A archive service"
    archive_failed: "Q&A archiving failed"
    archived: "High-rated Q&A archived: rating={0}, path={1}"
    document_feedback: "Document feedback {0} [{1}]: {2} - {3}"
    feedback_applied: "Feedback applied to document weight: {0}"
    feedback_pending: "Pending feedback: {0}"
    find_failed: "Failed to find record: {0}"
    load_failed: "Failed to load Q&A records: {0}"
    marked_as_quality: "Q&A record marked as quality content: [{0}]"
    overall_rating_submitted: "Submitted overall rating {0} [{1}]: {2} stars"
    pending_failed: "Failed to get pending records"
    rating_applied: "Star rating applied to document weight: {0} ({1} stars -> adjustment {2})"
    rating_pending: "Pending star rating: {0} ({1} stars)"
    rating_submitted: "Submitted star rating {0} [{1}]: {2} - {3} stars (weight adjustment: {4})"
    recent_failed: "Failed to get recent records"
    record_notfound: "Record not found: {0}"
    record_save_failed: "Failed to save Q&A record"
    record_saved: "Q&A record saved: {0} - {1}"
    record_update_failed: "Failed to update Q&A record: {0}"
    record_updated: "Q&A record updated: {0}"
    records_dir: "Q&A records storage directory: {0}"
    records_dir_failed: "Failed to create Q&A records directory: {0}"
    stats_failed: "Failed to calculate Q&A statistics"
    user_feedback: "User feedback [{0}]: rating={1}, content={2}"
  rag:
    batch_indexed: "Batch indexing complete {0} documents"
    cache_hit: "Query result from cache: {0}"
    closed: "Local file RAG closed successfully"
    close_error: "Error occurred while closing local file RAG"
    closing: "Closing local file RAG..."
    delete_failed: "Failed to delete document: {0}"
    deleted_count: "{0} documents deleted"
    deleting_all: "Deleting all documents..."
    doc_deleted: "Document deleted: {0}"
    doc_indexed: "Document indexing successful: {0}"
    doc_updated: "Document updated: {0}"
    enable_cache: "Cache enabled: {0}"
    enable_compression: "Compression enabled: {0}"
    found_to_delete: "Found {0} documents to delete"
    init: "Local file RAG initialization successful"
    init_done: "Local file RAG initialization successful"
    load_content_failed: "Failed to load document content: {0}"
    loaded_content: "Document content loaded: {0}, length: {1}"
    no_documents: "No documents to delete"
    optimized: "Index optimization complete"
    optimizing: "Optimizing index..."
    search_completed: "Search complete, time: {0}ms, found {1} results"
    simple_init: "Initializing simple RAG service..."
    simple_init_done: "Simple RAG service initialized successfully"
    storage: "Storage path: {0}"
  filetracking:
    loaded: "File tracking loaded, total {0} records."
    load_failed: "Failed to load file tracking: {0}"
    saved: "File tracking saved, total {0} records."
    save_failed: "Failed to save file tracking: {0}"
    check_failed: "Failed to check file tracking: {0}"
    mark_failed: "Failed to mark file as indexed: {0}"
    clear_failed: "Failed to clear file tracking: {0}"
    cleared: "File tracking cleared."
  model:
    checking: "üîç Checking model files..."
    dir_and_file: "üìÇ Model directory and files ready"
    found: "‚úÖ Found model: {0}"
    passed: "‚úÖ Model check passed"
    sep: "===================================================="
  storage:
    ai_image_analyzer_init: "Initializing AI image analyzer: enabled={0}, model={1}"
    chunk_storage_init: "Initializing document chunk storage service, path: {0}"
    document_image_extraction_init: "Initializing document image extraction service: AI analysis={0}"
    image_storage_init: "Initializing image storage service, path: {0}"
  optimizer:
    commit: "Committing optimizer changes"
    done: "Optimization complete"
    optimize: "Optimizing index"
    saving_vectors: "Saving vector index..."
    vectors_saved: "Vector index saved: {0} vectors"
    save_failed: "Vector index save failed"
    embedding_closed: "Embedding engine closed"
  tika:
    init: "Tika document parser initialized."
    max_content: "Maximum content length: {0} MB"
    extract_image_metadata: "Extract image metadata: {0}"
    include_image_placeholders: "Include image placeholders: {0}"
    active_image_strategy: "Active image extraction strategy: {0}"
    vision_batch_size: "Vision LLM batch size: {0} slides/batch"
    slide_cache_enabled: "‚úÖ Slide cache enabled"
    image_storage_enabled: "‚úÖ Image storage service enabled"
    ocr_config: "OCR configuration details:"
    enable_ocr: "Enable OCR: {0}"
    not_set: "Not set"
    ocr_language: "OCR language: {0}"
    ocr_disabled: "OCR disabled: {0}"
    file_not_exists: "File does not exist: {0}"
    detected_mime: "Detected MIME type: {0}, filename: {1}"
    office_pptx: "Processing PPTX file: {0}"
    office_docx: "Processing DOCX file: {0}"
    office_xlsx: "Processing XLSX file: {0}"
    office_done: "Office file processing complete: {0}, content length: {1}"
    parsed_file: "File parsing complete: {0}, content length: {1}"
    parse_failed: "File parsing failed: {0}"
    image_section_start: "\nImage metadata:\n"
    image_item: "Image {0}: {1} = {2}\n"
    embedded_section: "\nEmbedded resources:\n"
    embedded_item: "Embedded resource count: {0}\n"
    image_placeholder: "[Image placeholder {0}]"
    empty_bytes: "Byte array is empty."
    parsed_bytes: "Byte array parsing complete, MIME type: {0}, content length: {1}"
    parse_bytes_failed: "Byte array parsing failed, MIME type: {0}"
    detect_failed: "MIME type detection failed: {0}"
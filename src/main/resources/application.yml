# 知识库问答系统配置文件
# Knowledge QA System Configuration

server:
  port: 8080

  # Tomcat 配置
  tomcat:
    # 临时文件目录
    basedir: ./temp
    # 连接超时
    connection-timeout: 20000
    # 线程配置
    threads:
      max: 200
      min-spare: 10

spring:
  application:
    name: knowledge-qa-system

  # 文件上传配置
  servlet:
    multipart:
      # 单个文件最大大小（默认 1MB，调整为 100MB）
      max-file-size: 100MB
      # 单次请求最大大小（默认 10MB，调整为 100MB）
      max-request-size: 100MB
      # 是否启用文件上传
      enabled: true
      # 临时文件位置（使用自定义目录，避免系统临时目录权限问题）
      location: ./temp/uploads
      # 文件写入磁盘的阈值（0 表示始终写入磁盘）
      file-size-threshold: 0

# 知识库问答系统配置
knowledge:
  qa:
    # 知识库配置
    knowledge-base:
      # 知识库存储路径（索引、元数据等）
      storage-path: ./data/knowledge-base

      # 文档源路径（支持文件夹或单个文件）
      # 示例：
      #   - ./data/documents           # 文件夹
      #   - E:/文档/知识库             # 中文路径
      #   - ./data/documents/file.xlsx # 单个文件
      #   - classpath:simpleExcel      # resources下的目录
      source-path: classpath:simpleExcel

      # 启动时索引模式
      # true: 完全重建（删除旧索引，重新索引所有文件）
      # false: 增量索引（只索引新增和修改的文件，默认推荐）
      # 注意：UI界面可以随时触发完全重建或增量索引
      rebuild-on-startup: false

      # 是否启用缓存
      enable-cache: true

    # 向量检索配置
    vector-search:
      # 是否启用向量检索
      # true: 使用语义向量检索（需要模型文件）
      # false: 使用纯关键词检索
      enabled: true

      # 模型配置
      model:
        # 模型名称（用于日志显示）
        name: paraphrase-multilingual

        # 模型文件路径（相对于 resources）
        # 系统会自动查找以下目录中的模型文件：
        path: /models/paraphrase-multilingual/model.onnx

        # 模型搜索路径（按优先级排序）
        # 系统会按顺序在这些目录中查找模型文件
        search-paths:
          - bge-m3                      # BGE-M3 (推荐, 2024最新)
          - multilingual-e5-large       # Multilingual E5 Large
          - bge-large-zh                # BGE Large ZH (中文)
          - paraphrase-multilingual     # Paraphrase Multilingual
          - text2vec-base-chinese       # Text2Vec (旧版)

        # 模型文件名（按优先级排序）
        file-names:
          - model.onnx                  # 标准模型
          - model_O2.onnx               # 优化模型
          - model_quantized.onnx        # 量化模型
          - model_quint8_avx2.onnx      # AVX2 量化

      # 向量索引存储路径
      index-path: ./data/vector-index

      # 检索相似度阈值 (0.0-1.0)
      # 越高越严格，建议 0.3-0.5
      # 只有相似度高于此阈值的文档才会被返回
      similarity-threshold: 0.5

      # 检索返回的文档数量上限
      # 注意：实际返回数量会受相似度阈值和质量过滤影响
      top-k: 20

      # 文档相关性评分阈值（Lucene + 向量混合评分）
      # 低于此分数的文档会被过滤
      # 建议值：
      #   - 高精度：0.15-0.20
      #   - 平衡：0.10-0.15（默认）
      #   - 高召回：0.05-0.10
      min-score-threshold: 0.10

    # LLM 配置
    llm:
      # LLM 提供商
      # 可选值:
      #   - openai: OpenAI 兼容 API（默认，支持 OpenAI、DeepSeek 等所有兼容服务）
      #   - mock: Mock 模式（测试用，返回固定回答）
      #
      # 说明: OpenAI 客户端支持所有 OpenAI API 兼容的服务
      #       通过配置不同的 api-url 和 model 即可切换不同的服务
      provider: openai

      # API Key（从环境变量读取）
      # DeepSeek: export AI_API_KEY=your-deepseek-key
      # OpenAI: export OPENAI_API_KEY=your-openai-key
      api-key: ${AI_API_KEY:}

      # API 端点
      # DeepSeek: https://api.deepseek.com/v1/chat/completions（默认配置）
      # OpenAI: https://api.openai.com/v1/chat/completions
      # 其他兼容服务: 配置相应的 API 端点
      api-url: https://api.deepseek.com/v1/chat/completions

      # 模型名称
      # DeepSeek: deepseek-chat（默认配置）
      # OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4, gpt-3.5-turbo
      model: deepseek-chat

      # 最大上下文长度（字符数）
      # 建议值：
      #   - DeepSeek: 32000 (32K tokens，约24K汉字)
      #   - GPT-4o: 100000 (128K tokens)
      #   - Claude-3: 200000 (200K tokens)
      max-context-length: 20000

      # 单文档最大长度（字符数）
      # 建议值：max-context-length 的 1/4 到 1/2
      max-doc-length: 5000

      # 单次问答最大处理文档数
      # 用于防止内存溢出和控制响应时间
      # 如果检索到的文档超过此数量，会分批处理
      # 建议值：5-15
      max-documents-per-query: 5

      # 是否启用分批处理模式
      # 启用后，如果文档数超过 max-documents-per-query，
      # 会在答案中提示用户还有未处理的文档，支持继续提问
      enable-batch-processing: true

      # 文档切分策略
      # 可选值：
      #   - SIMPLE: 简单切分，按固定长度切分（性能最好，成本最低）
      #   - SMART_KEYWORD: 智能关键词切分，优先保留包含关键词的内容（推荐，平衡效果和成本）
      #   - AI_SEMANTIC: AI语义切分，使用AI模型智能���分（效果最好，成本最高）
      #   - NONE: 不切分，直接截断（不推荐）
      chunking-strategy: SMART_KEYWORD

      # 文档切分配置
      chunking:
        # 切分块大小（字符数）
        # 建议设置为 max-doc-length 的 80%，留出重叠空间
        chunk-size: 4000

        # 切分重叠大小（字符数）
        # 用于保持上下文连贯性，建议 10-20% 的 chunk-size
        chunk-overlap: 400

        # 是否在句子边界切分（强烈推荐）
        split-on-sentence: true

        # AI 语义切分配置（仅当 chunking-strategy=AI_SEMANTIC 时生效）
        ai-chunking:
          # 是否启用
          enabled: false

          # 用于语义切分的模型
          # 建议使用便宜的模型（如 gpt-4o-mini 或 deepseek-chat）
          model: deepseek-chat

          # 语义切分的 Prompt 模板
          prompt: |
            请将以下文档智能切分成多个语义完整的段落。
            
            要求：
            1. 每个段落应该是一个完整的主题或概念
            2. 保持段落之间的逻辑连贯性
            3. 每个段落大小在 {chunk_size} 字符左右
            4. 返回 JSON 格式：[{"content": "段落1内容", "title": "段落1标题"}, ...]
            
            文档内容：
            {content}

      # Prompt 提示词模板
      # 支持两个占位符：
      #   - {question}: 用户问题
      #   - {context}: 相关文档内容
      # 可以根据业务需求自定义提示词风格和要求
      prompt-template: |
        你是一个专业的知识助手。请基于文档内容回答用户问题。
        
        # 回答要求
        1. 必须基于文档内容回答，不要编造信息
        2. 如果文档中没有相关信息，明确告知用户
        3. 回答要清晰、准确、有条理
        4. 可以引用文档名称作为信息来源
        5. 保持专业友好的语气
        
        # 用户问题
        {question}
        
        # 相关文档
        {context}
        
        # 请提供你的回答：

    # 文档处理配置
    document:
      # 支持的文件格式
      supported-formats:
        - xlsx
        - xls
        - docx
        - doc
        - pptx
        - ppt
        - pdf
        - txt
        - md
        - html
        - xml

      # 最大文件大小（MB）
      max-file-size-mb: 200

      # 最大内容大小（MB）
      # 超过此大小会强制分块
      max-content-size-mb: 50

      # 自动分块阈值（MB）
      # 内容超过此大小会自动启用分块
      auto-chunk-threshold-mb: 2

      # 文档分块大小（字符数）
      chunk-size: 2000

      # 文档分块重叠（字符数）
      chunk-overlap: 400

      # 并行处理配置
      # 是否启用并行处理（文档数量 > 5 时自动启用，可显著提升索引速度）
      parallel-processing: true

      # 并行处理线程数
      # 0 = 自动（使用 CPU 核心数）
      # 建议值: CPU 核心数或略少（如 4核设置为 3-4）
      parallel-threads: 0

      # 批处理大小（文档数）
      # 每批处理多少个文档后提交一次
      # 建议值: 10-20
      batch-size: 10

      # ============================================================
      # 内容长度限制配置（影响内存占用和性能）
      # ============================================================

      # 索引时单个文档最大内容长度（字符数）
      # 超过此长度会被截断，防止后续处理内存溢出
      # 建议值：
      #   - 内存充足（8GB+）: 100000 (约 200KB)
      #   - 内存一般（4GB）: 50000 (约 100KB，默认)
      #   - 内存有限（2GB）: 30000 (约 60KB)
      max-index-content-length: 50000

      # 问答时文档切分最大内容长度（字符数）
      # 通常设置为 max-index-content-length 的 2 倍
      # 因为问答时只处理少量文档（5-10个）
      max-chunk-content-length: 100000

      # 问答时单次切分最大块数
      # 防止切分产生过多块导致内存溢出
      # 建议值：30-100
      max-chunks-per-document: 50

    # 图片处理配置
    image-processing:
      # 图片处理策略:
      #   - placeholder: 占位符（默认，零依赖）
      #   - ocr: OCR 文字识别（需要 Tesseract）
      #   - vision-llm: Vision LLM 语义理解（需要 API Key）
      #   - hybrid: 混合模式（OCR + Vision LLM，推荐）
      strategy: hybrid

      # 是否启用 OCR
      enable-ocr: true

      # OCR 配置
      ocr:
        # Tesseract 数据路径（从环境变量读取）
        # Linux/Mac: export TESSDATA_PREFIX=/path/to/tessdata
        # Windows: set TESSDATA_PREFIX=C:\path\to\tessdata
        # 如果未设置环境变量，将使用项目根目录下的 release/tessdata
        tessdata-path: ${TESSDATA_PREFIX:D:/Jetbrains/hackathon/ai-reviewer-base-file-rag/release/tessdata}

        # 识别语言
        # chi_sim: 简体中文
        # eng: 英文
        # chi_sim+eng: 中英文混合
        language: chi_sim+eng

      # Vision LLM 配置
      vision-llm:
        # 是否启用（混合模式下推荐启用）
        enabled: true

        # API Key（从环境变量读取）
        # 使用与主 LLM 相同的 API Key
        # export VISION_LLM_API_KEY=sk-your-api-key
        # 如果未设置，将使用 AI_API_KEY
        api-key: ${VISION_LLM_API_KEY:${AI_API_KEY:}}

        # 模型名称
        # DeepSeek 不支持 Vision，建议使用：
        # - gpt-4o (OpenAI, 推荐)
        # - gpt-4-vision-preview (OpenAI)
        # - claude-3-opus (Anthropic)
        # - gemini-pro-vision (Google)
        model: gpt-4o

        # API 端点（可选）
        # 如果使用 OpenAI，设置为：https://api.openai.com/v1/chat/completions
        endpoint: ${VISION_LLM_ENDPOINT:https://api.openai.com/v1/chat/completions}

# 日志配置
logging:
  level:
    root: INFO
    top.yumbo.ai.rag: DEBUG
    org.springframework: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"

---
# 开发环境配置
spring:
  config:
    activate:
      on-profile: dev

knowledge:
  qa:
    knowledge-base:
      rebuild-on-startup: true  # 开发时每次重建
    vector-search:
      enabled: true

logging:
  level:
    top.yumbo.ai.rag: DEBUG

---
# 生产环境配置
spring:
  config:
    activate:
      on-profile: prod

knowledge:
  qa:
    knowledge-base:
      rebuild-on-startup: false  # 生产环境使用已有知识库
    vector-search:
      enabled: true

logging:
  level:
    top.yumbo.ai.rag: INFO


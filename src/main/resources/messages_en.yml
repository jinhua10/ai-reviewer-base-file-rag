error:
  chunk:
    chunk_size_positive: "chunk-size must be positive"
    expected_json_array: "Expected JSON array, got: {0}"
    no_valid_chunks: "No valid chunks found in AI response"
    no_valid_json: "No valid JSON found in response"
    overlap_less_than_size: "chunk-overlap must be less than chunk-size"
    overlap_non_negative: "chunk-overlap must be non-negative"
  auth:
    user_exists: "User already exists"
    invalid_credentials: "Invalid credentials"
    permission_denied: "Permission denied: {0}"
  invalid_admin_endpoint: "Invalid admin endpoint"
  invalid_document_endpoint: "Invalid document endpoint"
  invalid_search_endpoint: "Invalid search endpoint"
  unknown_endpoint: "Unknown endpoint: {0}"
log:
  admin:
    cache_cleared: "Cache cleared"
    cache_cleared_success: "Cache cleared successfully"
    clear_cache_failed: "Failed to clear cache"
    clear_cache_failed_detail: "Failed to clear cache: {0}"
    index_optimize_failed: "Failed to optimize index"
    index_optimize_failed_detail: "Failed to optimize index: {0}"
    index_optimized: "Index optimized"
    index_optimized_success: "Index optimized successfully"
    stats_failed: "Failed to get statistics"
    stats_failed_detail: "Failed to get statistics: {0}"
  api:
    started: "API Server started on port {0}"
    stopped: "API Server stopped"
  audit:
    init_failed: "Failed to initialize audit log"
    logged: "Audit logged: {0}"
    write_failed: "Failed to write audit log"
  auth:
    logged_in: "User logged in: {0}"
    logged_out: "User logged out: {0}"
    registered: "User registered: {0}"
  batch:
    failed: "Batch processing failed"
  channel:
    exception: "Exception in channel"
  chunk:
    ai_completed: "AI semantic chunking completed: {0} chars -> {1} chunks in {2}ms"
    ai_failed: "AI semantic chunking failed, falling back to smart keyword chunking"
    ai_not_enabled: "AI chunking is not enabled in config"
    ai_parse_failed: "Failed to parse AI chunking response: {0}"
    ai_start: "Starting AI semantic chunking for {0} chars"
    content_truncate: "Content too large ({0} chars), truncating to {1} chars"
    coverage_low: "Coverage too low, adding sequential chunks"
    display_part: "Part {0}/{1}"
    keywords_extracted: "Extracted {0} keywords from query: {1}"
    max_chunks_reached: "Reached maximum chunk limit ({0}), stopping chunking"
    no_keywords_fallback: "No keywords found, falling back to simple chunking"
    simple_summary: "Simple chunking: {0} chars -> {1} chunks (size={2}, overlap={3})"
    smart_summary: "Smart keyword chunking: {0} chars -> {1} chunks with {2} keywords"
    truncate_warning: "Content too long ({0} chars), truncating to {1} chars for AI chunking"
  chunker:
    ai_disabled: "AI chunking is not enabled, falling back to SMART_KEYWORD strategy"
    creating: "Creating document chunker: strategy={0}, chunkSize={1}, overlap={2}"
    llm_null: "LLM client is null, falling back to SMART_KEYWORD strategy"
    unknown_strategy: "Unknown chunking strategy: {0}, using SMART_KEYWORD"
  default:
    missingKey: "Missing log key: {0}"
  docs:
    classpath_in_jar: "Classpath path is inside a JAR, write is not supported"
    classpath_load_failed: "Failed to load resource from classpath: {0} - {1}"
    classpath_not_exists: "Classpath resource not found: {0}"
    classpath_realpath: "Using classpath real path: {0}"
    classpath_resource_found: "Found resource in classpath: {0}"
    create:
      failed: "Failed to create document: {0}"
      success: Document created successfully
    create_failed: "Failed to create documents directory: {0}"
    created: "Document created: {0}"
    delete:
      failed: "Failed to delete document: {0}"
      success: Document deleted successfully
    deleted: "Document deleted: {0}"
    directory_ready: "Documents directory ready: {0}"
    file_exists_renamed: "File exists, renamed to: {0}"
    get:
      failed: "Failed to get document: {0}"
    invalid_directory: "Invalid directory: {0}"
    list:
      exception: "Exception while listing documents: {0}"
      failed: "Failed to list documents: {0}"
    not_found: "Document not found: {0}"
    notfound: "Document not found: {0}"
    process_failed: "Failed to process file: {0}"
    saved: "Document saved: {0}"
    scanned_types: "Scanned file types: {0}"
    total: "Total documents: {0}"
    update:
      failed: "Failed to update document: {0}"
      success: Document updated successfully
    updated: "Document updated: {0}"
    upload_to_external: "Uploads will be saved to external path: ./data/documents"
    using_default_path: "Using default path: ./data/documents"
    using_filesystem: "Using filesystem path: {0}"
    walk_failed: "Failed to walk directory: {0}"
  factory:
    create_caffeine: "Creating CaffeineCacheEngine"
    create_filesystem: "Creating FileSystemStorageEngine"
    create_lucene: "Creating LuceneIndexEngine"
  feedback:
    document_failed: "Failed to process document feedback"
    document_received: "Received document feedback {0}: recordId={1}, document={2}"
    get_pending_failed: "Failed to get pending QA records"
    get_recent_failed: "Failed to get recent QA records"
    get_record_failed: "Failed to get QA record"
    get_statistics_failed: "Failed to get QA statistics"
    overall_failed: "Failed to process overall feedback"
    overall_received: "Received overall feedback: recordId={0}, rating={1}"
    rating_failed: "Failed to process rating"
    rating_submitted: "Rating submitted {0} [{1}]: {2} - {3} stars (impact: {4})"
    weight_disabled: Dynamic weighting adjustment disabled
    weight_updated: "üìä Document weight updated: {0} -> weight={1} (üëç{2} üëé{3})"
    weight_reset: "üîÑ Reset document weight: {0} -> {1}"
    weights_cleared: üßπ Cleared all document weights
    weights_saved: "üíæ Saved document weights: {0} documents"
    save_failed: Failed to save document weights
    weights_loaded: "üìÇ Loaded document weights: {0} documents"
    load_failed: Failed to load document weights
    weights_file_not_exists: Document weights file not exists, using default weights
    rating_updated: "Document weight updated (rating): {0} -> weight={1} ({2}star, adjust{3}, üëç{4} üëé{5})"
  filetracking:
    check_failed: "Failed to check file status: {0}"
    clear_failed: "Failed to clear file tracking info: {0}"
    cleared: "Cleared file tracking information"
    load_failed: "Failed to load file tracking info: {0}"
    loaded: "Loaded file tracking info: {0} files"
    mark_failed: "Failed to mark file as indexed: {0}"
    save_failed: "Failed to save file tracking info: {0}"
    saved: "Saved file tracking info: {0} files"
  http:
    bad_request: "Bad request: {0}"
    internal_server_error: "Internal server error"
    processing_error: "Error processing HTTP request"
    received_request: "Received HTTP request: {0} {1}"
    shutdown: "HTTP server shutdown"
    start_failed: "Failed to start HTTP server"
    started: "HTTP server started on port {0}"
  hybrid:
    cannot_get_doc: "Cannot get document: id={0}, score={1}"
    completed: "Hybrid search completed: returned {0} documents in {1}ms"
    could_not_get_doc: "Could not get document for entry #{0} id={1} (score: {2})"
    detail_item: "{0}. {1} (hybrid: {2} = Lucene rank#{3} + vector:{4})"
    doc_id_list: "Document ID list: {0}"
    extract_keywords: "Extracted keywords: {0}"
    failed: "Hybrid search failed, falling back to keyword search"
    filtered: "Filtered out {0} low-score documents (score < {1}), keeping {2}"
    found_docs: "Found {0} documents"
    keyword_search: "Keyword search: {0}"
    lucene_found: "Lucene found {0} documents (total hits: {1}, limit={2})"
    lucene_top_header: "Lucene Top-10 documents (by relevance):"
    lucene_top_item: "{0}. {1} - {2} chars (Lucene rank score: {3,number,#.###})"
    severe_no_docs: "Severe: {0} scored docs but no document objects could be retrieved"
    top5_header: "Hybrid score Top-5 (before filtering, threshold={0}, topK={1}):"
    top5_item: "{0} {1}. {2} (score: {3})"
    topk_header: "Hybrid Top-{0} (Lucene weight:0.3 + Vector weight:0.7):"
    total_nulls: "Total {0} documents could not be retrieved (out of {1} scored docs)"
    vector_found: "Vector search found {0} documents (limit={1})"
    vector_top_header: "Vector Top-10 documents:"
    vector_top_item: "- {0} (similarity: {1,number,#.###})"
  image:
    delete_failed: "Failed to delete: {0}"
    deleted_all: "Deleted all images for document: {0}"
    read_info_failed: "Failed to read image info: {0}"
    saved: "Saved image: {0} for document: {1}"
    storage:
      created: "Created image storage directory: {0}"
      init_failed: "Failed to initialize image storage"
  imageproc:
    activated: "Image processing strategy activated: {0}"
    add_ocr: "Adding OCR strategy"
    add_vision: "Adding Vision LLM strategy"
    init: "Initializing image processing features..."
    language: "Recognition language: {0}"
    ocr_available: "OCR strategy available"
    ocr_hint: "Hint: add dependency net.sourceforge.tess4j:tess4j:5.9.0"
    ocr_unavailable: "OCR strategy unavailable: tess4j missing"
    placeholder: "Using placeholder strategy (default)"
    strategy: "Image processing strategy: {0}"
    tessdata: "Tessdata path: {0}"
    vision_apikey_hint: "Hint: set environment variable VISION_LLM_API_KEY or config knowledge.qa.image-processing.vision-llm.api-key"
    vision_available: "Vision LLM strategy available"
    vision_endpoint: "Vision LLM endpoint: {0}"
    vision_model: "Vision LLM model: {0}"
    vision_no_apikey: "Vision LLM enabled but API key not configured"
    vision_unavailable: "Vision LLM strategy unavailable"
  kb:
    batch_processing: "Batch processing: {0} docs ({1} / {2})"
    exists: "Existing knowledge base detected ({0} documents)"
    file_process_failed: "Failed to process file: {0}"
    files_to_index: "Files to index: {0}"
    final_batch: "Processing final batch: {0} docs"
    first_create: "First-time knowledge base creation"
    found_files: "Found {0} document files"
    hint_put_docs: "Hint: please put documents into {0}"
    incremental_done: "Incremental indexing completed!"
    incremental_failed: "Incremental indexing failed"
    incremental_stats: "Processed files: {0}/{1}, failed: {2}, total docs: {3}, time: {4}s"
    no_documents: "No supported documents found"
    parallel_mode: "Using parallel processing mode ({0} threads)"
    processing_start: "Start processing documents..."
    scanning: "Scanning documents: {0}"
    serial_mode: "Using serial processing mode"
    supported_formats: "Supported formats: {0}"
    up_to_date: "All files are up-to-date, no update needed"
    vector_enabled: "Vector search enabled"
    vector_init_failed: "Vector search init failed, falling back to keyword search"
  kqa:
    answer_header: "Answer:"
    found_chunks_images: "Found {0} chunks and {1} images for document"
    images_in_context: "Context contains {0} images"
    incremental_mode: "Rebuild mode: Incremental indexing"
    init_done: "Knowledge QA system initialized"
    init_failed: "Knowledge QA initialization failed"
    init_start: "Knowledge QA system initializing..."
    load_chunks_images_failed: "Failed to load chunks/images info"
    more_docs_available: "{0} more documents are available"
    rebuild_mode: "Rebuild mode: Full rebuild"
    response_time: "Response time: {0}ms"
    sep: "============================================================"
    source_path: "Source path: {0}"
    sources_header: "Sources (total {0} documents):"
    step: "Step {0}: {1}"
    storage_path: "Storage path: {0}"
    used_docs_count: "Using {0} documents for this answer"
  memory:
    critical: "Phase {0} - Critical memory usage: {1}% - OOM risk!"
    gc_done: "GC completed: before={0}% -> after={1}%"
    gc_trigger: "Memory usage {0}% exceeds threshold, triggering GC"
    usage: "Phase {0} - Used memory: {1} MB/ Max: {2} MB ({3}%)"
    usage_phase: "Phase {0} - Used memory: {1} MB/ Max: {2} MB ({3}%)"
    warning: "Phase {0} - High memory usage: {1}%"
  monitor:
    health:
      storage_ok: 'Storage OK, docs: {0}'
      storage_error: 'Storage error: {0}'
      index_ok: 'Index OK, indexed: {0}'
      index_error: 'Index error: {0}'
      cache_ok: Cache OK
      cache_error: 'Cache error: {0}'
    metrics:
      report: 'Metrics:\n  HTTP: {0} requests, {1} errors\n  Documents: {2} created, {3} updated, {4} deleted\n  Search: {5} requests, {6} errors\n  Auth: {7} attempts, {8} failures'
    performance:
      report: 'Performance Metrics:\n  Indexed: {0} docs, Avg: {1}ms, Errors: {2}\n  Searches: {3}, Avg: {4}ms, Errors: {5}\n  Cache Hit Rate: {6}%'
  optimizer:
    commit: "Committing RAG changes"
    done: "Commit and optimization completed"
    embedding_closed: "Embedding engine closed"
    optimize: "Optimizing index"
    save_failed: "Failed to save vector index"
    saving_vectors: "Saving vector index..."
    vectors_saved: "Vector index saved successfully"
  optimization:
    chunker:
      initialized: 'DocumentChunker initialized - chunkSize: {0}, overlap: {1}, smartSplit: {2}, maxContentLength: {3}, maxChunks: {4}'
      chunked: 'Document {0} chunked into {1} parts'
      batch_completed: 'Batch chunking completed: {0} documents -> {1} chunks'
    memory:
      suggest_gc: 'Suggesting garbage collection, current memory usage: {0}MB'
      gc_freed: 'GC completed, freed approximately {0}MB of memory'
      gc_no_freed: 'GC completed, no significant memory freed'
    context:
      initialized: 'SmartContextBuilder initialized: maxContext={0}chars, maxDoc={1}chars, preserveFullContent={2}'
      initialized_with_chunker: 'SmartContextBuilder initialized with chunker: strategy={0}, maxContext={1}chars, maxDoc={2}chars, storage={3}'
      built: 'Smart context built: {0}chars from {1} documents ({2}% of max)'
      remaining_chars: '\n[... {0} more characters not displayed, content extracted by keyword priority]'
  qa:
    archive_failed: "Failed to archive QA record"
    archived: "High rating QA archived: rating={0}, path={1}"
    document_feedback: "Document feedback {0} [{1}]: {2} - {3}"
    feedback_applied: "Feedback applied to document weight: {0}"
    feedback_pending: "Feedback pending review: {0}"
    find_failed: "Failed to find record: {0}"
    load_failed: "Failed to load QA record: {0}"
    pending_failed: "Failed to get pending records"
    rating_applied: "Rating applied to document weight: {0} ({1} stars -> adjust {2})"
    rating_pending: "Rating pending review: {0} ({1} stars)"
    rating_submitted: "Rating submitted {0} [{1}]: {2} - {3} stars (adjust: {4})"
    recent_failed: "Failed to get recent records"
    record_notfound: "Record not found: {0}"
    record_save_failed: "Failed to save QA record"
    record_saved: "Saved QA record: {0} - {1}"
    record_update_failed: "Failed to update QA record: {0}"
    record_updated: "Updated QA record: {0}"
    records_dir: "QA records directory: {0}"
    records_dir_failed: "Failed to create QA records directory: {0}"
    stats_failed: "Failed to compute QA statistics"
    user_feedback: "User feedback [{0}]: rating={1}, content={2}"
  query:
    cache_cleared: "Query cache cleared"
    cache_hit: "Cache hit for query: {0}"
    processed: "Query processed: text='{0}', hits={1}, time={2}ms"
    unknown_sort: "Unknown sort field: {0}, using relevance"
  rag:
    batch_indexed: "Batch indexed {0} documents"
    cache_hit: "Query result retrieved from cache: {0}"
    close_error: "Error closing Local File RAG"
    closed: "Local File RAG closed successfully"
    closing: "Closing Local File RAG..."
    delete_failed: "Failed to delete document: {0}"
    deleted_count: "Deleted {0} documents"
    deleting_all: "Deleting all documents..."
    doc_deleted: "Document deleted: {0}"
    doc_indexed: "Document indexed successfully: {0}"
    doc_updated: "Document updated: {0}"
    enable_cache: "Enable cache: {0}"
    enable_compression: "Enable compression: {0}"
    found_to_delete: "Found {0} documents to delete"
    init: "Initializing Local File RAG..."
    init_done: "Local File RAG initialization completed"
    initialized: "Local File RAG initialized with configuration: {0}"
    load_content_failed: "Failed to load content for document: {0}"
    loaded_content: "Loaded content for document: {0}, length: {1}"
    no_documents: "No documents to delete"
    optimized: "Index optimization completed"
    optimizing: "Optimizing index..."
    search_completed: "Search completed in {0}ms, found {1} results"
    simple_init: "Initializing simple RAG service..."
    simple_init_done: "Simple RAG service initialization completed"
    storage: "Storage path: {0}"
  search:
    advanced_completed: 'Advanced search completed: query=''{0}'', hits={1}'
    advanced_failed: Advanced search failed
    advanced_failed_detail: 'Advanced search failed: {0}'
    completed: 'Search completed: query=''{0}'', hits={1}'
    failed: Search failed
    failed_detail: 'Search failed: {0}'
  similar:
    failed: "Failed to find similar QAs"
    found: "Found {0} similar QAs (keywords: {1})"
  simple:
    batch_index_files_complete: "Batch indexing completed: success {0}, fail {1}"
    batch_indexed: "Batch indexed {0} documents"
    commit: "Commit index changes"
    doc_indexed: "Document indexed: {0} -> {1}"
    file_empty: "File content empty: {0}"
    index_file: "Indexed file: {0} -> {1} ({2} bytes)"
    index_file_failed: "Failed to index file: {0}"
    optimized: "Index optimization completed"
    scanned_files: "Scanned {0} files"
    search_results: "Search '{0}' found {1} results"
    shutdown: "Shutting down RAG service"
  vector:
    embedding_closed: 'üîÑ Vector embedding engine closed'
    embedding_dim: '   - Dimension: {0}'
    embedding_init_success: Embedding engine initialized successfully
    embedding_model: '   - Model: {0}'
    fix_check_logs: '   3. Check logs for detailed error information'
    fix_place_model: '   1. Place model files in src/main/resources/models/ directory'
    fix_supported_models: '   2. Supported models: bge-m3, paraphrase-multilingual, text2vec-large-chinese'
    index_init_success: Index engine initialized successfully
    index_path: 'Index path: {0}'
    init_embedding: Initializing embedding engine...
    init_index: Initializing index engine...
    possible_fixes: 'üîß Possible fixes:'
    possible_reasons: 'üí° Possible reasons:'
    reason_model_missing: '   1. Model file missing'
    reason_model_path: '   2. Model path incorrect'
    reason_onnx: '   3. ONNX Runtime version incompatible'
    test:
      start: 'üß™ Vector search functionality test'
      test_failed: '‚ùå Test failed'
      test_success: '‚úÖ Test successful! Vector search engine initialized normally'
    vector_count: '   - Vector count: {0}'
  file:
    not_exists: "File does not exist: {0}"
    not_a_file: "Not a file: {0}"
    empty: "File content is empty: {0}"
    index_failed: "Failed to index file: {0}"
    directory_not_exists: "Directory does not exist: {0}"
    not_a_directory: "Not a directory: {0}"
  llm:
    api_key_missing: ‚ö†Ô∏è LLM API Key not configured
    api_key_hint: "üí° Hint: Set environment variables:"
    api_key_deepseek: '      - DeepSeek: export AI_API_KEY=your-deepseek-key'
    api_key_openai: '      - OpenAI: export OPENAI_API_KEY=your-openai-key'
    fallback_mock: üí° Will fallback to Mock mode
    client_created: 'ü§ñ Creating {} LLM client'
    model: '   - Model: {}'
    api_url: '   - API: {}'
    mock_created: ü§ñ Creating Mock LLM client (for testing only)
    mock_warning: ‚ö†Ô∏è Mock mode will return fixed simulated responses
    mock_hint: "üí° For real LLM, configure:"
    mock_provider: '      knowledge.qa.llm.provider=openai'
    mock_apikey: '      and set the corresponding API Key and URL'
  storage:
    chunk_storage_init: 'Initializing ChunkStorageService with path: {}'
    image_storage_init: 'Initializing ImageStorageService with path: {}'
    ai_image_analyzer_init: 'Initializing AIImageAnalyzer: enabled={}, model={}'
    document_image_extraction_init: 'Initializing DocumentImageExtractionService: AI analysis={}'

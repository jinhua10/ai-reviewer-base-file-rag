# Flattened messages converted from messages_en.yml (English)
# Generated to support I18n.get(key) lookups
api:
  started: "API server started on port {0}"
banner:
  supports: "Supports: {0}"
  title: "AI Reviewer - Knowledge Base Intelligent Q&A System"
  version: "Version: {0}"
batch:
  failed: "Batch processing failed"
context:
  initialized: "SmartContextBuilder initialized: maxContext={0} chars, maxDoc={1} chars, preserveFullContent={2}"
document_management:
  api:
    error:
      delete_failed: "Deletion failed: {0}"
      file_empty: "File is empty"
      index_failed: "Indexing failed: {0}"
      list_failed: "Failed to retrieve list: {0}"
      not_found: "Document not found"
      upload_failed: "Upload failed: {0}"
    success:
      batch_result: "Success: {0}, Failed: {1}"
      batch_upload: "Batch upload completed"
      delete: "Document deleted successfully"
      index: "Document indexed successfully"
      list_loaded: "Document list retrieved successfully: Returned {0} documents, Total {1}"
      upload: "Document uploaded successfully"
  log:
    scanned_types: "Scanned file types: {0}"
    batch_upload: "Batch upload documents: {0}"
    upload_success: "Document uploaded successfully: {0}"
    upload_failed: "Document upload failed"
    file_upload_failed: "File upload failed: {0}"
    list_documents: "Get document list: page={0}, pageSize={1}, sortBy={2}, sortOrder={3}"
    list_failed: "Failed to get document list"
    auto_index_trigger: "ðŸ“¦ Auto-triggering incremental index..."
    auto_index_success: "âœ… Auto incremental index completed"
    auto_index_failed: "âš ï¸ Auto incremental index failed: {0}"
    trigger_auto_index_failed: "âš ï¸ Failed to trigger auto index: {0}"
document_service:
  error:
    cannot_create_dir: "Failed to create document directory: {0}"
    filename_empty: "Filename is empty"
    file_too_large: "File too large: {0} MB (Max: {1} MB)"
    illegal_path: "Illegal file path"
    unsupported_format: "Unsupported file format: {0}"
  log:
    scanned_types: "Scanned file types: {0}"
error:
  auth:
    invalid_credentials: "Invalid credentials"
    permission_denied: "Permission denied: {0}"
    user_exists: "User already exists"
  chunk:
    expected_json_array: "Expected JSON array, but got: {0}"
    no_valid_chunks: "No valid chunks found in AI response"
    no_valid_json: "No valid JSON found in response"
  docs:
    create_failed: "Failed to create document: {0}"
  file:
    directory_not_exists: "Directory does not exist: {0}"
    empty: "File content is empty: {0}"
    index_failed: "File indexing failed: {0}"
    not_a_directory: "Not a directory: {0}"
    not_a_file: "Not a file: {0}"
    not_exists: "File does not exist: {0}"
feedback:
  api:
    emoji:
      "1": "ðŸ˜ž Completely useless"
      "2": "ðŸ™ Not helpful"
      "3": "ðŸ˜ Average"
      "4": "ðŸ˜Š Very helpful"
      "5": "ðŸ¤© Extremely useful"
    error:
      invalid_feedback_type: "feedbackType must be LIKE or DISLIKE"
      invalid_rating: "Rating must be between 1-5"
      missing_params: "{0} cannot be empty"
      processing_failed: "Processing failed: {0}"
      record_not_found: "Record not found"
    impact:
      document:
        "1": "This document is not helpful, the system will significantly reduce its recommendation weight âš ï¸"
        "2": "This document is not very helpful, the system will reduce its recommendation weight ðŸ“‰"
        "3": "This document is average, the system will keep its current weight âž¡ï¸"
        "4": "This document is very helpful, the system will increase its recommendation weight ðŸ“ˆ"
        "5": "This document is extremely useful! The system will prioritize recommending it ðŸš€"
      overall:
        "1": "Sorry we couldn't help you! We will focus on improvement ðŸ™"
        "2": "Thank you for your feedback! We will analyze and improve ðŸ”§"
        "3": "Thank you for your rating! We will continue to optimize ðŸ’¡"
        "4": "Thank you for your feedback! This helps us improve answer quality ðŸ‘"
        "5": "Great! This answer will be recommended to other users ðŸŒŸ"
    success:
      feedback_received: "Thank you for your feedback!"
      thank_you: "Thank you for your rating!"
image:
  log:
    get_failed: "Failed to retrieve image: {0}/{1}"
    list_failed: "Failed to retrieve document image list: {0}"
  saved: "Image saved: {0} (Document: {1})"
  service:
    saved: "Image saved: {0}"
knowledge_qa_service:
  doc_item: "{0}. {1}"
  error_processing: "Error occurred while processing Q&A: {0}"
  found_chunks_images: "ðŸ“¦ Found {0} chunks and {1} images"
  image_guide_1: "1. Related images are provided below with ready-to-use Markdown links (marked with ðŸ“·)."
  image_guide_2: "2. [IMPORTANT] When your answer involves image content, you MUST reference the image by copying the link code after ðŸ“·."
  image_guide_3: "3. Referencing images helps users visually understand your answer and judge its quality."
  image_guide_4: "4. Example: If an image shows architecture diagram, mention it as: ![Architecture](/api/images/xxx)"
  doc_images_header: "Images from document \"{0}\" ({1} total):"
  image_desc_default: "Image {0}"
  images_in_context: "ðŸ–¼ï¸ Context contains {0} images of information"
  # Added keys requested
  close_existing_kb: "Closing existing knowledge base..."
  destroy_start: "Destroying knowledge Q&A service..."
  kb_closed: "Knowledge base closed"
  kb_closed_safe: "Knowledge base closed safely"
  rebuild_start: "Starting knowledge base rebuild..."
  system_closed: "Knowledge Q&A system closed"
  log:
    build_complete: "   âœ… Knowledge base build completed"
    failed_files: "      - Failed files: {0}"
    kb_ready: "   âœ… Knowledge base is ready"
    processed_files: "      - Processed files: {0}"
    total_documents: "      - Total documents: {0}"
    total_files: "      - Total files: {0}"
    chunk_overlap_chars: "Chunk overlap: {0} chars"
    chunk_size_chars: "Chunk size: {0} chars"
    chunking_strategy: "Chunking strategy: {0}"
    create_qa_system: "Creating QA system..."
    document_count: "Documents: {0}"
    index_count: "Index entries: {0}"
    init_llm: "Initializing LLM client..."
    init_vector_engine: "Initializing vector engine: {0}"
    llm_client_ready: "LLM client ready"
    llm_client_type: "LLM client type: {0}"
    llm_provider: "LLM provider: {0}"
    max_context_chars: "Max context chars: {0}"
    max_doc_length_chars: "Max document length chars: {0}"
    smart_context_initialized: "Smart context initialized: maxContext={0}, maxDoc={1}"
    vector_init_failed_hint: "âš ï¸ Vector search initialization failed, system will use text search only"
    vector_init_model_hint: "ðŸ’¡ Hint: embedding model file is incomplete or corrupted"
    vector_init_solution: "ðŸ“ Solutions:"
    vector_init_solution_1: "   1. Set knowledge.qa.vector-search.enabled: false in application.yml"
    vector_init_solution_2: "   2. Or download complete ONNX model files (including .onnx and .onnx_data files)"
    llm_call: "ðŸ¤– Calling LLM, prompt length: {0} characters"
    llm_call_failed: "âŒ LLM call failed"
    llm_call_error: "LLM call failed: {0}"
    direct_qa_mode: "ðŸ“„ Direct QA mode (without knowledge base retrieval)"
    prompt_length: "Prompt length: {0} characters"
    direct_qa_complete: "âœ… Direct QA complete, time: {0}ms"
    direct_qa_failed: "âŒ Direct QA failed"
    direct_qa_error: "Direct QA processing failed: {0}"
    context_info: "Context information:"
    using_keyword_mode: "Using keyword mode for context building"
    record_saved: "QA record saved: {0}"
    using_vector_enhancement: "Using vector enhancement for retrieval"
    vector_count: "Vector index contains {0} vectors"
    vector_dimension: "Vector dimension: {0}"
    vector_engine_loaded: "Vector engine loaded: {0}"
    vector_index_loaded: "Vector index loaded: {0}"
    vector_index_path: "Vector index path: {0}"
    vector_model: "Vector model: {0}"
  model_doc_hint: "Hint: Place model files in {0}"
  model_download_hint: "Hint: Download the model from {0}"
  more_images: "  ... {0} more images"
  question_prompt: "â“ Question: {0}"
  question_separator: "\n================================================================================\n"
  referenced_docs: "\n\n**Referenced Documents**:"
  related_image: "Related Images"
  remaining_docs: "â„¹ï¸ {0} more related documents not included in this answer"
  remaining_docs_unprocessed: "â„¹ï¸ {0} more related documents not included in this answer"
  response_time: "\nâ±ï¸  Response time: {0}ms"
  separator: "================================================================================="
  sources_label: "\nðŸ“š Data Sources (Total {0} documents):"
  too_many_docs_retrieved: "âš ï¸ Retrieved {0} documents, processing first {1} (Config: documents-per-query)"
  using_docs: "ðŸ“š Used {0} documents to generate this answer"
  using_hybrid_search: "âœ… Using hybrid search (Lucene + Vector)"
  using_strategy_dispatcher: "ðŸŽ¯ Using search strategy dispatcher"
  answer_label: "\nðŸ’¡ Answer:"
  available_images: "Available images: {0}"
  context_stats: "Context: {0} chars from {1} documents"
  create_session: "Creating QA session: {0}"
  image_item: "Image {0}: {1} (Source: {2})"
  important_notice: "Important: {0}"
  using_keyword_search: "Using keyword search mode"
  classpath_prefix: "Classpath: {0}"
  closing_existing_kb: "Closing existing knowledge base before proceeding..."
  debug_enhanced_stats: "ðŸ“Š Statistics: Filesystem={0} files, Indexed={1} docs, Unindexed={2}, Progress={3}%"
  debug_enhanced_stats_v2: "ðŸ“Š Detailed Stats: Filesystem={0} files, Unique docs={1}, Index chunks={2}, Unindexed={3}, Progress={4}%"
  existing_kb_closed: "Existing knowledge base closed"
  failed_files: "      - Failed files: {0}"
  incremental_index_start: "Starting incremental indexing..."
  more_docs_notice: "\nâ„¹ï¸ {0} more related documents not included in this answer"
  question_label: "â“ Question:"
  success_files: "      - Successful files: {0}"
  total_documents: "      - Total documents: {0}"
  using_session_docs: "Using session-stored documents: {0}"

# File System Storage Engine
storage_engine:
  log:
    initialized: "FileSystemStorageEngine initialized at: {}"
    failed_init_dirs: "Failed to initialize directories"
    failed_init_storage_dirs: "Failed to initialize storage directories"
    document_with_same_content: "Document with same content already exists: {}"
    document_stored: "Document stored: {}"
    failed_store: "Failed to store document"
    failed_store_batch: "Failed to store document in batch: {}"
    document_file_not_found: "Document file not found: {}"
    failed_retrieve: "Failed to retrieve document: {}"
    document_deleted: "Document deleted: {}"
    failed_delete: "Failed to delete document: {}"
    document_updated: "Document updated: {}"
    storage_cleared: "Storage cleared"
    clear_storage_failed: "Failed to clear storage"
  error:
    failed_init_dirs: "Failed to initialize directories"
    failed_init_storage_dirs: "Failed to initialize storage directories"
    failed_store_document: "Failed to store document"
    failed_retrieve_document: "Failed to retrieve document: {}"
    failed_delete_document: "Failed to delete document: {}"
    clear_storage: "Failed to clear storage"

# Audit Event
audit:
  event:
    title: "Audit Event"
    event_id: "Event ID"
    event_type: "Event Type"
    user_id: "User ID"
    username: "Username"
    action: "Action"
    resource: "Resource"
    details: "Details"
    success: "Success"
    timestamp: "Timestamp"
    ip_address: "IP Address"

# Audit Log Service
audit_log:
  log:
    logged: "Audit logged: {}"
    write_failed: "Failed to write audit log"
    init_failed: "Audit log initialization failed"
  document_operation: "Document operation"
  search_operation: "Search operation"
  auth_event: "Authentication event"

# Document Chunker
document_chunker:
  interface:
    title: "Document chunker interface"
    chunk_method: "Chunk a document"
    chunk_method_desc: "Split document content into multiple document chunks"
    chunk_method_no_query: "Chunk a document without query context"
    chunk_method_no_query_desc: "Split document content without query context"
    get_name: "Get chunker name"
    get_description: "Get chunker description"
  param:
    content: "Document content"
    query: "User query (optional - used by smart chunkers)"
    return: "List of document chunks"

# Document Chunk
document_chunk:
  class:
    title: "Document chunk"
    description: "Represents a fragment of a chunked document"
  field:
    content: "Chunk content"
    title: "Chunk title (optional)"
    index: "Chunk index (zero-based)"
    total_chunks: "Total number of chunks"
    start_position: "Start position in original document"
    end_position: "End position in original document"

# Cache Engine
cache_engine:
  interface:
    title: "Cache engine interface"
    description: "Provides multi-level cache support"
  method:
    get_document: "Get cached document"
    put_document: "Cache document"
    get_query_result: "Get cached query result"
    put_query_result: "Cache query result"
    invalidate_document: "Invalidate document cache"
  param:
    doc_id: "Document ID"
    query_key: "Query key"
    result: "Query result"
    document: "Document object"

# Index Engine
index_engine:
  interface:
    title: "Index engine interface"
    description: "Responsible for document indexing and search"
  method:
    index_document: "Index document"
    index_batch: "Batch index documents"
    update_index: "Update index"
    delete_from_index: "Delete document from index"
    search: "Search documents"
  param:
    documents: "List of documents"
    new_document: "New document object"
    query: "Query object"
    return: "Search result"

# SQLite Metadata Manager
sqlite_metadata_manager:
  class:
    title: "SQLite Metadata Manager"
    description: "Stores document metadata using SQLite database"
  log:
    initialized: "SQLite metadata manager initialized: {}"
    failed_init: "Failed to initialize SQLite database"
    failed_execute_update: "Failed to execute update: {}"
    document_saved: "Document metadata saved: {}"
    failed_save: "Failed to save document metadata: {}"
    document_found: "Document metadata found: {}"
    failed_find: "Failed to find document metadata: {}"
    document_updated: "Document metadata updated: {}"
    failed_update: "Failed to update document metadata: {}"
    document_deleted: "Document metadata deleted: {}"
    failed_delete: "Failed to delete document metadata: {}"
    cleared: "Metadata table cleared"
    failed_clear: "Failed to clear metadata table"
    failed_exists: "Failed to check document existence"
    failed_count: "Failed to get document count"
    failed_get_all_ids: "Failed to get all document IDs"
    failed_find_by_hash: "Failed to find documents by hash"
    failed_close: "Failed to close database connection"
    retrieved_ids: "Retrieved {} document IDs"
    connection_closed: "SQLite connection closed"
    connection_close_failed: "Failed to close SQLite connection"
    all_cleared: "All document metadata cleared"
    clear_failed: "Failed to clear document metadata"

# SHA-256 Document Hasher
sha256_document_hasher:
  class:
    title: "SHA-256 Document Hasher"
    description: "Computes hash values of document content for deduplication"
  method:
    compute_hash: "Compute content hash"
    compute_hash_string: "Compute string content hash"
  param:
    content_bytes: "Content byte array"
    content_string: "Content string"
    return: "Base64 encoded hash value"
  log:
    failed_compute: "Failed to compute hash: algorithm not found"
  error:
    failed_compute: "Failed to compute hash"
    connection_closed: "Database connection closed"

# Application
app:
  log:
    started: "âœ… Application started successfully, Address: {0}"
    start_failed: "âŒ Application startup failed"
    startup_failed: "Application startup failed"

# Document QA Service
doc_qa:
  log:
    create_temp_dir: "Creating document QA temp directory: {0}"
    create_temp_dir_failed: "Failed to create temp directory"
    start_qa: "ðŸ“„ Starting document QA: {0} (Session ID: {1})"
    question: "â“ Question: {0}"
    batch_mode: "ðŸ“¦ Document is large, enabling batch processing mode"
    direct_mode: "ðŸ“ Document is small, processing directly"
    qa_complete: "âœ… Document QA completed: {0} (Time: {1}ms)"
    qa_error: "âŒ Document QA failed"
    direct_start: "ðŸ“„ Starting direct document analysis (without knowledge base): Document={0}, Session ID={1}"
    direct_question: "â“ Analysis question: {0}"
    direct_content_length: "ðŸ“ Document content length: {0} characters"
    direct_exceed_limit: "ðŸ“¦ Document content exceeds limit ({0}), using memo mechanism for batch processing"
    direct_full_analysis: "ðŸ“ Direct full document analysis"
    direct_split_batches: "ðŸ“¦ Document split into {0} batches for analysis"
    direct_process_batch: "ðŸ”„ Processing batch {0}/{1}, content length: {2} characters"
    direct_complete: "âœ… Direct document analysis completed: Document={0}, Time={1}ms"
    direct_failed: "âŒ Direct document analysis failed"
    split_batches: "ðŸ“¦ Document split into {0} batches"
    process_batch: "ðŸ”„ Processing batch {0}/{1} (Size: {2} characters)"
    batch_key_points: "ðŸ’¡ Batch {0} key information extracted ({1} characters)"
    batch_complete: "âœ… Batch {0}/{1} processing completed"
    analysis_failed: "Analysis failed"
    final_report_failed: "Failed to generate final report"
    direct_process_failed: "Failed to process document directly"
  error:
    document_not_found: "Document not found: {0}"
    processing_failed: "Document processing failed"
  prompt:
    analysis_task: "# Document Analysis Task\n\n"
    document_info: "## Document Information\n"
    filename: "- Filename: {0}\n"
    content_length: "- Content Length: {0} characters\n\n"
    user_question: "## User Question\n"
    full_content: "## Full Document Content\n"
    analysis_requirements: "\n\n## Analysis Requirements\n"
    req_read_content: "1. Carefully read the complete document content above\n"
    req_answer_question: "2. Directly answer the user's question based on the document content\n"
    req_structured_answer: "3. Provide a structured, organized answer\n"
    req_quote_data: "4. If there is key data, please quote accurately\n"
    req_organize_content: "5. Use headings, lists, etc. to organize content\n"

# PPT Analysis Service
ppt_analysis:
  log:
    start_analysis: "ðŸ“Š Starting progressive PPT analysis: {0} ({1} slides)"
    analyze_slide: "ðŸ” Analyzing slide {0}/{1}"
    slide_complete: "âœ… Slide {0} analysis completed, key points: {1}"
    analysis_complete: "ðŸŽ‰ PPT progressive analysis completed, time: {0}ms"
    analysis_failed: "PPT analysis failed"
    slide_failed: "Slide {0} analysis failed"
    generate_summary: "ðŸ“Š Generating PPT comprehensive summary..."
    summary_complete: "âœ… Comprehensive summary generated"
    summary_failed: "Failed to generate comprehensive summary"

# Knowledge Base Service
kb_service:
  log:
    vector_disabled: "Vector search enabled."
    progress: "Processing progress:"
    path_not_exists: "Path does not exist: {0}"
    scan_classpath: "Scanning classpath resources: {0}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    resource_found: "Resource found: {0}"
    resource_file_not_exists: "Resource file does not exist: {0}"
    resource_path: "Resource path: {0}"
    scan_directory: "Scanning files in directory."
    files_found: "Total files found: {0}"
    add_file: "Adding file: {0}"
    scan_classpath_failed: "Failed to scan classpath resources: {0}"
    file_not_exists: "File does not exist: {0}"
    index_single_file: "Starting to index single file: {0}"
    file_indexed: "File indexed successfully: {0}"
    index_file_failed: "Failed to index file: {0}"
    file_too_large: "File too large, size: {0} MB, maximum allowed size: {1} MB"
    content_empty: "Document content is empty."
    image_extraction_failed: "Image extraction failed: {0}"
    force_chunk: "Document content too large, forcing chunking, size: {0} MB"
    auto_chunk: "Document content large, auto chunking, size: {0} KB"
    chunked: "Document chunked successfully, number of chunks: {0}"
    processing_failed: "Document processing failed."
    file_process_failed: "Failed to process file: {0}"
    batch_task_failed: "Batch task execution failed."
    vector_generation_failed: "Vector generation failed: {0}"
    incremental_start: "Starting incremental indexing."
    incremental_complete: "Incremental indexing completed."
    incremental_files: "Number of files for incremental indexing: {0}"
    build_complete: "Knowledge base build completed"
    build_failed: "Knowledge base build failed"
    build_memory: "Knowledge base build memory usage"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build succeeded"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total items processed: {0}"
    memory_after: "Memory after build: {0} MB"
    memory_before: "Memory before build: {0} MB"
    old_kb_cleared: "Old knowledge base cleared"
    rebuild_prepare: "Preparing for knowledge base rebuild..."
    tracking_cleared: "Knowledge base tracking cleared"
    tracking_saved: "Knowledge base tracking saved: {0}"
    batch_processing: "ðŸ“¦ Batch processing: {0} documents ({1} / {2})"
    batch_commit: "ðŸ“ Committing batch index"
    content_extracted: "âœ… Content extracted: {0}"
    content_too_large: "âš ï¸  Content too large ({0} chars)"
    content_truncated: "âœ‚ï¸  Content truncated to {0} chars"
    exists: "ðŸ“š Existing knowledge base detected ({0} documents)"
    files_to_index: "ðŸ“ Files to index: {0}"
    final_batch: "ðŸ“¦ Processing final batch: {0} documents"
    first_create: "ðŸ“š Creating knowledge base for first time"
    found_files: "âœ… Found {0} document files"
    gc_before: "ðŸ—‘ï¸  Executing GC to free memory"
    hint_put_docs: "ðŸ’¡ Hint: Place documents in {0} directory"
    images_added: "âž• {0} images added to index"
    images_extracted: "ðŸ–¼ï¸  Extracted {0} images"
    incremental_done: "\nâœ… Incremental indexing completed!"
    incremental_failed: "âŒ Incremental indexing failed"
    incremental_stats: "   - Processed files: {0}/{1}, Failed: {2}, Total documents: {3}, Time taken: {4}s"
    indexation_complete: "âœ… Indexing completed: {0}"
    indexing_complete: "âœ… Knowledge base indexing complete: {0}"
    no_documents: "âš ï¸  No supported document files found"
    parallel_mode: "ðŸš€ Using parallel processing mode ({0} threads)"
    parallel_progress: "ðŸ“Š Parallel progress: {0}%"
    parallel_memory: "ðŸ’¾ Memory usage: {0} MB"
    processing_file: "ðŸ“„ Processing file: {0}, size: {0} KB"
    processing_start: "\nðŸ“ Starting document processing..."
    scanning: "ðŸ“‚ Scanning documents: {0}"
    serial_mode: "ðŸ“ Using serial processing mode"
    source_path: "   - Document path: {0}"
    supported_formats: "      Supported formats: {0}"
    up_to_date: "âœ… All files are up to date, no updates needed"
    vector_init_failed: "âŒ Vector search engine initialization failed"
    files_to_update: "ðŸ“ Files to update: {0}"
    saved_chunks: "âœ… Saved {0} chunks for document: {1}"
    save_chunks_failed: "âš ï¸ Failed to save chunks for document {0}: {1}"
    preprocess_start: "ðŸ”„ Starting document preprocessing (image extraction + text conversion)..."
    preprocess_complete: "âœ… Document preprocessing completed, final content length: {0}"
    preprocess_failed: "âš ï¸ Document preprocessing failed: {0}"
    ppl_chunking_start: "ðŸ§  Using PPL-based intelligent chunking..."
    ppl_chunking_complete: "âœ… PPL chunking completed: {0} chunks"
    ppl_chunking_failed: "âš ï¸ PPL chunking failed, falling back to traditional chunking: {0}"
    file_item: "   - {0}"

# Knowledge QA Service Logs
log:
  kqa:
    sep: "================================================================================="
    init_start: "ðŸš€ Initializing knowledge QA service"
    init_done: "âœ… Knowledge base Q&A system initialized successfully!"
    init_failed: "âŒ Knowledge Q&A service initialization failed"
    step: "\nðŸš€ Step {0}: {1}"
    init_kb: "Initializing knowledge base"
    rebuild_mode: "   ðŸš€ Starting full knowledge base rebuild..."
    incremental_mode: "   ðŸ”„ Starting incremental knowledge base indexing..."
    storage_path: "   - Storage path: {0}"
    source_path: "   - Source path: {0}"
    build_failed: "Knowledge base build failed: {0}"
  kb:
    vector_init_failed: "Vector search initialization failed"
    scanned_files_count: "ðŸ“‚ Filesystem scan completed, found {0} supported documents"
    scan_failed: "âŒ Filesystem scan failed"

# Document QA Service Prompts
doc_qa:
  log:
    create_temp_dir: "Creating document QA temp directory: {0}"
    create_temp_dir_failed: "Failed to create temp directory"
    start_qa: "ðŸ“„ Starting document QA: {0} (Session ID: {1})"
    question: "â“ Question: {0}"
    batch_mode: "ðŸ“¦ Document is large, enabling batch processing mode"
    direct_mode: "ðŸ“ Document is small, processing directly"
    qa_complete: "âœ… Document QA completed: {0} (Time: {1}ms)"
    qa_error: "âŒ Document QA failed"
    direct_start: "ðŸ“„ Starting direct document analysis (without knowledge base): Document={0}, Session ID={1}"
    direct_question: "â“ Analysis question: {0}"
    direct_content_length: "ðŸ“ Document content length: {0} characters"
    direct_exceed_limit: "ðŸ“¦ Document content exceeds limit ({0}), using memo mechanism for batch processing"
    direct_full_analysis: "ðŸ“ Direct full document analysis"
    direct_split_batches: "ðŸ“¦ Document split into {0} batches for analysis"
    direct_process_batch: "ðŸ”„ Processing batch {0}/{1}, content length: {2} characters"
    direct_complete: "âœ… Direct document analysis completed: Document={0}, Time={1}ms"
    direct_failed: "âŒ Direct document analysis failed"
    split_batches: "ðŸ“¦ Document split into {0} batches"
    process_batch: "ðŸ”„ Processing batch {0}/{1} (Size: {2} characters)"
    batch_key_points: "ðŸ’¡ Batch {0} key information extracted ({1} characters)"
    batch_complete: "âœ… Batch {0}/{1} processing completed"
    analysis_failed: "Analysis failed"
    final_report_failed: "Failed to generate final report"
    direct_process_failed: "Failed to process document directly"
  error:
    document_not_found: "Document not found: {0}"
    processing_failed: "Document processing failed"
  prompt:
    analysis_task: "# Document Analysis Task\\n\\n"
    document_info: "## Document Information\\n"
    filename: "- Filename: {0}\\n"
    content_length: "- Content Length: {0} characters\\n\\n"
    user_question: "## User Question\\n"
    full_content: "## Full Document Content\\n"
    analysis_requirements: "\\n\\n## Analysis Requirements\\n"
    req_read_content: "1. Carefully read the complete document content above\\n"
    req_answer_question: "2. Directly answer the user's question based on the document content\\n"
    req_structured_answer: "3. Provide a structured, organized answer\\n"
    req_quote_data: "4. If there is key data, please quote accurately\\n"
    req_organize_content: "5. Use headings, lists, etc. to organize content\\n"
    batch_analysis_task: "# Document Batch Analysis Task\\n\\n"
    background_info: "## Background Information\\n"
    current_batch: "## Current Batch Content\\n"
    analysis_req: "## Analysis Requirements\\n"
    req_analyze_current: "1. Carefully analyze the content of the current batch\\n"
    req_combine_memory: "2. Understand the overall context by combining with previous key points\\n"
    req_extract_key: "3. Extract the 3-5 most important key information from this batch\\n"
    req_focus_question: "4. Focus on content related to the user's question\\n"
    req_keep_open: "5. This is not the final part, keep it open-ended\\n"
    req_final_summary: "5. This is the final part, can provide a complete summary\\n"
    output_format: "\\nPlease output in the following format:\\n"
    output_format_analysis: "### Current Batch Analysis\\n[Your analysis]\\n\\n"
    output_format_key: "### Key Points\\n[Use - to list 3-5 key points]\\n\\n"
    previous_memory: "## Key Points from Previous Content (Memory Context)\\n"
    final_summary_task: "# Document Analysis Final Summary Task\\n\\n"
    background_summary: "## Background\\n"
    background_description: "You have completed analyzing all content of a document in batches, now need to generate a final comprehensive report.\\n\\n"
    doc_info: "## Document Information\\n"
    analysis_batches: "- Number of analysis batches: {0}\\n\\n"
    key_points_summary: "## Key Points Summary from All Batches\\n"
    current_part: "### Part {0}\\n"
    final_requirements: "## Generation Requirements\\n"
    req_overall_understand: "1. **Overall Understanding**: Synthesize key information from all batches\\n"
    req_answer_question_directly: "2. **Answer Question**: Answer the user's question directly and clearly\\n"
    req_structure_clear: "3. **Clear Structure**: Use headings, lists, etc. to organize content\\n"
    req_extract_core: "4. **Extract Core**: Highlight the 3-5 most core viewpoints\\n"
    req_coherent_expression: "5. **Coherent Expression**: Ensure content is coherent and logically smooth\\n"
    generate_final_report: "Please generate the final comprehensive analysis report:\\n"

# Query Expansion Service
query_expansion:
  init: "ðŸ“Š Query expansion service initialized: {} synonym entries, {} stopwords"
  reverse_index: "ðŸ”„ Synonym reverse index built: {} entries"
  log:
    init: "Query expansion service initialized: {} synonym entries, {} stopwords"
    reverse_index: "Synonym reverse index built: {} entries"
    expand_query: "Expanding query: {}"
    add_synonyms: "Adding synonyms: {}"
    llm_rewrite_success: "LLM rewrite successful: {}"
    llm_rewrite_failed: "LLM rewrite failed: {}"
    synonym_replace: "Synonym replacement: {} -> {}"
    filter_stopwords: "Filtering stopwords: {}"
    add_phrases: "Adding phrases: {}"
    expansion_complete: "Query expansion completed: {} -> {}"

# Active Learning Service
active_learning:
  log:
    init: "Active learning service initialized"
    recommendation_generated: "Generated active learning recommendations: uncertain={}, potential={}, history={}"
    uncertain_docs_found: "Found uncertain documents: {}"
    potential_docs_found: "Found potentially relevant documents: {}"
    history_recommendations: "History-based recommendations: {}"
    confidence_calculated: "Recommendation confidence: {}"
    reason_generated: "Generated recommendation reason: {}"
    save_history: "Saved query history: {}"
    cache_updated: "Updated query cache: {}"

# Log Related
log:
  image:
    service:
      init: "Image extraction service initialized: extractors={}, AI analysis enabled={}"
      extract_failed: "Failed to extract images: {}"
      start: "Starting image extraction: {}"
      no_extractor: "No extractor found for document type: {}"
      using_extractor: "Using extractor: {}"
      no_images: "No images found in document: {}"
      extracted: "Extracted image count: {}"
      saved: "Saved image: {} (type: {}, size: {}KB)"
      save_failed: "Failed to save image: {}"
      success: "Image extraction completed: {} images (document: {})"
      failed: "Image extraction failed: {}"
  query_expansion:
    init: "Query expansion service initialized: {} synonym entries, {} stopwords"
    reverse_index: "Synonym reverse index built: {} entries"
    expand_query: "Expanding query: {}"
    add_synonyms: "Adding synonyms: {}"
    llm_rewrite_success: "LLM rewrite successful: {}"
    llm_rewrite_failed: "LLM rewrite failed: {}"
    synonym_replace: "Synonym replacement: {} -> {}"
    filter_stopwords: "Filtering stopwords: {}"
    add_phrases: "Adding phrases: {}"
    expansion_complete: "Query expansion completed: {} -> {}"
  feedback:
    weight_disabled: "Dynamic weight adjustment disabled"
    weight_updated: "Document weight updated: {} weight={} (likes:{}, dislikes:{})"
  qa:
    record_saved: "QA record saved: {} (file: {})"
    save_failed: "Failed to save QA record"

# Active Learning Service Feedback
active_learning:
  feedback:
    relevant: "relevant"
    irrelevant: "irrelevant"
    processed: "ðŸŽ¯ Active learning feedback: {} -> {} ({})"
  log:
    potential_doc_reason: "This document has received high ratings in the past but did not appear in current search results"
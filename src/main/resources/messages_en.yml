# Flattened messages converted from messages_en.yml (English)
# Generated to support I18n.get(key) lookups
api:
  started: "API server started on port {0}"
banner:
  supports: "Supports: {0}"
  title: "AI Reviewer - Knowledge Base Intelligent Q&A System"
  version: "Version: {0}"
batch:
  failed: "Batch processing failed"
context:
  initialized: "SmartContextBuilder initialized: maxContext={0} chars, maxDoc={1} chars, preserveFullContent={2}"
document_management:
  api:
    error:
      delete_failed: "Deletion failed: {0}"
      file_empty: "File is empty"
      index_failed: "Indexing failed: {0}"
      list_failed: "Failed to retrieve list: {0}"
      not_found: "Document not found"
      upload_failed: "Upload failed: {0}"
    success:
      batch_result: "Success: {0}, Failed: {1}"
      batch_upload: "Batch upload completed"
      delete: "Document deleted successfully"
      index: "Document indexed successfully"
      list_loaded: "Document list retrieved successfully: Returned {0} documents, Total {1}"
      upload: "Document uploaded successfully"
  log:
    scanned_types: "Scanned file types: {0}"
    batch_upload: "Batch upload documents: {0}"
    upload_success: "Document uploaded successfully: {0}"
    upload_failed: "Document upload failed"
    file_upload_failed: "File upload failed: {0}"
    list_documents: "Get document list: page={0}, pageSize={1}, sortBy={2}, sortOrder={3}"
    list_failed: "Failed to get document list"
    auto_index_trigger: "üì¶ Auto-triggering incremental index..."
    auto_index_success: "‚úÖ Auto incremental index completed"
    auto_index_failed: "‚ö†Ô∏è Auto incremental index failed: {0}"
    trigger_auto_index_failed: "‚ö†Ô∏è Failed to trigger auto index: {0}"
document_service:
  error:
    cannot_create_dir: "Failed to create document directory: {0}"
    filename_empty: "Filename is empty"
    file_too_large: "File too large: {0} MB (Max: {1} MB)"
    illegal_path: "Illegal file path"
    unsupported_format: "Unsupported file format: {0}"
  log:
    scanned_types: "Scanned file types: {0}"
error:
  auth:
    invalid_credentials: "Invalid credentials"
    permission_denied: "Permission denied: {0}"
    user_exists: "User already exists"
  chunk:
    expected_json_array: "Expected JSON array, but got: {0}"
    no_valid_chunks: "No valid chunks found in AI response"
    no_valid_json: "No valid JSON found in response"
  docs:
    create_failed: "Failed to create document: {0}"
  file:
    directory_not_exists: "Directory does not exist: {0}"
    empty: "File content is empty: {0}"
    index_failed: "File indexing failed: {0}"
    not_a_directory: "Not a directory: {0}"
    not_a_file: "Not a file: {0}"
    not_exists: "File does not exist: {0}"
feedback:
  api:
    emoji:
      "1": "üòû Completely useless"
      "2": "üôÅ Not helpful"
      "3": "üòê Average"
      "4": "üòä Very helpful"
      "5": "ü§© Extremely useful"
    error:
      invalid_feedback_type: "feedbackType must be LIKE or DISLIKE"
      invalid_rating: "Rating must be between 1-5"
      missing_params: "{0} cannot be empty"
      processing_failed: "Processing failed: {0}"
      record_not_found: "Record not found"
    impact:
      document:
        "1": "This document is not helpful, the system will significantly reduce its recommendation weight ‚ö†Ô∏è"
        "2": "This document is not very helpful, the system will reduce its recommendation weight üìâ"
        "3": "This document is average, the system will keep its current weight ‚û°Ô∏è"
        "4": "This document is very helpful, the system will increase its recommendation weight üìà"
        "5": "This document is extremely useful! The system will prioritize recommending it üöÄ"
      overall:
        "1": "Sorry we couldn't help you! We will focus on improvement üôè"
        "2": "Thank you for your feedback! We will analyze and improve üîß"
        "3": "Thank you for your rating! We will continue to optimize üí°"
        "4": "Thank you for your feedback! This helps us improve answer quality üëç"
        "5": "Great! This answer will be recommended to other users üåü"
    success:
      feedback_received: "Thank you for your feedback!"
      thank_you: "Thank you for your rating!"
image:
  log:
    get_failed: "Failed to retrieve image: {0}/{1}"
    list_failed: "Failed to retrieve document image list: {0}"
  saved: "Image saved: {0} (Document: {1})"
  service:
    saved: "Image saved: {0}"
knowledge_qa_service:
  doc_item: "{0}. {1}"
  error_processing: "Error occurred while processing Q&A: {0}"
  found_chunks_images: "üì¶ Found {0} chunks and {1} images"
  image_guide_1: "1. Below are image resources related to your question from the knowledge base. You can reference these images in your answer."
  image_guide_2: "2. If your answer involves content from these images (e.g., architecture diagrams, flowcharts, data charts), please reference the images using Markdown format."
  image_guide_3: "3. The reference format is provided below, please copy and use it directly."
  image_guide_4: "4. Please ensure the referenced image URL is complete and correct."
  images_in_context: "üñºÔ∏è Context contains {0} images of information"
  # Added keys requested
  close_existing_kb: "Closing existing knowledge base..."
  destroy_start: "Destroying knowledge Q&A service..."
  kb_closed: "Knowledge base closed"
  kb_closed_safe: "Knowledge base closed safely"
  rebuild_start: "Starting knowledge base rebuild..."
  system_closed: "Knowledge Q&A system closed"
  log:
    build_complete: "   ‚úÖ Knowledge base build completed"
    failed_files: "      - Failed files: {0}"
    kb_ready: "   ‚úÖ Knowledge base is ready"
    processed_files: "      - Processed files: {0}"
    total_documents: "      - Total documents: {0}"
    total_files: "      - Total files: {0}"
    chunk_overlap_chars: "Chunk overlap: {0} chars"
    chunk_size_chars: "Chunk size: {0} chars"
    chunking_strategy: "Chunking strategy: {0}"
    create_qa_system: "Creating QA system..."
    document_count: "Documents: {0}"
    index_count: "Index entries: {0}"
    init_llm: "Initializing LLM client..."
    init_vector_engine: "Initializing vector engine: {0}"
    llm_client_ready: "LLM client ready"
    llm_client_type: "LLM client type: {0}"
    llm_provider: "LLM provider: {0}"
    max_context_chars: "Max context chars: {0}"
    max_doc_length_chars: "Max document length chars: {0}"
    smart_context_initialized: "Smart context initialized: maxContext={0}, maxDoc={1}"
    using_keyword_mode: "Using keyword mode for context building"
    record_saved: "QA record saved: {0}"
    using_vector_enhancement: "Using vector enhancement for retrieval"
    vector_count: "Vector index contains {0} vectors"
    vector_dimension: "Vector dimension: {0}"
    vector_engine_loaded: "Vector engine loaded: {0}"
    vector_index_loaded: "Vector index loaded: {0}"
    vector_index_path: "Vector index path: {0}"
    vector_model: "Vector model: {0}"
  model_doc_hint: "Hint: Place model files in {0}"
  model_download_hint: "Hint: Download the model from {0}"
  more_images: "  ... {0} more images"
  question_prompt: "‚ùì Question: {0}"
  question_separator: "\n================================================================================\n"
  referenced_docs: "\n\n**Referenced Documents**:"
  related_image: "Related Images"
  remaining_docs: "‚ÑπÔ∏è {0} more related documents not included in this answer"
  remaining_docs_unprocessed: "‚ÑπÔ∏è {0} more related documents not included in this answer"
  response_time: "\n‚è±Ô∏è  Response time: {0}ms"
  separator: "================================================================================="
  sources_label: "\nüìö Data Sources (Total {0} documents):"
  too_many_docs_retrieved: "‚ö†Ô∏è Retrieved {0} documents, processing first {1} (Config: documents-per-query)"
  using_docs: "üìö Used {0} documents to generate this answer"
  using_hybrid_search: "‚úÖ Using hybrid search (Lucene + Vector)"
  answer_label: "\nüí° Answer:"
  available_images: "Available images: {0}"
  context_stats: "Context: {0} chars from {1} documents"
  create_session: "Creating QA session: {0}"
  image_item: "Image {0}: {1} (Source: {2})"
  important_notice: "Important: {0}"
  using_keyword_search: "Using keyword search mode"
  classpath_prefix: "Classpath: {0}"
  closing_existing_kb: "Closing existing knowledge base before proceeding..."
  debug_enhanced_stats: "üìä Statistics: Filesystem={0} files, Indexed={1} docs, Unindexed={2}, Progress={3}%"
  debug_enhanced_stats_v2: "üìä Detailed Stats: Filesystem={0} files, Unique docs={1}, Index chunks={2}, Unindexed={3}, Progress={4}%"
  existing_kb_closed: "Existing knowledge base closed"
  failed_files: "      - Failed files: {0}"
  incremental_index_start: "Starting incremental indexing..."
  more_docs_notice: "\n‚ÑπÔ∏è {0} more related documents not included in this answer"
  question_label: "‚ùì Question:"
  success_files: "      - Successful files: {0}"
  total_documents: "      - Total documents: {0}"
  using_session_docs: "Using session-stored documents: {0}"

# Knowledge QA API messages and logs
knowledge_qa:
  api:
    message:
      all_indexed: "All documents indexed"
      incremental_complete: "Incremental indexing completed"
      system_running: "Knowledge QA system is running"
      needs_indexing: "Knowledge base needs indexing"
      rebuild_complete: "Knowledge base rebuild completed"
    status:
      up: "Running"
  log:
    received_question: "Received question: {0}"
    get_statistics: "Retrieving knowledge QA statistics"
    incremental_request: "Incremental indexing requested: {0}"
    statistics_result: "Statistics result: {0}"
    search_documents: "Search documents for question: {0} (found {1})"
    session_question: "Session question [{0}]: {1}"
    rebuild_request: "Rebuild requested: {0}"
    indexing_in_progress: "Indexing in progress"
    indexing_idle: "System idle, ready for indexing"
# Vision LLM Strategy Messages
vision_llm:
  log:
    api_format_detected: "Detected {0} API format"
    api_format_default: "Using default OpenAI Chat Completions API format"
    processing_image: "Using Vision LLM to process image: {0}"
    processing_image_file: "Using Vision LLM to process image file: {0}"
    content_extracted: "Vision LLM extracted content [{0}]: {1} characters"
    processing_failed: "Vision LLM processing failed: {0}"
    sending_request: "Sending Vision API request: {0} (format: {1})"
    sending_request_batch: "Sending batch Vision API request: {0} (format: {1})"
    received_response: "Received Vision API response, elapsed: {0}ms"
    token_usage: "Token usage - Prompt: {0}, Completion: {1}, Total: {2}"
    ollama_complete_chat: "Ollama processing completed (/api/chat format)"
    ollama_complete_generate: "Ollama processing completed (/api/generate format)"
    service_available: "‚úÖ Vision LLM available"
    api_format: "   - API Format: {0}"
    model: "   - Model: {0}"
    endpoint: "   - Endpoint: {0}"
    service_unavailable: "‚ö†Ô∏è  Vision LLM service unavailable: {0}"
    check_service: "üí° Hint: Please check if the service is running properly"
    unavailable_no_apikey: "‚ö†Ô∏è  Vision LLM unavailable: API Key not configured"
    hint_set_apikey: "üí° Hint: Set vision-llm.api-key in configuration file"
    service_response_error: "Service response error: HTTP {0}"
    unsupported_format: "‚ö†Ô∏è  Skipping unsupported image format: {0} (.{1}), Vision API only supports jpg/png/gif/webp/bmp"
  error:
    api_error: "Vision API error: HTTP {0}"
    api_error_with_body: "Vision API error: HTTP {0}, Body: {1}"
    no_response_body: "No response body"
    parse_openai_failed: "Failed to parse OpenAI API response: {0}"
    parse_ollama_failed: "Failed to parse Ollama API response: {0}"
    unavailable: "[Image: {0} - Vision LLM unavailable]"
    processing_failed: "[Image: {0} - Vision LLM processing failed: {1}]"
    unsupported_format: "[Image: {0} - Unsupported format .{1}, Vision API only supports jpg/png/gif/webp/bmp]"
  prompt:
    # Concise prompt: extract key information, avoid redundancy
    extract_text_concise: |
      Please analyze this image and extract **key information**. Be concise but don't miss important content:

      1. **Image Theme**: One sentence summarizing core content and purpose

      2. **Core Text**: Extract all important text content
         - Titles, main labels, key annotations
         - Important data/numbers/parameters
         - Core conclusions or key points

      3. **Key Structural Information** (based on image type, keep only essential content):
         „ÄêArchitecture/System Diagrams„Äë:
         - All component/module names
         - Connections between components and data flow
         - Tech stack, protocols (if any)

         „ÄêFlowcharts„Äë: Key steps and branch logic

         „ÄêCharts/Graphs„Äë: Core data points, trend conclusions

         „ÄêTables„Äë: Headers and key data

         „ÄêOthers„Äë: Main content elements

      **Output Requirements**:
      - Be concise, avoid redundant descriptions and subjective comments
      - But must retain all key information, don't miss important content
      - For complex diagrams like architecture, preserve complete component and relationship information

    # Detailed prompt: full analysis
    extract_text: |
      Please comprehensively analyze this image and provide output in the following structure:
      
      1. **Image Overview**: Summarize the main content, type, and purpose of the image in 1-2 sentences
      
      2. **Text Content**: Extract all text from the image completely
         - Titles, labels, annotations
         - Component names, module names
         - Descriptions, legends
         - Maintain hierarchical and positional relationships of text
      
      3. **Structured Information** (describe in detail based on image type):
      
         „ÄêArchitecture/System Diagrams„Äë:
         - List all components/modules/nodes with their names
         - Explain connections between components (who connects to whom, direction)
         - Describe data flow or information flow directions
         - Explain hierarchical relationships of components (frontend, backend, database, etc.)
         - Note technology stacks, protocols, ports used
      
         „ÄêFlowcharts„Äë:
         - List all process steps in order
         - Explain conditional branches and decision logic
         - Mark start and end points
      
         „ÄêCharts/Graphs„Äë:
         - Chart type (bar/line/pie chart, etc.)
         - Axis meanings and units
         - Specific data points and values
         - Data trends and key findings
      
         „ÄêTables„Äë:
         - Header information
         - Extract data completely by rows and columns
         - Maintain table structure relationships
      
         „ÄêScreenshots/Interfaces„Äë:
         - Interface type and functionality
         - Main functional modules
         - Interactive elements like menus, buttons
      
         „ÄêOther Images„Äë:
         - Describe main content and composition
         - Key visual elements
      
      4. **Key Information Extraction**:
         - Core technical points
         - Important data or parameters
         - Key conclusions or points
         - Information requiring special attention
      
      Output Requirements:
      - Information must be complete, accurate, and structured
      - For architecture diagrams, must detail component connections
      - Technical terms, numbers, formulas must be precisely identified
      - Use clear hierarchical structure to organize content
      - No subjective comments, only state objective information
kb_service:
  image:
    description: "Image description: {0}"
    filename: "Filename: {0}"
    image_number: "Image {0}/{1}"
    original_file: "Original file: {0}"
    section_end: "End of section"
    section_title: "Section: {0}"
    url: "URL: {0}"
llm:
  error:
    openai_failed: "OpenAI API call failed: {0}"
    openai_http_error: "OpenAI API error: HTTP {0}, {1}"
    parse_failed: "Failed to parse OpenAI response: {0}"
  log:
    mock_init: "‚úÖ Mock LLM client initialized (for testing only)"
    mock_request: "Mock LLM received request, prompt length: {0}"
    mock_response: "üìù Mock LLM returned simulated answer"
    openai_error: "OpenAI API error: HTTP {0}, Body: {1}"
    openai_failed: "OpenAI API call failed"
    openai_init: "‚úÖ OpenAI LLM client initialized"
    openai_request: "Sending request to OpenAI: {0}"
    openai_response: "OpenAI response content: {0}"
  mock:
    default_answer: "This is a simulated answer.\n\nBased on the provided context, I understand your question. However, as a Mock LLM, I can only provide simulated responses.\n\nPlease configure a real LLM service (e.g., OpenAI) to get accurate answers.\n\n(Note: This is a simulated answer from Mock LLM)"
    marriage_answer: "According to the document content, marriage statistics include the distribution of unmarried, married, divorced, widowed and other statuses.\n\n(Note: This is a simulated answer from Mock LLM, please refer to the document content for actual data)"
    population_answer: "According to the document content, the total population of China is approximately 1.4 billion.\n\n(Note: This is a simulated answer from Mock LLM, please refer to the document content for actual data)"
log:
  admin:
    cache_cleared: "Cache cleared"
    cache_cleared_success: "Cache cleared successfully"
    clear_cache_failed: "Failed to clear cache"
    clear_cache_failed_detail: "Failed to clear cache: {0}"
    index_optimize_failed: "Index optimization failed"
    index_optimize_failed_detail: "Index optimization failed: {0}"
    index_optimized: "Index optimized"
    index_optimized_success: "Index optimized successfully"
    stats_failed: "Failed to retrieve statistics"
    stats_failed_detail: "Failed to retrieve statistics: {0}"
  app:
    started: "‚úÖ Application started successfully, Address: {0}"
    start_failed: "‚ùå Application startup failed"
  api:
    stopped: "API server stopped"
  audit:
    init_failed: "Audit log initialization failed"
    logged: "Audit logged: {0}"
    write_failed: "Failed to write audit log"
  auth:
    logged_in: "User logged in: {0}"
    logged_out: "User logged out: {0}"
    registered: "User registered: {0}"
  session:
    create: "Creating session: id={0}, totalDocuments={1}, perQuery={2}"
    no_next: "No next page for session: {0}"
    next: "Session {0} advanced to offset {1}"
    no_prev: "No previous page for session: {0}"
    prev: "Session {0} moved back to offset {1}"
    invalid_page: "Invalid page number"
    page_out_of_range: "Requested page {0} out of range (totalPages={1})"
    goto_page: "Going to page {0} for session {1}, offset {2}"
    deleted: "Session deleted: {0}"
    cleaned: "Cleaned expired session: {0}"
    cleaned_count: "Cleaned {0} expired sessions"
    not_found: "Session not found: {0}"
  hybrid:
    found_docs: "Hybrid search found {0} documents"
    keyword_search: "Hybrid keyword search executed: {0}"
    completed: "Hybrid search completed: Time {0}ms"
    detail_item: "{0}. {1} (score={2})"
    extract_keywords: "Extracted keywords: {0}"
    filtered: "Filtered {0} items"
    lucene_found: "Lucene found {0} documents"
    lucene_top_header: "Top Lucene results:"
    lucene_top_item: "{0}. {1} (luceneScore={2})"
    top5_header: "Top 5 combined results:"
    top5_item: "{0}. {1} (score={2})"
    topk_header: "Top K results:"
    vector_found: "Vector search found {0} documents"
    feedback_weight_applied: "üìä Applied feedback weight adjustments to {0} documents"
    feedback_weight_detail: "üìä Document weight adjusted: {0} | Original: {1} | Weight: {2} | Adjusted: {3}"
  similar:
    found: "Found {0} similar documents"
  chunk:
    ai_completed: "AI semantic chunking completed: {0} chars -> {1} chunks, Time taken: {2}ms"
    ai_failed: "AI semantic chunking failed, falling back to smart keyword chunking"
    ai_not_enabled: "AI chunking not enabled in configuration"
    ai_parse_failed: "Failed to parse AI semantic chunking response: {0}"
    ai_start: "Starting AI semantic chunking, content length: {0} chars"
    content_truncate: "Content too large ({0} chars), truncated to {1} chars to prevent OOM"
    coverage_low: "Low coverage, adding sequential chunking"
    delete_failed: "Failed to delete file: {0}"
    deleted_all: "All chunks of the document deleted: {0}"
    keywords_extracted: "Extracted {0} keywords from query: {1}"
    max_chunks_reached: "Maximum chunk limit ({0}) reached, stopping chunking"
    no_keywords_fallback: "No keywords found, falling back to simple chunking"
    read_meta_failed: "Failed to read chunk metadata: {0}"
    save_failed: "Failed to save document chunk: {0} (Index: {1})"
    saved: "{0} document chunks saved to document: {1}"
    simple_summary: "Simple chunking: {0} chars -> {1} chunks (size={2}, overlap={3})"
    smart_summary: "Smart keyword chunking: {0} chars -> {1} chunks, {2} keywords"
    storage:
      created: "Document chunk storage directory created: {0}"
      init_failed: "Document chunk storage initialization failed"
    truncate_warning: "Content too long ({0} chars), truncated to {1} chars for AI chunking"
  chunker:
    ai_disabled: "AI chunking not enabled, falling back to SMART_KEYWORD strategy"
    creating: "Creating document chunker: strategy={0}, chunkSize={1}, overlap={2}"
    llm_null: "LLM client is null, falling back to SMART_KEYWORD strategy"
    unknown_strategy: "Unknown chunking strategy: {0}, using SMART_KEYWORD"
  docs:
    classpath_in_jar: "Classpath path is inside JAR, writing not supported"
    classpath_load_failed: "Failed to load resource from classpath: {0} - {1}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    classpath_realpath: "Using classpath real path: {0}"
    classpath_resource_found: "‚úÖ Resource found from classpath: {0}"
    create:
      failed: "Failed to create document: {0}"
      success: "Document created successfully"
    create_failed: "Failed to create document directory: {0}"
    created: "Document created: {0}"
    delete:
      failed: "Failed to delete document: {0}"
      success: "Document deleted successfully"
    deleted: "Document deleted: {0}"
    directory_ready: "Document directory is ready: {0}"
    file_exists_renamed: "File already exists, renamed to: {0}"
    get:
      failed: "Failed to retrieve document: {0}"
    invalid_directory: "Invalid directory: {0}"
    list:
      exception: "Exception while listing documents: {0}"
      failed: "Failed to list documents: {0}"
    not_found: "Document not found: {0}"
    process_failed: "Failed to process file: {0}"
    saved: "Document saved: {0}"
    scanned_types: "Scanned file types: {0}"
    total: "Total documents: {0}"
    update:
      failed: "Failed to update document: {0}"
      success: "Document updated successfully"
    updated: "Document updated: {0}"
    upload_to_external: "Uploaded documents will be saved to external path: ./data/documents"
    using_default_path: "Using default path: ./data/documents"
    using_filesystem: "Using filesystem path: {0}"
    walk_failed: "Failed to traverse directory: {0}"
  factory:
    create_caffeine: "Creating Caffeine cache engine"
    create_filesystem: "Creating filesystem storage engine"
    create_lucene: "Creating Lucene index engine"
  feedback:
    document_failed: "Failed to process document feedback"
    document_received: "Document feedback received {0}: recordId={1}, document={2}"
    get_pending_failed: "Failed to retrieve pending QA records"
    get_record_failed: "Failed to retrieve QA record"
    get_recent_failed: "Failed to retrieve recent QA records"
    get_statistics_failed: "Failed to retrieve QA statistics"
    load_failed: "Failed to load document weights"
    overall_failed: "Failed to process overall feedback"
    overall_received: "Overall feedback received: recordId={0}, rating={1}"
    overall_rating_submitted: "Overall rating submitted {0} [{1}]: {2} stars"
    rating_failed: "Failed to process rating"
    rating_submitted: "Rating submitted {0} [{1}]: {2} - {3} stars (Impact: {4})"
    rating_updated: "üìä Document weight updated (by rating): {0} -> Weight={1} ({2} stars, Adjustment {3}, üëç{4} üëé{5})"
    save_failed: "Failed to save document weights"
    weight_disabled: "Dynamic weight adjustment disabled"
    weight_reset: "üîÑ Document weight reset: {0} -> {1}"
    weight_updated: "üìä Document weight updated: {0} -> Weight={1} (üëç{2} üëé{3})"
    weights_cleared: "üßπ All document weights cleared"
    weights_loaded: "üìÇ Document weights loaded: {0} documents"
    weights_saved: "üíæ Document weights saved: {0} documents"
    weights_file_not_exists: "üìÇ Document weight file not found, using default weights"
  image:
    saved: "Image saved: {0} (Document: {1})"
    delete_failed: "Failed to delete file: {0}"
    deleted_all: "All images of the document deleted: {0}"
    excel:
      extracted: "Extracted {0} images from Excel"
      legacy:
        extracted: "Extracted {0} images from legacy Excel"
        processing: "Processing legacy Excel images"
      processing: "Processing Excel images"
    pdf:
      extracted: "Extracted {0} images from PDF"
      processing: "Processing PDF images"
    ppt:
      found: "PPT images found: {0}"
      processing: "Processing PPT images"
      extracted: "Extracted {0} images from PPT"
    read_info_failed: "Failed to read image information: {0}"
    service:
      saved: "Image saved: {0}"
      extracted: "Extracted {0} images"
      init: "Image service initialized"
      no_images: "No images found"
      start: "Starting image extraction"
      success: "Image processing completed"
      using_extractor: "Using image extractor: {0}"
    storage:
      created: "Image storage directory created: {0}"
      init_failed: "Image storage initialization failed"
  imageproc:
    llm_fallback_vision: "‚ö†Ô∏è  Primary LLM does not support image input, falling back to vision-llm configuration"
    check_llm_support: "Checking if LLM client supports images: Model={0}, Image support={1}"
    tessdata_default: "System default"
    activated: "‚úÖ Image processing strategy activated: {0}"
    add_ocr: "   Adding OCR strategy:"
    add_vision: "   Adding Vision LLM strategy:"
    add_llm_vision: "‚úÖ Adding LLM Vision strategy (reusing main LLM client)"
    init: "üñºÔ∏è  Initializing image processing functionality..."
    extraction_mode: "üì∏ Image extraction mode: {0}"
    language: "      - Recognition language: {0}"
    llm_vision_client_type: "   - Client type: {0}"
    llm_vision_model: "   - Model: {0}"
    llm_vision_no_client: "‚ö†Ô∏è  Cannot add LLM Vision strategy: LLMClient not initialized"
    llm_vision_no_image_support: "‚ö†Ô∏è  Cannot add LLM Vision strategy: Model {0} does not support image input"
    llm_vision_available: "‚úÖ LLM Vision strategy available"
    llm_vision_unavailable: "‚ö†Ô∏è  LLM Vision strategy unavailable"
    ocr_available: "   ‚úÖ OCR strategy available"
    ocr_hint: "   üí° Hint: Add dependency net.sourceforge.tess4j:tess4j:5.9.0"
    ocr_unavailable: "   ‚ö†Ô∏è OCR strategy unavailable: Missing tess4j dependency"
    placeholder: "   Using placeholder strategy (default)"
    strategy: "   Configured strategy: {0}"
    strategy_selected: "‚úÖ Selected image processing strategy: {0}"
    strategy_none: "‚ö†Ô∏è  No available image processing strategy, using placeholder"
    extract_failed: "Image content extraction failed: {0}"
    extract_error: "[Image: {0} - Extraction failed]"
    image_placeholder: "[Image: {0}]"
    tessdata: "      - Tessdata path: {0}"
    vision_apikey_hint: "   üí° Hint: Set environment variable VISION_LLM_API_KEY or configure knowledge.qa.image-processing.vision-llm.api-key"
    vision_available: "   ‚úÖ Vision LLM strategy available"
    vision_endpoint: "      - Endpoint: {0}"
    vision_model: "      - Model: {0}"
    vision_no_apikey: "   ‚ö†Ô∏è Vision LLM enabled but API Key not configured"
    vision_unavailable: "   ‚ö†Ô∏è Vision LLM strategy unavailable"
    position:
      simple_desc: "Image {0}: {1}"
      full_desc: "Image {0}: {1} (Position: {2}, Coordinates: {3},{4}, Size: {5}x{6})"
      top: "top"
      bottom: "bottom"
      middle: "middle"
      left: "left"
      right: "right"
      center: "center"
  office:
    pptx_start: "Starting to process PPTX file: {0}, total {1} slides"
    pptx_complete: "‚úÖ PPTX processing completed: {0}"
    pptx_failed: "Failed to process PPTX file: {0}"
    docx_start: "Starting to process DOCX file: {0}"
    docx_complete: "‚úÖ DOCX processing completed: {0}"
    docx_failed: "Failed to process DOCX file: {0}"
    xlsx_start: "Starting to process XLSX file: {0}, total {1} sheets"
    xlsx_complete: "‚úÖ XLSX processing completed: {0}"
    xlsx_failed: "Failed to process XLSX file: {0}"
    slide_title: "\n\n========== Slide {0} ==========\n"
    slide_text: "„ÄêText Content„Äë\n"
    sheet_title: "\n\n========== Sheet: {0} ==========\n"
    image_section: "\n\n========== Document Images ==========\n"
    image_content: "\n„ÄêImage Content„Äë\n"
    extract_image: "üì∑ Extracting image: {0} ({1}KB)"
    extract_success: "‚úÖ Image content extracted successfully: {0} -> {1} characters"
    extract_empty: "‚ö†Ô∏è  Image content is empty: {0}"
    slide_images: "Slide {0} contains {1} images"
    sheet_images: "Sheet {0} contains {1} images"
    docx_images: "‚úÖ Extracted {0} images from DOCX"
    xlsx_extract_failed: "Failed to extract XLSX images"
    process_failed: "\n[Processing failed: {0}]\n"
    batch_config: "Batch processing config: {0} slides per batch"
    processing_slides: "üì¶ Processing slides {0}-{1}/{2}"
    use_cache: "üíæ Using cache: Slide {0} ({1} images)"
    need_process: "üì∏ Need to process {0} images (from {1} slides)"
    batch_complete: "‚úÖ Batch analysis completed: {0} images -> {1} characters"
    cache_stats: "üíæ Cache statistics: {0} cached, {1} processed, {2} total"
    save_image: "üíæ Saving image: {0} -> {1}/{2}"
    save_image_failed: "Failed to save image: {0} - {1}"
  kb:
    vector_enabled: "Vector search enabled."
    progress: "Processing progress:"
    path_not_exists: "Path does not exist: {0}"
    scan_classpath: "Scanning classpath resources: {0}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    resource_found: "Resource found: {0}"
    resource_file_not_exists: "Resource file does not exist: {0}"
    resource_path: "Resource path: {0}"
    scan_directory: "Scanning files in directory."
    files_found: "Total files found: {0}"
    add_file: "Adding file: {0}"
    scan_classpath_failed: "Failed to scan classpath resources: {0}"
    file_not_exists: "File does not exist: {0}"
    index_single_file: "Starting to index single file: {0}"
    file_indexed: "File indexed successfully: {0}"
    index_file_failed: "Failed to index file: {0}"
    file_too_large: "File too large, size: {0} MB, maximum allowed size: {1} MB"
    content_empty: "Document content is empty."
    image_extraction_failed: "Image extraction failed: {0}"
    force_chunk: "Document content too large, forcing chunking, size: {0} MB"
    auto_chunk: "Document content large, auto chunking, size: {0} KB"
    chunked: "Document chunked successfully, number of chunks: {0}"
    processing_failed: "Document processing failed."
    file_process_failed: "Failed to process file: {0}"
    batch_task_failed: "Batch task execution failed."
    vector_generation_failed: "Vector generation failed: {0}"
    incremental_start: "Starting incremental indexing."
    incremental_complete: "Incremental indexing completed."
    incremental_files: "Number of files for incremental indexing: {0}"
    build_complete: "Knowledge base build completed"
    build_failed: "Knowledge base build failed"
    build_memory: "Knowledge base build memory usage"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build succeeded"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total items processed: {0}"
    memory_after: "Memory after build: {0} MB"
    memory_before: "Memory before build: {0} MB"
    old_kb_cleared: "Old knowledge base cleared"
    rebuild_prepare: "Preparing for knowledge base rebuild..."
    tracking_cleared: "Knowledge base tracking cleared"
    tracking_saved: "Knowledge base tracking saved: {0}"
    batch_processing: "üì¶ Batch processing: {0} documents ({1} / {2})"
    batch_commit: "üìù Committing batch index"
    content_extracted: "‚úÖ Content extracted: {0}"
    content_too_large: "‚ö†Ô∏è  Content too large ({0} chars)"
    content_truncated: "‚úÇÔ∏è  Content truncated to {0} chars"
    exists: "üìö Existing knowledge base detected ({0} documents)"
    files_to_index: "üìù Files to index: {0}"
    final_batch: "üì¶ Processing final batch: {0} documents"
    first_create: "üìö Creating knowledge base for the first time"
    found_files: "‚úÖ Found {0} document files"
    gc_before: "üóëÔ∏è  Executing GC to free memory"
    hint_put_docs: "üí° Hint: Place documents in {0} directory"
    images_added: "‚ûï {0} images added to index"
    images_extracted: "üñºÔ∏è  Extracted {0} images"
    incremental_done: "\n‚úÖ Incremental indexing completed!"
    incremental_failed: "‚ùå Incremental indexing failed"
    incremental_stats: "   - Processed files: {0}/{1}, Failed: {2}, Total documents: {3}, Time taken: {4}s"
    indexation_complete: "‚úÖ Indexing completed: {0}"
    indexing_complete: "‚úÖ Knowledge base indexing complete: {0}"
    no_documents: "‚ö†Ô∏è  No supported document files found"
    parallel_mode: "üöÄ Using parallel processing mode ({0} threads)"
    parallel_progress: "üìä Parallel progress: {0}%"
    parallel_memory: "üíæ Memory usage: {0} MB"
    processing_file: "üìÑ Ê≠£Âú®Â§ÑÁêÜÊñá‰ª∂Ôºö{0}ÔºåÂ§ßÂ∞èÔºö{0} KB"
    processing_start: "\nüìù Starting document processing..."
    scanning: "üìÇ Scanning documents: {0}"
    serial_mode: "üìù Using serial processing mode"
    source_path: "   - Document path: {0}"
    supported_formats: "      Supported formats: {0}"
    up_to_date: "‚úÖ All files are up to date, no updates needed"
    vector_init_failed: "‚ùå Vector search engine initialization failed"
    files_to_update: "üìù Files to update: {0}"
    saved_chunks: "‚úÖ Saved {0} chunks for document: {1}"
    save_chunks_failed: "‚ö†Ô∏è Failed to save chunks for document {0}: {1}"
    preprocess_start: "üîÑ Starting document preprocessing (image extraction + text conversion)..."
    preprocess_complete: "‚úÖ Document preprocessing completed, final content length: {0}"
    preprocess_failed: "‚ö†Ô∏è Document preprocessing failed: {0}"
    ppl_chunking_start: "üß† Using PPL-based intelligent chunking..."
    ppl_chunking_complete: "‚úÖ PPL chunking completed: {0} chunks"
    ppl_chunking_failed: "‚ö†Ô∏è PPL chunking failed, falling back to traditional chunking: {0}"
    file_item: "   - {0}"
  kqa:
    response_time: "\n‚è±Ô∏è  Response time: {0}ms"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build succeeded"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total items processed: {0}"
    memory_after: "Memory after build: {0} MB"
    memory_before: "Memory before build: {0} MB"
    tracking_saved: "KB tracking saved: {0}"
    vector_enabled: "Vector indexing enabled: {0}"
    sep: "================================================================================="
    # Added missing kb/log keys
    build_complete: "Knowledge base build completed: {0}"
    answer_header: "\nüí° Answer:"
    build_failed: "Knowledge base build failed: {0}"
    docs_dir_missing: "‚ö†Ô∏è Document directory does not exist: {0}"
    incremental_complete: "Incremental indexing completed"
    incremental_error: "Incremental indexing error"
    incremental_failed: "Incremental indexing failed: {0}"
    incremental_mode: "   üîÑ Starting incremental knowledge base indexing..."
    init_done: "‚úÖ Knowledge base Q&A system initialized successfully!"
    init_failed: "‚ùå Knowledge Q&A service initialization failed"
    init_kb: "Initializing knowledge base"
    init_start: "üöÄ Initializing knowledge Q&A service"
    kb_not_initialized: "Knowledge base not initialized"
    load_chunks_images_failed: "Failed to load chunks/images information"
    more_docs_available: "‚ÑπÔ∏è {0} more related documents not included in this answer"
    rebuild_complete: "Knowledge base rebuilt successfully"
    rebuild_error: "Knowledge base rebuild error"
    rebuild_failed: "Knowledge base rebuild failed: {0}"
    rebuild_mode: "   üöÄ Starting full knowledge base rebuild..."
    recover_failed: "Knowledge base recovery failed"
    recover_kb: "Recovering knowledge base"
    reinit_complete: "Knowledge base reinitialized successfully"
    reinit_kb: "Reinitializing knowledge base"
    scanned_files_count: "üìÇ Filesystem scan completed, found {0} supported documents"
    scan_failed: "‚ùå Filesystem scan failed"
    source_path: "   - Source path: {0}"
    sources_header: "\nüìö Data Sources (Total {0} documents):"
    step: "\nüöÄ Step {0}: {1}"
    storage_path: "   - Storage path: {0}"
    system_not_initialized: "Knowledge Q&A system not initialized"
    used_docs_count: "üìö Used {0} documents to generate this answer"
  llm:
    api_key_deepseek: "      - DeepSeek: export AI_API_KEY=your-deepseek-key"
    api_key_hint: "üí° Hint: Set environment variables:"
    api_key_missing: "‚ö†Ô∏è LLM API Key not configured"
    api_key_openai: "      - OpenAI: export OPENAI_API_KEY=your-openai-key"
    client_created: "ü§ñ Created {0} LLM client"
    fallback_mock: "üí° Falling back to Mock mode"
    model: "   - Model: {0}"
    api_url: "   - API: {0}"
    mock_created: "ü§ñ Created Mock LLM client (for testing only)"
    mock_hint: "üí° To use real LLM, configure:"
    mock_provider: "      knowledge.qa.llm.provider=openai"
    mock_apikey: "      And set corresponding API Key and URL"
    mock_warning: "‚ö†Ô∏è Mock mode will return fixed simulated answers"
  memory:
    usage: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}%)"
    usage_phase: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}% )"
    warning: "Phase {0} - High memory usage: {1}%"
    gc_trigger: "‚ö†Ô∏è  High memory usage ({0}%), triggering garbage collection"
    gc_done: "‚úÖ Garbage collection completed: {0}% -> {1}%"
  optimization:
    chunker:
      batch_completed: "Batch chunking completed: {0} documents -> {1} chunks"
      chunked: "Document {0} chunked into {1} parts"
      initialized: "DocumentChunker initialized - chunkSize: {0}, overlap: {1}, smartSplit: {2}, maxContentLength: {3}, maxChunks: {4}"
    context:
      built: "Smart context built: {0} chars from {1} documents ({2}% of max)"
      initialized_with_chunker: "SmartContextBuilder initialized (with chunker): strategy={0}, maxContext={1} chars, maxDoc={2} chars, storage={3}"
      remaining_chars: "\n[... {0} more chars not displayed, content extracted by keyword priority]"
    memory:
      gc_freed: "GC completed, approximately {0}MB memory freed"
      gc_no_freed: "GC completed, no significant memory freed"
      suggest_gc: "Suggest executing garbage collection, current memory usage: {0}MB"
  qa:
    archive:
      init: "Initializing QA archiving service"
    archive_failed: "QA archiving failed"
    archived: "High-rated QA archived: rating={0}, Path={1}"
    document_feedback: "Document feedback {0} [{1}]: {2} - {3}"
    feedback_applied: "Feedback applied to document weight: {0}"
    feedback_pending: "Feedback pending review: {0}"
    find_failed: "Failed to find record: {0}"
    load_failed: "Failed to load QA records: {0}"
    marked_as_quality: "QA record marked as high-quality content: [{0}]"
    overall_rating_submitted: "Overall rating submitted {0} [{1}]: {2} stars"
    pending_failed: "Failed to retrieve pending review records"
    rating_applied: "Star rating applied to document weight: {0} ({1} stars -> Adjustment {2})"
    rating_pending: "Star rating pending review: {0} ({1} stars)"
    rating_submitted: "Star rating submitted {0} [{1}]: {2} - {3} stars (Weight adjustment: {4})"
    recent_failed: "Failed to retrieve recent records"
    record_notfound: "Record not found: {0}"
    record_save_failed: "Failed to save QA record"
    record_saved: "QA record saved: {0} - {1}"
    record_update_failed: "Failed to update QA record: {0}"
    record_updated: "QA record updated: {0}"
    records_dir: "QA records storage directory: {0}"
    records_dir_failed: "Failed to create QA records directory: {0}"
    stats_failed: "Failed to calculate QA statistics"
    user_feedback: "User feedback [{0}]: Rating={1}, Content={2}"
  rag:
    batch_indexed: "Batch indexed {0} documents"
    cache_hit: "Query result from cache: {0}"
    closed: "Local File RAG closed successfully"
    close_error: "Error while closing Local File RAG"
    closing: "Closing Local File RAG..."
    delete_failed: "Failed to delete document: {0}"
    deleted_count: "{0} documents deleted"
    deleting_all: "Deleting all documents..."
    doc_deleted: "Document deleted: {0}"
    doc_indexed: "Document indexed successfully: {0}"
    doc_updated: "Document updated: {0}"
    enable_cache: "Cache enabled: {0}"
    enable_compression: "Compression enabled: {0}"
    found_to_delete: "Found {0} documents to delete"
    init: "Local File RAG initialized successfully"
    init_done: "Local File RAG initialized successfully"
    load_content_failed: "Failed to load document content: {0}"
    loaded_content: "Document content loaded: {0}, Length: {1}"
    no_documents: "No documents to delete"
    optimized: "Index optimization completed"
    optimizing: "Optimizing index..."
    search_completed: "Search completed, Time taken: {0}ms, Found {1} results"
    simple_init: "Initializing simple RAG service..."
    simple_init_done: "Simple RAG service initialized successfully"
    storage: "Storage path: {0}"
  filetracking:
    loaded: "File tracking loaded with {0} entries."
    load_failed: "Failed to load file tracking: {0}"
    saved: "File tracking saved with {0} entries."
    save_failed: "Failed to save file tracking: {0}"
    check_failed: "Failed to check file tracking for {0}"
    mark_failed: "Failed to mark file as indexed: {0}"
    clear_failed: "Failed to clear file tracking: {0}"
    cleared: "File tracking cleared."
  model:
    checking: "üîç Checking model files..."
    dir_and_file: "üìÇ Model directory and files are ready"
    found: "‚úÖ Model found: {0}"
    passed: "‚úÖ Model check passed"
    sep: "===================================================="
  storage:
    ai_image_analyzer_init: "Initializing AI image analyzer: enabled={0}, model={1}"
    chunk_storage_init: "Initializing document chunk storage service, Path: {0}"
    document_image_extraction_init: "Initializing document image extraction service: AI analysis={0}"
    image_storage_init: "Initializing image storage service, Path: {0}"
  optimizer:
    commit: "Committing optimizer changes"
    done: "Optimization completed"
    optimize: "Optimizing index"
    saving_vectors: "Saving vector index..."
    vectors_saved: "Vector index saved: {0} vectors"
    save_failed: "Failed to save vector index"
    embedding_closed: "Embedding engine closed"
  tika:
    init: "Tika Document Parser initialized."
    max_content: "Max content length: {0} MB"
    extract_image_metadata: "Extract image metadata: {0}"
    include_image_placeholders: "Include image placeholders: {0}"
    active_image_strategy: "Active image strategy: {0}"
    ocr_config: "OCR configuration details:"
    enable_ocr: "Enable OCR: {0}"
    tessdata: "Tesseract data path: {0}"
    not_set: "Not set"
    ocr_language: "OCR language: {0}"
    ocr_disabled: "OCR is disabled: {0}"
    file_not_exists: "File does not exist: {0}"
    detected_mime: "Detected MIME type: {0}, filename: {1}"
    office_pptx: "Processing PPTX file: {0}"
    office_docx: "Processing DOCX file: {0}"
    office_xlsx: "Processing XLSX file: {0}"
    office_done: "Office file processed: {0}, content length: {1}"
    parsed_file: "File parsed: {0}, content length: {1}"
    parse_failed: "File parsing failed: {0}"
    image_section_start: "\nImage metadata:\n"
    image_item: "Image {0}: {1} = {2}\n"
    embedded_section: "\nEmbedded resources:\n"
    embedded_item: "Embedded resource count: {0}\n"
    image_placeholder: "[Image placeholder {0}]"
    empty_bytes: "Byte array is empty."
    parsed_bytes: "Byte array parsed, MIME type: {0}, content length: {1}"
    parse_bytes_failed: "Byte array parsing failed, MIME type: {0}"
    detect_failed: "MIME type detection failed: {0}"
vector_index:
  log:
    init: "‚úÖ Simple vector index engine initialized"
    index_path: "   - Index path: {0}"
    dimension: "   - Vector dimension: {0}"
    current_count: "   - Current vector count: {0}"
    search_method: "   - Search method: Linear scan (suitable for <100K)"
    add_vector: "Add vector: docId={0}, dim={1}"
    batch_add: "Batch added {0} vectors"
    index_empty: "Index is empty, returning empty result"
    search_complete: "Vector search completed: scanned={0}, filtered={1}, found={2}, time={3}ms"
    delete_vector: "Delete vector: docId={0}"
    save_start: "Start saving vector index..."
    save_complete: "‚úÖ Vector index saved: {0} vectors, file size: {1} KB"
    load_start: "Start loading vector index..."
    load_complete: "‚úÖ Vector index loaded: {0} vectors"
    clear_complete: "‚úÖ Vector index cleared"
  error:
    dimension_mismatch: "Vector dimension mismatch: expected {0}, actual {1}"
    query_dimension_mismatch: "Query vector dimension mismatch: expected {0}, actual {1}"
    vector_dimension_mismatch: "Vector dimension mismatch"
    index_dimension_mismatch: "Vector dimension mismatch: index={0}, expected={1}"
# LLM Result Document Service
llm_result:
  log:
    result_saved: "‚úÖ LLM result saved: {0} -> {1}"
    save_failed: "Failed to save LLM result"
    read_failed: "Failed to read document: {0}"
    pdf_not_implemented: "PDF export not implemented, returning Markdown bytes"
    document_deleted: "‚úÖ Document deleted: {0}"
    delete_failed: "Failed to delete document: {0}"
    download_image_failed: "Failed to download image: {0}"
    add_to_kb: "üìö Auto-added to knowledge base: {0}"
  error:
    save_failed: "Failed to save LLM result: {0}"
  markdown:
    generated_time: "Generated Time"
    source_document: "Source Document"
    analysis_question: "Analysis Question"
    analysis_type: "Analysis Type"
    key_points: "üìå Key Points"
    related_images: "üñºÔ∏è Related Images"
    image_number: "Image {0}"
    footer: "This document was generated by AI intelligent analysis"
# Document Parser
doc_parser:
  log:
    ppt_parsing: "üìä Parsing PPT document: {0}"
    ppt_slide_count: "Slide count: {0}"
    ppt_parse_complete: "PPT parsing complete: {0} segments"
    pdf_parsing: "üìÑ Parsing PDF document: {0}"
    pdf_page_count: "Page count: {0}"
    pdf_parse_complete: "PDF parsing complete: {0} segments"
    word_parsing: "üìù Parsing Word document: {0}"
    word_parse_complete: "Word parsing complete: {0} segments"
    parse_failed: "Document parsing failed: {0}"
  error:
    unsupported_format: "Unsupported document format: {0}"
    file_not_found: "File not found: {0}"
    parse_error: "Parse error: {0}"
# Token Estimator
token_estimator:
  log:
    estimate_complete: "Token estimate: {0} chars ‚âà {1} tokens"
# LLM Result Collector
llm_collector:
  log:
    qa_result_saved: "‚úÖ QA result auto-saved"
    qa_collect_failed: "Failed to collect QA result"
    doc_analysis_saved: "‚úÖ Document analysis result auto-saved"
    doc_analysis_collect_failed: "Failed to collect document analysis result"
    image_analysis_saved: "‚úÖ Image analysis result auto-saved"
    image_analysis_collect_failed: "Failed to collect image analysis result"
    session_start: "üìù Session collection started: {0} ({1})"
    session_not_found: "Session not found: {0}"
    session_add: "‚ûï Added to session {0}: {1}"
    session_empty: "Session is empty or not found: {0}"
    session_saved: "‚úÖ Session results saved: {0} ({1} segments)"
    session_save_failed: "Failed to save session results: {0}"
    session_cancelled: "‚ùå Session cancelled: {0}"
  title:
    qa_prefix: "Q&A: "
    doc_analysis_prefix: "Document Analysis: "
    image_analysis: "Image Analysis"
    progressive_analysis: "Progressive Analysis"
  section:
    analysis_process: "Analysis Process"
    comprehensive_summary: "Comprehensive Summary"
# ============================================================
# PPL Service Configuration (PPLConfiguration)
# ============================================================
ppl:
  log:
    onnx_init_start: "üöÄ Initializing ONNX PPL service..."
    onnx_init_success: "‚úÖ ONNX PPL service initialized successfully"
    onnx_init_failed: "‚ùå ONNX PPL service initialization failed: {0}"
    facade_init_start: "üöÄ Initializing PPL service facade..."
    facade_init_success: "‚úÖ PPL service facade initialized successfully, default provider: {0}"
# ============================================================
# DocumentManagementController
# ============================================================
doc_management:
  log:
    pagination: "Pagination: Page {0}, {1} items per page, total {2} pages, returning {3} items"
    sort_complete: "Sorting completed: {0} {1}"
    search_mode_failed: "Search mode '{0}' processing failed: {1}"
    date_filter_failed: "Date filter processing failed: {0}"
    delete_document: "Deleting document: {0}"
    delete_failed: "Failed to delete document"
    download_document: "Downloading document: {0}"
    filename_bytes: "Filename bytes: {0}"
    find_path: "Finding path: {0}"
    file_not_exists: "‚ùå File does not exist: {0} (Path: {1})"
    possible_reasons: "üí° Possible reasons:"
    reason_deleted: "   1. File only exists in knowledge base index but source file has been deleted"
    reason_special_chars: "   2. Filename contains special characters causing path parsing error"
    reason_not_uploaded: "   3. File was never uploaded to documents directory"
    file_not_readable: "File is not readable: {0}"
    download_failed: "Failed to download document: {0}"
    batch_download: "Batch downloading documents: {0} items"
    filename_list: "Filename list: {0}"
    find_file: "Finding file: {0} -> {1}"
    added_to_zip: "Added to ZIP: {0}"
    file_not_exists_skip: "File does not exist, skipping: {0} (Path: {1})"
    add_to_zip_failed: "Failed to add file to ZIP: {0}"
    temp_zip_deleted: "Temporary ZIP file deleted: {0}"
    delete_temp_zip_failed: "Failed to delete temporary ZIP file: {0}"
    batch_download_failed: "Batch download failed"
    get_file_types: "Retrieving list of uploaded document file types"
    get_file_types_failed: "Failed to retrieve file type list"
    get_file_types_success: "Successfully retrieved file type list, total {0} types"
    get_file_types_error: "Failed to retrieve file type list: {0}"
# ============================================================
# DocumentQAController and DocumentQAService
# ============================================================
doc_qa:
  log:
    receive_request: "Received document QA request: Document={0}, Parse path={1}, Question={2}"
    qa_failed: "Document QA failed"
    ppt_request: "Received PPT progressive analysis request: Document={0}, Parse path={1}, Question={2}"
    ppt_failed: "PPT analysis failed"
    cleanup_failed: "Failed to clean up session"
    create_temp_dir: "Creating document QA temporary directory: {0}"
    create_temp_dir_failed: "Failed to create temporary directory"
    start_qa: "üìÑ Starting document QA: {0} (Session ID: {1})"
    question: "‚ùì Question: {0}"
    batch_mode: "üì¶ Document is large, enabling batch processing mode"
    direct_mode: "üìù Document is small, processing directly"
    qa_complete: "‚úÖ Document QA completed: {0} (Elapsed time: {1}ms)"
    qa_error: "‚ùå Document QA failed"
    direct_process_failed: "Failed to process document directly"
    split_batches: "üì¶ Document split into {0} batches"
    process_batch: "üîÑ Processing batch {0}/{1} (Size: {2} characters)"
    batch_key_points: "üí° Batch {0} key information extracted ({1} characters)"
    batch_complete: "‚úÖ Batch {0}/{1} processing completed"
    batch_process_failed: "Failed to process document in batches"
    generate_summary: "üìä Starting final summary generation..."
    summary_complete: "‚úÖ Final summary generation completed ({0} characters)"
    generate_summary_failed: "Failed to generate final summary"
    batch_result_saved: "üíæ Batch result saved: {0}"
    save_batch_failed: "Failed to save batch result"
    report_saved: "üìä Final report saved: {0}"
    save_report_failed: "Failed to save final report"
    temp_file_deleted: "üóëÔ∏è Temporary file deleted: {0}"
    delete_temp_failed: "Failed to delete temporary file: {0}"
    file_process_failed: "File processing failed"
    doc_analysis_request: "üìÑ Received document analysis request: {0}, Question: {1}"
    # Direct analysis (without knowledge base) related
    direct_start: "üìÑ Starting direct document analysis (without knowledge base): Document={0}, Session ID={1}"
    direct_question: "‚ùì Analysis question: {0}"
    direct_content_length: "üìè Document content length: {0} characters"
    direct_exceed_limit: "üì¶ Document content exceeds limit ({0}), using memo mechanism for batch processing"
    direct_full_analysis: "üìù Performing full document analysis directly"
    direct_split_batches: "üì¶ Document split into {0} batches for analysis"
    direct_process_batch: "üîÑ Processing batch {0}/{1}, content length: {2} characters"
    direct_complete: "‚úÖ Direct document analysis completed: Document={0}, Elapsed time={1}ms"
    direct_failed: "‚ùå Direct document analysis failed"
    # Document parsing related
    read_text_file: "üìù Reading text file directly: {0}"
    parse_with_utils: "üîç Parsing document with DocumentUtils: {0}"
    parse_empty_fallback: "‚ö†Ô∏è Document parsing result is empty, trying direct read: {0}"
    parse_success: "‚úÖ Document parsed successfully: {0}, content length: {1} characters"
    parse_failed_fallback: "‚ö†Ô∏è DocumentUtils parsing failed, trying direct read: {0}, error: {1}"
    analysis_failed: "Analysis failed"
    final_report_failed: "Failed to generate final report"
# ============================================================
# DocumentProgressiveAnalysisController
# ============================================================
doc_progressive_ctrl:
  log:
    receive_request: "üìÑ Received document analysis request: {0}, Question: {1}"
    file_process_failed: "File processing failed"
# ============================================================
# PPTProgressiveAnalysisService
# ============================================================
ppt_analysis:
  log:
    start_analysis: "üìä Starting progressive PPT analysis: {0} ({1} slides)"
    analyze_slide: "üîç Analyzing slide {0}/{1}"
    slide_complete: "‚úÖ Slide {0} analysis completed, key points: {1}"
    analysis_complete: "üéâ PPT progressive analysis completed, elapsed time: {0}ms"
    analysis_failed: "PPT analysis failed"
    slide_failed: "Slide {0} analysis failed"
    generate_summary: "üìä Generating comprehensive PPT summary..."
    direct_llm_summary: "üìù Directly calling LLM to generate final summary"
    summary_complete: "‚úÖ Comprehensive summary generation completed"
    summary_failed: "Failed to generate comprehensive summary"
# ============================================================
# SearchConfigService
# ============================================================
search_config:
  log:
    lucene_updated: "‚úÖ Dynamic configuration updated: luceneTopK = {0}"
    vector_updated: "‚úÖ Dynamic configuration updated: vectorTopK = {0}"
    hybrid_updated: "‚úÖ Dynamic configuration updated: hybridTopK = {0}"
    docs_per_query_updated: "‚úÖ Dynamic configuration updated: documentsPerQuery = {0}"
    min_score_updated: "‚úÖ Dynamic configuration updated: minScoreThreshold = {0}"
    batch_update_complete: "‚úÖ Batch configuration update completed"
    reset_to_default: "‚úÖ Configuration reset to default values (yml configuration)"
# ============================================================
# SlideContentCacheService
# ============================================================
slide_cache:
  log:
    init_success: "‚úÖ Slide cache service initialized"
    create_cache_dir: "Creating slide cache directory: {0}"
    init_failed: "Failed to initialize slide cache"
    save_failed: "Failed to save slide cache"
    hash_failed: "Failed to calculate slide hash"
    clear_failed: "Failed to clear cache"
    delete_ppt_cache_failed: "Failed to delete PPT cache"
    load_cache_index: "Loading slide cache index: {0} PPT files"
    load_cache_index_failed: "Failed to load cache index, new index will be created"
    save_cache_index: "Saving slide cache index: {0} PPT files"
# SimilarQAService
# ============================================================
similar_qa:
  log:
    no_valid_keywords: "No valid keywords in query question: {0}"
    query_keywords: "Query keywords: {0}"
# ============================================================
# DocumentPreprocessingService
# ============================================================
doc_preprocess:
  log:
    ppl_disabled: "üì¶ PPL service not enabled"
    ppl_enabled: "‚úÖ PPL service enabled"
# ============================================================
# KnowledgeQAService Supplement
# ============================================================
knowledge_qa_extra:
  log:
    vector_init_failed_warning: "‚ö†Ô∏è Vector search initialization failed, system will only use text search functionality"
    model_incomplete_hint: "üí° Hint: embedding model file is incomplete or corrupted"
    solutions: "üìù Solutions:"
    solution_disable_vector: "   1. Set knowledge.qa.vector-search.enabled: false in application.yml"
    solution_download_model: "   2. Or download complete ONNX model files (including .onnx and .onnx_data files)"
# ============================================================
# KnowledgeBaseService
# ============================================================
kb_service_extra:
  log:
    slide_cache_init: "‚úÖ Slide cache service initialized"
# ============================================================
# LLMResultDocumentController
# ============================================================
llm_result_ctrl:
  log:
    save_failed: "Failed to save result"
    batch_save_failed: "Batch save failed"
# ============================================================
# DocumentMemoManagerImpl
# ============================================================
memo_manager:
  log:
    start_document: "üìÑ Starting document analysis: {0} (Total {1} segments)"
    add_segment: "üìù Adding segment analysis: {0} - {1}"
    mark_independent: "‚≠ê Marked as independent important entry: {0}"
    user_mark_important: "‚≠ê User marked segment {0} as important"
    recall_memos: "üìã Recalled {0} related memos, total {1} tokens"
    skip_compress: "‚è≠Ô∏è Skipping compression: {0} (Already sufficiently concise)"
    llm_compress_complete: "‚úÖ LLM compression completed: {0} -> {1} tokens"
    llm_compress_failed: "‚ö†Ô∏è LLM compression failed, using truncation: {0}"
    memo_cleared: "üßπ Memos cleared"
    export_json_failed: "Failed to export JSON"
    move_to_long_term: "üì¶ Moved to long-term memos: {0} ({1} tokens)"
    remove_old_entry: "üóëÔ∏è Removed old entry: {0}"
    new_document: "üìÑ Starting new document: {0}"
    segment_added: "üìù Added segment {0}: {1}"
    memo_entry_created: "üìå Created memo entry: {0}"
    context_built: "üìã Context construction completed: {0} short-term memories, {1} memos"
# ============================================================
# DocumentProgressiveAnalysisService
# ============================================================
doc_progressive:
  log:
    start_analysis: "üìö Starting progressive document analysis: {0} (Parser: {1})"
    total_segments: "üìÑ Total {0} segments in document"
    analyze_segment: "üîç Analyzing segment {0}/{1}: {2}"
    stage_output: "üìä Generating stage output: {0}"
    segment_complete: "‚úÖ Segment {0} analysis completed"
    analysis_complete: "üéâ Document analysis completed, elapsed time: {0}ms"
    analysis_failed: "Document analysis failed: {0}"
    segment_failed: "Segment {0} analysis failed"
    generate_summary: "üìä Generating comprehensive document summary..."
    summary_complete: "‚úÖ Comprehensive summary generation completed"
    summary_failed: "Failed to generate comprehensive summary"
    unsupported_type: "Unsupported document type: {0}"
    analyzing_segment: "Analyzing segment {0}/{1}: {2}"
    segment_analysis_complete: "Segment analysis completed"
    generating_summary: "Generating final summary..."
  error:
    unsupported_type: "Unsupported document type: {0}"
  prompt:
    segment_analysis: |
      You are analyzing a document segment by segment.

      ## Current Progress
      Progress: {0}/{1} ({2}%)

      ## Historical Key Points
      {3}

      ## Current Segment
      Title: {4}
      Content: {5}

      ## User Question
      {6}

      Please analyze the core content of the current segment and extract key points.
    final_summary: |
      Please generate a complete summary report based on the following analysis points.

      ## User Question
      {0}

      ## Segment Analysis Points
      {1}

      Please generate a well-structured and comprehensive summary report.
# ============================================================
# KeywordInvertedIndex
# ============================================================
keyword_index:
  log:
    index_entry: "Index entry: {0} ({1} keywords)"
    index_cleared: "Keyword index cleared"
    index_built: "üîç Keyword index construction completed: {0} entries"
    search_complete: "Search completed: {0} matches found"
# ============================================================
# MemoAggregator
# ============================================================
memo_aggregator:
  log:
    aggregate_by_topic: "Aggregating by topic: {0} entries -> {1} topic groups"
    llm_aggregate_failed: "LLM aggregation failed, using simple aggregation: {0}"
    aggregation_start: "üîÑ Starting memo aggregation: {0} entries"
    aggregation_complete: "‚úÖ Aggregation completed: {0} -> {1} entries"
    merge_similar: "Merging similar entries: {0} entries -> 1 entry"
    compress_large: "Compressing large entry: {0} characters -> {1} characters"
# ============================================================
# MemoDocumentExporter
# ============================================================
memo_exporter:
  log:
    export_json_failed: "Failed to export JSON"
    exported_to: "Memos exported to: {0}"
    export_markdown: "üìÑ Exporting Markdown: {0}"
    export_json: "üìã Exporting JSON: {0}"
    export_html: "üåê Exporting HTML: {0}"
    export_failed: "Export failed: {0}"
  title:
    document_memo: "Document Analysis Memos"
    key_points: "Key Points"
    memo_entries: "Memo Entries"
    generated_by: "Generated by AI Intelligent Analysis System"
# ============================================================
# RelevanceRecallService
# ============================================================
relevance_recall:
  log:
    recall_complete: "üìå Recalled {0} relevant entries, total tokens: {1}"
# ============================================================
# StageOutputManagerImpl
# ============================================================
stage_output:
  log:
    generate_output: "üìä Generating stage output ({0}%)"
    output_complete: "‚úÖ Stage output generation completed: {0}"
    manager_cleared: "üßπ Stage output manager cleared"
    generate_failed: "Failed to generate stage summary, using simple summary: {0}"
    stage_checkpoint: "üìç Stage checkpoint: {0}%"
    stage_summary_saved: "üíæ Stage summary saved: Stage {0}"
    final_summary_saved: "üìã Final summary saved"
  title:
    stage_summary: "Stage Summary ({0}%)"
    final_summary: "Final Summary"
# ============================================================
# DocumentParserFactory
# ============================================================
parser_factory:
  log:
    registered_parsers: "üìö {0} document parsers registered"
    select_parser: "Selecting parser: {0} for {1}"
    no_parser_found: "No supported parser found: {0}"
    register_parser: "Registering parser: {0} - {1}"
# ============================================================
# PDFDocumentParser
# ============================================================
pdf_parser:
  log:
    parse_file: "üìÑ Parsing PDF file: {0} ({1} pages)"
    parse_page: "  Parsing page {0}: {1} ({2} characters)"
    parse_failed: "Failed to parse PDF file: {0}"
  error:
    file_not_found: "File not found: {0}"
    content_truncated: "...[Content truncated due to length]"
  title:
    default_page: "Page {0}"
# ============================================================
# PPTDocumentParser
# ============================================================
ppt_parser:
  log:
    parse_file: "üìä Parsing PPT file: {0} ({1} slides)"
    parse_slide: "  Parsing slide {0}: {1}"
    parse_failed: "Failed to parse PPT file: {0}"
  error:
    file_not_found: "File not found: {0}"
  title:
    default_slide: "Slide {0}"
  content:
    image_info: "[Image: {0}, Type: {1}]"
    table_header: "[Table]"
# ============================================================
# WordDocumentParser
# ============================================================
word_parser:
  log:
    parse_file: "üìù Parsing Word file: {0}"
    parse_chapter: "  Parsing chapter {0}: {1} ({2} characters)"
    parse_complete: "‚úÖ Word file parsing completed: {0} segments"
    parse_failed: "Failed to parse Word file: {0}"
  error:
    file_not_found: "File not found: {0}"
  title:
    continued: "Continued..."
    default_content: "Document Content"
# ============================================================
# PPL ONNX Service
# ============================================================
ppl_onnx:
  log:
    init_start: "üöÄ Initializing ONNX PPL Service..."
    model_path: "üì¶ Model path: {0}"
    tokenizer_path: "üì¶ Tokenizer path: {0}"
    env_created: "‚úÖ ONNX Runtime environment created"
    model_loaded: "‚úÖ ONNX model loaded from: {0}"
    tokenizer_loaded: "‚úÖ Tokenizer loaded from: {0}"
    cache_init: "‚úÖ PPL cache initialized (size: {0}, TTL: {1}s)"
    init_success: "‚úÖ ONNX PPL Service initialized successfully"
    init_failed: "‚ùå Failed to initialize ONNX PPL Service"
    shutdown_start: "üõë Shutting down ONNX PPL Service..."
    session_closed: "‚úÖ ONNX session closed"
    tokenizer_closed: "‚úÖ Tokenizer closed"
    cache_cleared: "‚úÖ PPL cache cleared"
    shutdown_success: "‚úÖ ONNX PPL Service shut down successfully"
    shutdown_error: "Error during shutdown"
    calc_ppl_failed: "Failed to calculate perplexity for text: {0}"
    health_check_failed: "Health check failed"
    rerank_detail: "üìä PPL Rerank - Doc: {0} | Original: {1} | PPL: {2} | PPL Score: {3} | Final: {4}"
  error:
    init_failed: "ONNX initialization failed"
    calc_ppl_failed: "Failed to calculate perplexity"
    chunk_failed: "Failed to chunk document"
    rerank_failed: "Failed to rerank documents"
# ============================================================
# Strategy Module
# ============================================================
strategy:
  dispatcher:
    log:
      init_start: "üì¶ Initializing Strategy Dispatcher..."
      strategy_registered: "  ‚úÖ Registered strategy: {0} - {1}"
      init_complete: "‚úÖ Strategy Dispatcher initialized with {0} strategies"
      no_suitable_strategy: "No suitable strategy found"
      selected_strategies: "üéØ Selected strategies: {0}"
      executing_single: "Executing single strategy: {0}"
      executing_combined: "Executing combined strategies: {0}"
      merging_results: "Merging analysis results..."
      evaluate_failed: "Failed to evaluate strategy {0}: {1}"
  abstract:
    log:
      analysis_start: "üöÄ Starting analysis, strategy: {0}, documents: {1}"
      analysis_complete: "‚úÖ Analysis completed, strategy: {0}, time: {1}ms"
      analysis_failed: "‚ùå Analysis failed, strategy: {0}"
      llm_call_failed: "LLM call failed"
  parallel_summary:
    log:
      generating_summaries: "Generating document summaries in parallel..."
      summary_progress: "Completed {0}/{1} document summaries"
      generating_report: "Generating comprehensive analysis report..."
      summary_failed: "Failed to generate document summary: {0}"
      parallel_failed: "Parallel summary generation failed"
    error:
      no_documents: "No document content to analyze"
      summary_failed: "Summary generation failed"
      document_empty: "Document content is empty"
  structured_compare:
    log:
      preparing_content: "Preparing document contents..."
      executing_comparison: "Executing comparison analysis..."
      extracting_results: "Extracting comparison results..."
    error:
      min_documents: "Comparison analysis requires at least 2 documents"
  entity_relation:
    log:
      extracting_entities: "Extracting document entities..."
      analyzing_relations: "Analyzing entity relationships..."
      building_graph: "Building relationship graph..."
    error:
      min_documents: "Relationship analysis requires at least 2 documents"
  question_driven:
    log:
      retrieving_content: "Retrieving relevant content..."
      generating_answer: "Generating answer..."
      organizing_results: "Organizing results..."
    error:
      no_documents: "No documents to analyze"
# ============================================================
# Smart Analysis Service
# ============================================================
smart_analysis:
  log:
    loaded_from_slide_cache: "üì¶ Loaded PPT content from slide cache: {0} ({1} chars)"
    no_slide_cache: "No slide cache found for: {0}"
    load_slide_cache_failed: "Failed to load from slide cache: {0}"
    start: "üìä Starting smart analysis, documents: {0}, goal: {1}"
    complete: "‚úÖ Smart analysis completed, success: {0}, time: {1}ms"
    failed: "‚ùå Smart analysis failed"
    loading_documents: "Loading document contents..."
    document_loaded: "Loaded document: {0} ({1} characters)"
    document_not_found: "Document not found: {0}"
    document_load_failed: "Failed to load document: {0}"
    document_truncated: "Document {0} content truncated: {1} -> {2} characters"
    no_documents_loaded: "Unable to load any document content"
  error:
    file_not_found: "File not found: {0}"
    binary_file: "Binary file, cannot parse text content"
    parse_failed: "Cannot parse document: {0}"
# ============================================================
# Document Parser Service
# ============================================================
doc_parser_service:
  log:
    parse_failed: "Document parsing failed {0}: {1}"
  error:
    file_not_found: "File not found: {0}"
    binary_file: "Binary file, cannot parse text content"
    parse_failed: "Cannot parse document: {0}"

# ============================================================
# Chunking Strategy
# ============================================================
chunking_strategy:
  simple:
    initialized: "üìù Using simple chunking strategy (fallback)"
    completed: "üìù Simple chunking completed: {0} chunks"
  ppl:
    initialized: "‚úÖ PPL Chunking Strategy enabled"
    unavailable: "PPL Chunking Strategy unavailable"
  llm:
    initialized: "‚úÖ LLM Chunking Strategy enabled"
    using_custom_template: "   Using custom prompt template"
    using_default_template: "   Using default prompt template"
    unavailable: "LLM Chunking Strategy unavailable"
    start: "ü§ñ Starting LLM intelligent chunking, document length: {0} characters"
    doc_small: "üìù Document is small, no chunking needed"
    doc_large: "üìö Large document, using segmented strategy"
    doc_medium: "üìñ Medium document, direct LLM chunking"
    completed: "‚úÖ LLM chunking completed: {0} chunks, time: {1}ms"
    failed: "‚ùå LLM chunking failed"
    calling_llm: "ü§ñ Calling LLM for chunking analysis..."
    no_valid_chunks: "‚ö†Ô∏è LLM returned no valid chunks, using original text"
    fallback_warning: "‚ö†Ô∏è LLM chunking failed, falling back to simple chunking: {0}"
    coarse_split: "üìë Large document coarsely split into {0} segments"
    processing_segment: "   Processing segment {0}/{1}, length: {2}"
    using_fallback: "üìù Using fallback chunking strategy"
  factory:
    initialized: "üì¶ Chunking strategy factory initialized"
    ppl_available: "   - PPL Service: Available"
    ppl_unavailable: "   - PPL Service: Unavailable"
    llm_available: "   - LLM Client: Available"
    llm_unavailable: "   - LLM Client: Unavailable"
    unknown_strategy: "‚ö†Ô∏è Unknown strategy type: {0}, using default strategy"
    auto_select_llm: "ü§ñ Auto-selected: LLM chunking strategy"
    auto_select_ppl: "üìä Auto-selected: PPL chunking strategy"
    fallback_simple: "‚ö†Ô∏è Both PPL and LLM unavailable, using simple chunking strategy"

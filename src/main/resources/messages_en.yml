# Flattened messages converted from messages_en.yml (English)
# Generated to support I18n.get(key) lookups
api:
  started: "API server started on port {0}"
banner:
  supports: "Supports: {0}"
  title: "AI Reviewer - Knowledge Base Intelligent Q&A System"
  version: "Version: {0}"
batch:
  failed: "Batch processing failed"
context:
  initialized: "SmartContextBuilder initialized: maxContext={0} chars, maxDoc={1} chars, preserveFullContent={2}"
document_management:
  api:
    error:
      delete_failed: "Deletion failed: {0}"
      file_empty: "File is empty"
      index_failed: "Indexing failed: {0}"
      list_failed: "Failed to retrieve list: {0}"
      not_found: "Document not found"
      upload_failed: "Upload failed: {0}"
    success:
      batch_result: "Success: {0}, Failed: {1}"
      batch_upload: "Batch upload completed"
      delete: "Document deleted successfully"
      index: "Document indexed successfully"
      list_loaded: "Document list retrieved successfully: Returned {0} documents, Total {1}"
      upload: "Document uploaded successfully"
  log:
    scanned_types: "Scanned file types: {0}"
    batch_upload: "Batch upload documents: {}"
    upload_success: "Document uploaded successfully: {}"
    upload_failed: "Document upload failed"
    file_upload_failed: "File upload failed: {}"
    list_documents: "Get document list: page={}, pageSize={}, sortBy={}, sortOrder={}"
    list_failed: "Failed to get document list"
document_service:
  error:
    cannot_create_dir: "Failed to create document directory: {0}"
    filename_empty: "Filename is empty"
    file_too_large: "File too large: {0} MB (Max: {1} MB)"
    illegal_path: "Illegal file path"
    unsupported_format: "Unsupported file format: {0}"
  log:
    scanned_types: "Scanned file types: {0}"
error:
  auth:
    invalid_credentials: "Invalid credentials"
    permission_denied: "Permission denied: {0}"
    user_exists: "User already exists"
  chunk:
    expected_json_array: "Expected JSON array, but got: {0}"
    no_valid_chunks: "No valid chunks found in AI response"
    no_valid_json: "No valid JSON found in response"
  docs:
    create_failed: "Failed to create document: {0}"
  file:
    directory_not_exists: "Directory does not exist: {0}"
    empty: "File content is empty: {0}"
    index_failed: "File indexing failed: {0}"
    not_a_directory: "Not a directory: {0}"
    not_a_file: "Not a file: {0}"
    not_exists: "File does not exist: {0}"
feedback:
  api:
    emoji:
      "1": "üòû Completely useless"
      "2": "üôÅ Not helpful"
      "3": "üòê Average"
      "4": "üòä Very helpful"
      "5": "ü§© Extremely useful"
    error:
      invalid_feedback_type: "feedbackType must be LIKE or DISLIKE"
      invalid_rating: "Rating must be between 1-5"
      missing_params: "{0} cannot be empty"
      processing_failed: "Processing failed: {0}"
      record_not_found: "Record not found"
    impact:
      document:
        "1": "This document is not helpful, the system will significantly reduce its recommendation weight ‚ö†Ô∏è"
        "2": "This document is not very helpful, the system will reduce its recommendation weight üìâ"
        "3": "This document is average, the system will keep its current weight ‚û°Ô∏è"
        "4": "This document is very helpful, the system will increase its recommendation weight üìà"
        "5": "This document is extremely useful! The system will prioritize recommending it üöÄ"
      overall:
        "1": "Sorry we couldn't help you! We will focus on improvement üôè"
        "2": "Thank you for your feedback! We will analyze and improve üîß"
        "3": "Thank you for your rating! We will continue to optimize üí°"
        "4": "Thank you for your feedback! This helps us improve answer quality üëç"
        "5": "Great! This answer will be recommended to other users üåü"
    success:
      feedback_received: "Thank you for your feedback!"
      thank_you: "Thank you for your rating!"
image:
  log:
    get_failed: "Failed to retrieve image: {0}/{1}"
    list_failed: "Failed to retrieve document image list: {0}"
  saved: "Image saved: {0} (Document: {1})"
  service:
    saved: "Image saved: {0}"
knowledge_qa_service:
  doc_item: "{0}. {1}"
  found_chunks_images: "üì¶ Found {0} chunks and {1} images"
  image_guide_1: "1. Below are image resources related to your question from the knowledge base. You can reference these images in your answer."
  image_guide_2: "2. If your answer involves content from these images (e.g., architecture diagrams, flowcharts, data charts), please reference the images using Markdown format."
  image_guide_3: "3. The reference format is provided below, please copy and use it directly."
  image_guide_4: "4. Please ensure the referenced image URL is complete and correct."
  images_in_context: "üñºÔ∏è Context contains {0} images of information"
  # Added keys requested
  close_existing_kb: "Closing existing knowledge base..."
  destroy_start: "Destroying knowledge Q&A service..."
  kb_closed: "Knowledge base closed"
  kb_closed_safe: "Knowledge base closed safely"
  rebuild_start: "Starting knowledge base rebuild..."
  system_closed: "Knowledge Q&A system closed"
  log:
    build_complete: "   ‚úÖ Knowledge base build completed"
    failed_files: "      - Failed files: {0}"
    kb_ready: "   ‚úÖ Knowledge base is ready"
    processed_files: "      - Processed files: {0}"
    total_documents: "      - Total documents: {0}"
    total_files: "      - Total files: {0}"
    chunk_overlap_chars: "Chunk overlap: {0} chars"
    chunk_size_chars: "Chunk size: {0} chars"
    chunking_strategy: "Chunking strategy: {0}"
    create_qa_system: "Creating QA system..."
    document_count: "Documents: {0}"
    index_count: "Index entries: {0}"
    init_llm: "Initializing LLM client..."
    init_vector_engine: "Initializing vector engine: {0}"
    llm_client_ready: "LLM client ready"
    llm_client_type: "LLM client type: {0}"
    llm_provider: "LLM provider: {0}"
    max_context_chars: "Max context chars: {0}"
    max_doc_length_chars: "Max document length chars: {0}"
    smart_context_initialized: "Smart context initialized: maxContext={0}, maxDoc={1}"
    using_keyword_mode: "Using keyword mode for context building"
    record_saved: "QA record saved: {0}"
    using_vector_enhancement: "Using vector enhancement for retrieval"
    vector_count: "Vector index contains {0} vectors"
    vector_dimension: "Vector dimension: {0}"
    vector_engine_loaded: "Vector engine loaded: {0}"
    vector_index_loaded: "Vector index loaded: {0}"
    vector_index_path: "Vector index path: {0}"
    vector_model: "Vector model: {0}"
  model_doc_hint: "Hint: Place model files in {0}"
  model_download_hint: "Hint: Download the model from {0}"
  more_images: "  ... {0} more images"
  question_prompt: "‚ùì Question: {0}"
  question_separator: "\n================================================================================\n"
  referenced_docs: "\n\n**Referenced Documents**:"
  related_image: "Related Images"
  remaining_docs: "‚ÑπÔ∏è {0} more related documents not included in this answer"
  remaining_docs_unprocessed: "‚ÑπÔ∏è {0} more related documents not included in this answer"
  response_time: "\n‚è±Ô∏è  Response time: {0}ms"
  separator: "================================================================================="
  sources_label: "\nüìö Data Sources (Total {0} documents):"
  too_many_docs_retrieved: "‚ö†Ô∏è Retrieved {0} documents, processing first {1} (Config: documents-per-query)"
  using_docs: "üìö Used {0} documents to generate this answer"
  using_hybrid_search: "‚úÖ Using hybrid search (Lucene + Vector)"
  answer_label: "\nüí° Answer:"
  available_images: "Available images: {0}"
  context_stats: "Context: {0} chars from {1} documents"
  create_session: "Creating QA session: {0}"
  image_item: "Image {0}: {1} (Source: {2})"
  important_notice: "Important: {0}"
  using_keyword_search: "Using keyword search mode"
  classpath_prefix: "Classpath: {0}"
  closing_existing_kb: "Closing existing knowledge base before proceeding..."
  debug_enhanced_stats: "Debug: Enhanced statistics enabled"
  existing_kb_closed: "Existing knowledge base closed"
  failed_files: "      - Failed files: {0}"
  incremental_index_start: "Starting incremental indexing..."
  more_docs_notice: "\n‚ÑπÔ∏è {0} more related documents not included in this answer"
  question_label: "‚ùì Question:"
  success_files: "      - Successful files: {0}"
  total_documents: "      - Total documents: {0}"
  using_session_docs: "Using session-stored documents: {0}"

# Knowledge QA API messages and logs
knowledge_qa:
  api:
    message:
      all_indexed: "All documents indexed"
      incremental_complete: "Incremental indexing completed"
      system_running: "Knowledge QA system is running"
      needs_indexing: "Knowledge base needs indexing"
      rebuild_complete: "Knowledge base rebuild completed"
    status:
      up: "UP"
  log:
    received_question: "Received question: {0}"
    get_statistics: "Retrieving knowledge QA statistics"
    incremental_request: "Incremental indexing requested: {0}"
    statistics_result: "Statistics result: {0}"
    search_documents: "Search documents for question: {0} (found {1})"
    session_question: "Session question [{0}]: {1}"
    rebuild_request: "Rebuild requested: {0}"

kb_service:
  image:
    description: "Image description: {0}"
    filename: "Filename: {0}"
    image_number: "Image {0}/{1}"
    original_file: "Original file: {0}"
    section_end: "End of section"
    section_title: "Section: {0}"
    url: "URL: {0}"
llm:
  error:
    openai_failed: "OpenAI API call failed: {0}"
    openai_http_error: "OpenAI API error: HTTP {0}, {1}"
    parse_failed: "Failed to parse OpenAI response: {0}"
  log:
    mock_init: "‚úÖ Mock LLM client initialized (for testing only)"
    mock_request: "Mock LLM received request, prompt length: {0}"
    mock_response: "üìù Mock LLM returned simulated answer"
    openai_error: "OpenAI API error: HTTP {0}, Body: {1}"
    openai_failed: "OpenAI API call failed"
    openai_init: "‚úÖ OpenAI LLM client initialized"
    openai_request: "Sending request to OpenAI: {0}"
    openai_response: "OpenAI response content: {0}"
  mock:
    default_answer: "This is a simulated answer.\n\nBased on the provided context, I understand your question. However, as a Mock LLM, I can only provide simulated responses.\n\nPlease configure a real LLM service (e.g., OpenAI) to get accurate answers.\n\n(Note: This is a simulated answer from Mock LLM)"
    marriage_answer: "According to the document content, marriage statistics include the distribution of unmarried, married, divorced, widowed and other statuses.\n\n(Note: This is a simulated answer from Mock LLM, please refer to the document content for actual data)"
    population_answer: "According to the document content, the total population of China is approximately 1.4 billion.\n\n(Note: This is a simulated answer from Mock LLM, please refer to the document content for actual data)"
log:
  admin:
    cache_cleared: "Cache cleared"
    cache_cleared_success: "Cache cleared successfully"
    clear_cache_failed: "Failed to clear cache"
    clear_cache_failed_detail: "Failed to clear cache: {0}"
    index_optimize_failed: "Index optimization failed"
    index_optimize_failed_detail: "Index optimization failed: {0}"
    index_optimized: "Index optimized"
    index_optimized_success: "Index optimized successfully"
    stats_failed: "Failed to retrieve statistics"
    stats_failed_detail: "Failed to retrieve statistics: {0}"
  app:
    started: "‚úÖ Application started successfully, Address: {0}"
    start_failed: "‚ùå Application startup failed"
  api:
    stopped: "API server stopped"
  audit:
    init_failed: "Audit log initialization failed"
    logged: "Audit logged: {0}"
    write_failed: "Failed to write audit log"
  auth:
    logged_in: "User logged in: {0}"
    logged_out: "User logged out: {0}"
    registered: "User registered: {0}"
  session:
    create: "Creating session: id={0}, totalDocuments={1}, perQuery={2}"
    no_next: "No next page for session: {0}"
    next: "Session {0} advanced to offset {1}"
    no_prev: "No previous page for session: {0}"
    prev: "Session {0} moved back to offset {1}"
    invalid_page: "Invalid page number"
    page_out_of_range: "Requested page {0} out of range (totalPages={1})"
    goto_page: "Going to page {0} for session {1}, offset {2}"
    deleted: "Session deleted: {0}"
    cleaned: "Cleaned expired session: {0}"
    cleaned_count: "Cleaned {0} expired sessions"
    not_found: "Session not found: {0}"
  hybrid:
    found_docs: "Hybrid search found {0} documents"
    keyword_search: "Hybrid keyword search executed: {0}"
    completed: "Hybrid search completed: Time {0}ms"
    detail_item: "{0}. {1} (score={2})"
    extract_keywords: "Extracted keywords: {0}"
    filtered: "Filtered {0} items"
    lucene_found: "Lucene found {0} documents"
    lucene_top_header: "Top Lucene results:"
    lucene_top_item: "{0}. {1} (luceneScore={2})"
    top5_header: "Top 5 combined results:"
    top5_item: "{0}. {1} (score={2})"
    topk_header: "Top K results:"
    vector_found: "Vector search found {0} documents"
  similar:
    found: "Found {0} similar documents"
  chunk:
    ai_completed: "AI semantic chunking completed: {0} chars -> {1} chunks, Time taken: {2}ms"
    ai_failed: "AI semantic chunking failed, falling back to smart keyword chunking"
    ai_not_enabled: "AI chunking not enabled in configuration"
    ai_parse_failed: "Failed to parse AI semantic chunking response: {0}"
    ai_start: "Starting AI semantic chunking, content length: {0} chars"
    content_truncate: "Content too large ({0} chars), truncated to {1} chars to prevent OOM"
    coverage_low: "Low coverage, adding sequential chunking"
    delete_failed: "Failed to delete file: {0}"
    deleted_all: "All chunks of the document deleted: {0}"
    keywords_extracted: "Extracted {0} keywords from query: {1}"
    max_chunks_reached: "Maximum chunk limit ({0}) reached, stopping chunking"
    no_keywords_fallback: "No keywords found, falling back to simple chunking"
    read_meta_failed: "Failed to read chunk metadata: {0}"
    save_failed: "Failed to save document chunk: {0} (Index: {1})"
    saved: "{0} document chunks saved to document: {1}"
    simple_summary: "Simple chunking: {0} chars -> {1} chunks (size={2}, overlap={3})"
    smart_summary: "Smart keyword chunking: {0} chars -> {1} chunks, {2} keywords"
    storage:
      created: "Document chunk storage directory created: {0}"
      init_failed: "Document chunk storage initialization failed"
    truncate_warning: "Content too long ({0} chars), truncated to {1} chars for AI chunking"
  chunker:
    ai_disabled: "AI chunking not enabled, falling back to SMART_KEYWORD strategy"
    creating: "Creating document chunker: strategy={0}, chunkSize={1}, overlap={2}"
    llm_null: "LLM client is null, falling back to SMART_KEYWORD strategy"
    unknown_strategy: "Unknown chunking strategy: {0}, using SMART_KEYWORD"
  docs:
    classpath_in_jar: "Classpath path is inside JAR, writing not supported"
    classpath_load_failed: "Failed to load resource from classpath: {0} - {1}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    classpath_realpath: "Using classpath real path: {0}"
    classpath_resource_found: "‚úÖ Resource found from classpath: {0}"
    create:
      failed: "Failed to create document: {0}"
      success: "Document created successfully"
    create_failed: "Failed to create document directory: {0}"
    created: "Document created: {0}"
    delete:
      failed: "Failed to delete document: {0}"
      success: "Document deleted successfully"
    deleted: "Document deleted: {0}"
    directory_ready: "Document directory is ready: {0}"
    file_exists_renamed: "File already exists, renamed to: {0}"
    get:
      failed: "Failed to retrieve document: {0}"
    invalid_directory: "Invalid directory: {0}"
    list:
      exception: "Exception while listing documents: {0}"
      failed: "Failed to list documents: {0}"
    not_found: "Document not found: {0}"
    process_failed: "Failed to process file: {0}"
    saved: "Document saved: {0}"
    scanned_types: "Scanned file types: {0}"
    total: "Total documents: {0}"
    update:
      failed: "Failed to update document: {0}"
      success: "Document updated successfully"
    updated: "Document updated: {0}"
    upload_to_external: "Uploaded documents will be saved to external path: ./data/documents"
    using_default_path: "Using default path: ./data/documents"
    using_filesystem: "Using filesystem path: {0}"
    walk_failed: "Failed to traverse directory: {0}"
  factory:
    create_caffeine: "Creating Caffeine cache engine"
    create_filesystem: "Creating filesystem storage engine"
    create_lucene: "Creating Lucene index engine"
  feedback:
    document_failed: "Failed to process document feedback"
    document_received: "Document feedback received {0}: recordId={1}, document={2}"
    get_pending_failed: "Failed to retrieve pending QA records"
    get_record_failed: "Failed to retrieve QA record"
    get_recent_failed: "Failed to retrieve recent QA records"
    get_statistics_failed: "Failed to retrieve QA statistics"
    load_failed: "Failed to load document weights"
    overall_failed: "Failed to process overall feedback"
    overall_received: "Overall feedback received: recordId={0}, rating={1}"
    overall_rating_submitted: "Overall rating submitted {0} [{1}]: {2} stars"
    rating_failed: "Failed to process rating"
    rating_submitted: "Rating submitted {0} [{1}]: {2} - {3} stars (Impact: {4})"
    rating_updated: "üìä Document weight updated (by rating): {0} -> Weight={1} ({2} stars, Adjustment {3}, üëç{4} üëé{5})"
    save_failed: "Failed to save document weights"
    weight_disabled: "Dynamic weight adjustment disabled"
    weight_reset: "üîÑ Document weight reset: {0} -> {1}"
    weight_updated: "üìä Document weight updated: {0} -> Weight={1} (üëç{2} üëé{3})"
    weights_cleared: "üßπ All document weights cleared"
    weights_loaded: "üìÇ Document weights loaded: {0} documents"
    weights_saved: "üíæ Document weights saved: {0} documents"
    weights_file_not_exists: "üìÇ Document weight file not found, using default weights"
  image:
    saved: "Image saved: {0} (Document: {1})"
    delete_failed: "Failed to delete file: {0}"
    deleted_all: "All images of the document deleted: {0}"
    excel:
      extracted: "Extracted {0} images from Excel"
      legacy:
        extracted: "Extracted {0} images from legacy Excel"
        processing: "Processing legacy Excel images"
      processing: "Processing Excel images"
    pdf:
      extracted: "Extracted {0} images from PDF"
      processing: "Processing PDF images"
    ppt:
      found: "PPT images found: {0}"
      processing: "Processing PPT images"
      extracted: "Extracted {0} images from PPT"
    read_info_failed: "Failed to read image information: {0}"
    service:
      saved: "Image saved: {0}"
      extracted: "Extracted {0} images"
      init: "Image service initialized"
      no_images: "No images found"
      start: "Starting image extraction"
      success: "Image processing completed"
      using_extractor: "Using image extractor: {0}"
    storage:
      created: "Image storage directory created: {0}"
      init_failed: "Image storage initialization failed"
  imageproc:
    activated: "‚úÖ Image processing strategy activated: {0}"
    add_ocr: "   Adding OCR strategy:"
    add_vision: "   Adding Vision LLM strategy:"
    init: "üñºÔ∏è  Initializing image processing functionality..."
    language: "      - Recognition language: {0}"
    ocr_available: "   ‚úÖ OCR strategy available"
    ocr_hint: "   üí° Hint: Add dependency net.sourceforge.tess4j:tess4j:5.9.0"
    ocr_unavailable: "   ‚ö†Ô∏è OCR strategy unavailable: Missing tess4j dependency"
    placeholder: "   Using placeholder strategy (default)"
    strategy: "   Configured strategy: {0}"
    tessdata: "      - Tessdata path: {0}"
    vision_apikey_hint: "   üí° Hint: Set environment variable VISION_LLM_API_KEY or configure knowledge.qa.image-processing.vision-llm.api-key"
    vision_available: "   ‚úÖ Vision LLM strategy available"
    vision_endpoint: "      - Endpoint: {0}"
    vision_model: "      - Model: {0}"
    vision_no_apikey: "   ‚ö†Ô∏è Vision LLM enabled but API Key not configured"
    vision_unavailable: "   ‚ö†Ô∏è Vision LLM strategy unavailable"
  kb:
    vector_enabled: "Vector search enabled."
    progress: "Processing progress:"
    path_not_exists: "Path does not exist: {}"
    scan_classpath: "Scanning classpath resources: {}"
    classpath_not_exists: "Classpath resource does not exist: {}"
    resource_found: "Resource found: {}"
    resource_file_not_exists: "Resource file does not exist: {}"
    resource_path: "Resource path: {}"
    scan_directory: "Scanning files in directory."
    files_found: "Total files found: {}"
    add_file: "Adding file: {}"
    scan_classpath_failed: "Failed to scan classpath resources: {}"
    file_not_exists: "File does not exist: {}"
    index_single_file: "Starting to index single file: {}"
    file_indexed: "File indexed successfully: {}"
    index_file_failed: "Failed to index file: {}"
    file_too_large: "File too large, size: {} MB, maximum allowed size: {} MB"
    content_empty: "Document content is empty."
    image_extraction_failed: "Image extraction failed: {}"
    force_chunk: "Document content too large, forcing chunking, size: {} MB"
    auto_chunk: "Document content large, auto chunking, size: {} KB"
    chunked: "Document chunked successfully, number of chunks: {}"
    processing_failed: "Document processing failed."
    file_process_failed: "Failed to process file: {}"
    batch_task_failed: "Batch task execution failed."
    vector_generation_failed: "Vector generation failed: {}"
    incremental_start: "Starting incremental indexing."
    incremental_complete: "Incremental indexing completed."
    incremental_files: "Number of files for incremental indexing: {}"
    build_complete: "Knowledge base build completed"
    build_failed: "Knowledge base build failed"
    build_memory: "Knowledge base build memory usage"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build succeeded"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total items processed: {0}"
    memory_after: "Memory after build: {0} MB"
    memory_before: "Memory before build: {0} MB"
    old_kb_cleared: "Old knowledge base cleared"
    rebuild_prepare: "Preparing for knowledge base rebuild..."
    tracking_cleared: "Knowledge base tracking cleared"
    tracking_saved: "Knowledge base tracking saved: {0}"
    batch_processing: "üì¶ Batch processing: {0} documents ({1} / {2})"
    batch_commit: "üìù Committing batch index"
    content_extracted: "‚úÖ Content extracted: {0}"
    content_too_large: "‚ö†Ô∏è  Content too large ({0} chars)"
    content_truncated: "‚úÇÔ∏è  Content truncated to {0} chars"
    exists: "üìö Existing knowledge base detected ({0} documents)"
    files_to_index: "üìù Files to index: {0}"
    final_batch: "üì¶ Processing final batch: {0} documents"
    first_create: "üìö Creating knowledge base for the first time"
    found_files: "‚úÖ Found {0} document files"
    gc_before: "üóëÔ∏è  Executing GC to free memory"
    hint_put_docs: "üí° Hint: Place documents in {0} directory"
    images_added: "‚ûï {0} images added to index"
    images_extracted: "üñºÔ∏è  Extracted {0} images"
    incremental_done: "\n‚úÖ Incremental indexing completed!"
    incremental_failed: "‚ùå Incremental indexing failed"
    incremental_stats: "   - Processed files: {0}/{1}, Failed: {2}, Total documents: {3}, Time taken: {4}s"
    indexation_complete: "‚úÖ Indexing completed: {0}"
    indexing_complete: "‚úÖ Knowledge base indexing complete: {0}"
    no_documents: "‚ö†Ô∏è  No supported document files found"
    parallel_mode: "üöÄ Using parallel processing mode ({0} threads)"
    parallel_progress: "üìä Parallel progress: {0}%"
    parallel_memory: "üíæ Memory usage: {0} MB"
    processing_file: "üìÑ Ê≠£Âú®Â§ÑÁêÜÊñá‰ª∂Ôºö{0}ÔºåÂ§ßÂ∞èÔºö{} KB"
    processing_start: "\nüìù Starting document processing..."
    scanning: "üìÇ Scanning documents: {0}"
    serial_mode: "üìù Using serial processing mode"
    source_path: "   - Document path: {0}"
    supported_formats: "      Supported formats: {0}"
    up_to_date: "‚úÖ All files are up to date, no updates needed"
    vector_init_failed: "‚ùå Vector search engine initialization failed"
  kqa:
    response_time: "\n‚è±Ô∏è  Response time: {0}ms"
    build_separator: "----------------------------------------"
    build_success: "Knowledge base build succeeded"
    build_time: "Knowledge base build time: {0} ms"
    build_total: "Total items processed: {0}"
    memory_after: "Memory after build: {0} MB"
    memory_before: "Memory before build: {0} MB"
    tracking_saved: "KB tracking saved: {0}"
    vector_enabled: "Vector indexing enabled: {0}"
    sep: "================================================================================="
    # Added missing kb/log keys
    build_complete: "Knowledge base build completed: {0}"
    answer_header: "\nüí° Answer:"
    build_failed: "Knowledge base build failed: {0}"
    docs_dir_missing: "‚ö†Ô∏è Document directory does not exist: {0}"
    incremental_complete: "Incremental indexing completed"
    incremental_error: "Incremental indexing error"
    incremental_failed: "Incremental indexing failed: {0}"
    incremental_mode: "   üîÑ Starting incremental knowledge base indexing..."
    init_done: "‚úÖ Knowledge base Q&A system initialized successfully!"
    init_failed: "‚ùå Knowledge Q&A service initialization failed"
    init_kb: "Initializing knowledge base"
    init_start: "üöÄ Initializing knowledge Q&A service"
    kb_not_initialized: "Knowledge base not initialized"
    load_chunks_images_failed: "Failed to load chunks/images information"
    more_docs_available: "‚ÑπÔ∏è {0} more related documents not included in this answer"
    rebuild_complete: "Knowledge base rebuilt successfully"
    rebuild_error: "Knowledge base rebuild error"
    rebuild_failed: "Knowledge base rebuild failed: {0}"
    rebuild_mode: "   üöÄ Starting full knowledge base rebuild..."
    recover_failed: "Knowledge base recovery failed"
    recover_kb: "Recovering knowledge base"
    reinit_complete: "Knowledge base reinitialized successfully"
    reinit_kb: "Reinitializing knowledge base"
    scanned_files_count: "üìÇ Filesystem scan completed, found {0} supported documents"
    scan_failed: "‚ùå Filesystem scan failed"
    source_path: "   - Source path: {0}"
    sources_header: "\nüìö Data Sources (Total {0} documents):"
    step: "\nüöÄ Step {0}: {1}"
    storage_path: "   - Storage path: {0}"
    system_not_initialized: "Knowledge Q&A system not initialized"
    used_docs_count: "üìö Used {0} documents to generate this answer"
  llm:
    api_key_deepseek: "      - DeepSeek: export AI_API_KEY=your-deepseek-key"
    api_key_hint: "üí° Hint: Set environment variables:"
    api_key_missing: "‚ö†Ô∏è LLM API Key not configured"
    api_key_openai: "      - OpenAI: export OPENAI_API_KEY=your-openai-key"
    client_created: "ü§ñ Created {0} LLM client"
    fallback_mock: "üí° Falling back to Mock mode"
    model: "   - Model: {0}"
    api_url: "   - API: {0}"
    mock_created: "ü§ñ Created Mock LLM client (for testing only)"
    mock_hint: "üí° To use real LLM, configure:"
    mock_provider: "      knowledge.qa.llm.provider=openai"
    mock_apikey: "      And set corresponding API Key and URL"
    mock_warning: "‚ö†Ô∏è Mock mode will return fixed simulated answers"
  memory:
    usage: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}%)"
    usage_phase: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}% )"
    warning: "Phase {0} - High memory usage: {1}%"
  optimization:
    chunker:
      batch_completed: "Batch chunking completed: {0} documents -> {1} chunks"
      chunked: "Document {0} chunked into {1} parts"
      initialized: "DocumentChunker initialized - chunkSize: {0}, overlap: {1}, smartSplit: {2}, maxContentLength: {3}, maxChunks: {4}"
    context:
      built: "Smart context built: {0} chars from {1} documents ({2}% of max)"
      initialized_with_chunker: "SmartContextBuilder initialized (with chunker): strategy={0}, maxContext={1} chars, maxDoc={2} chars, storage={3}"
      remaining_chars: "\n[... {0} more chars not displayed, content extracted by keyword priority]"
    memory:
      gc_freed: "GC completed, approximately {0}MB memory freed"
      gc_no_freed: "GC completed, no significant memory freed"
      suggest_gc: "Suggest executing garbage collection, current memory usage: {0}MB"
  qa:
    archive:
      init: "Initializing QA archiving service"
    archive_failed: "QA archiving failed"
    archived: "High-rated QA archived: rating={0}, Path={1}"
    document_feedback: "Document feedback {0} [{1}]: {2} - {3}"
    feedback_applied: "Feedback applied to document weight: {0}"
    feedback_pending: "Feedback pending review: {0}"
    find_failed: "Failed to find record: {0}"
    load_failed: "Failed to load QA records: {0}"
    marked_as_quality: "QA record marked as high-quality content: [{0}]"
    overall_rating_submitted: "Overall rating submitted {0} [{1}]: {2} stars"
    pending_failed: "Failed to retrieve pending review records"
    rating_applied: "Star rating applied to document weight: {0} ({1} stars -> Adjustment {2})"
    rating_pending: "Star rating pending review: {0} ({1} stars)"
    rating_submitted: "Star rating submitted {0} [{1}]: {2} - {3} stars (Weight adjustment: {4})"
    recent_failed: "Failed to retrieve recent records"
    record_notfound: "Record not found: {0}"
    record_save_failed: "Failed to save QA record"
    record_saved: "QA record saved: {0} - {1}"
    record_update_failed: "Failed to update QA record: {0}"
    record_updated: "QA record updated: {0}"
    records_dir: "QA records storage directory: {0}"
    records_dir_failed: "Failed to create QA records directory: {0}"
    stats_failed: "Failed to calculate QA statistics"
    user_feedback: "User feedback [{0}]: Rating={1}, Content={2}"
  rag:
    batch_indexed: "Batch indexed {0} documents"
    cache_hit: "Query result from cache: {0}"
    closed: "Local File RAG closed successfully"
    close_error: "Error while closing Local File RAG"
    closing: "Closing Local File RAG..."
    delete_failed: "Failed to delete document: {0}"
    deleted_count: "{0} documents deleted"
    deleting_all: "Deleting all documents..."
    doc_deleted: "Document deleted: {0}"
    doc_indexed: "Document indexed successfully: {0}"
    doc_updated: "Document updated: {0}"
    enable_cache: "Cache enabled: {0}"
    enable_compression: "Compression enabled: {0}"
    found_to_delete: "Found {0} documents to delete"
    init: "Local File RAG initialized successfully"
    init_done: "Local File RAG initialized successfully"
    load_content_failed: "Failed to load document content: {0}"
    loaded_content: "Document content loaded: {0}, Length: {1}"
    no_documents: "No documents to delete"
    optimized: "Index optimization completed"
    optimizing: "Optimizing index..."
    search_completed: "Search completed, Time taken: {0}ms, Found {1} results"
    simple_init: "Initializing simple RAG service..."
    simple_init_done: "Simple RAG service initialized successfully"
    storage: "Storage path: {0}"
  filetracking:
    loaded: "File tracking loaded with {0} entries."
    load_failed: "Failed to load file tracking: {0}"
    saved: "File tracking saved with {0} entries."
    save_failed: "Failed to save file tracking: {0}"
    check_failed: "Failed to check file tracking for {0}"
    mark_failed: "Failed to mark file as indexed: {0}"
    clear_failed: "Failed to clear file tracking: {0}"
    cleared: "File tracking cleared."
  model:
    checking: "üîç Checking model files..."
    dir_and_file: "üìÇ Model directory and files are ready"
    found: "‚úÖ Model found: {0}"
    passed: "‚úÖ Model check passed"
    sep: "===================================================="
  storage:
    ai_image_analyzer_init: "Initializing AI image analyzer: enabled={0}, model={1}"
    chunk_storage_init: "Initializing document chunk storage service, Path: {0}"
    document_image_extraction_init: "Initializing document image extraction service: AI analysis={0}"
    image_storage_init: "Initializing image storage service, Path: {0}"
  optimizer:
    commit: "Optimizer commit: {0}"
    done: "Optimizer done"
    embedding_closed: "Embeddings closed"
    optimize: "Optimizing knowledge base..."
    saving_vectors: "Saving vectors..."
    vectors_saved: "Vectors saved: {0}"
  tika:
    init: "Tika Document Parser initialized."
    max_content: "Max content length: {} MB"
    extract_image_metadata: "Extract image metadata: {}"
    include_image_placeholders: "Include image placeholders: {}"
    active_image_strategy: "Active image strategy: {}"
    ocr_config: "OCR configuration details:"
    enable_ocr: "Enable OCR: {}"
    tessdata: "Tesseract data path: {}"
    not_set: "Not set"
    ocr_language: "OCR language: {}"
    ocr_disabled: "OCR is disabled: {}"
    file_not_exists: "File does not exist: {}"
    detected_mime: "Detected MIME type: {}, filename: {}"
    office_pptx: "Processing PPTX file: {}"
    office_docx: "Processing DOCX file: {}"
    office_xlsx: "Processing XLSX file: {}"
    office_done: "Office file processed: {}, content length: {}"
    parsed_file: "File parsed: {}, content length: {}"
    parse_failed: "File parsing failed: {}"
    image_section_start: "\nImage metadata:\n"
    image_item: "Image {}: {} = {}\n"
    embedded_section: "\nEmbedded resources:\n"
    embedded_item: "Embedded resource count: {}\n"
    image_placeholder: "[Image placeholder {}]"
    empty_bytes: "Byte array is empty."
    parsed_bytes: "Byte array parsed, MIME type: {}, content length: {}"
    parse_bytes_failed: "Byte array parsing failed, MIME type: {}"
    detect_failed: "MIME type detection failed: {}"


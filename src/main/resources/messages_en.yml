# Flattened messages converted from messages_en.yml (English)
# Generated to support I18n.get(key) lookups
api:
  started: "API server started on port {0}"
banner:
  supports: "Supports: {0}"
  title: "AI Reviewer - Knowledge Base Intelligent Q&A System"
  version: "Version: {0}"
batch:
  failed: "Batch processing failed"
context:
  initialized: "SmartContextBuilder initialized: maxContext={0} chars, maxDoc={1} chars, preserveFullContent={2}"
document_management:
  api:
    error:
      delete_failed: "Deletion failed: {0}"
      file_empty: "File is empty"
      index_failed: "Indexing failed: {0}"
      list_failed: "Failed to retrieve list: {0}"
      not_found: "Document not found"
      upload_failed: "Upload failed: {0}"
    success:
      batch_result: "Success: {0}, Failed: {1}"
      batch_upload: "Batch upload completed"
      delete: "Document deleted successfully"
      index: "Document indexed successfully"
      list_loaded: "Document list retrieved successfully: Returned {0} documents, Total {1}"
      upload: "Document uploaded successfully"
document_service:
  error:
    cannot_create_dir: "Failed to create document directory: {0}"
    filename_empty: "Filename is empty"
    file_too_large: "File too large: {0} MB (Max: {1} MB)"
    illegal_path: "Illegal file path"
    unsupported_format: "Unsupported file format: {0}"
  log:
    scanned_types: "Scanned file types: {0}"
error:
  auth:
    invalid_credentials: "Invalid credentials"
    permission_denied: "Permission denied: {0}"
    user_exists: "User already exists"
  chunk:
    expected_json_array: "Expected JSON array, but got: {0}"
    no_valid_chunks: "No valid chunks found in AI response"
    no_valid_json: "No valid JSON found in response"
  docs:
    create_failed: "Failed to create document: {0}"
  file:
    directory_not_exists: "Directory does not exist: {0}"
    empty: "File content is empty: {0}"
    index_failed: "File indexing failed: {0}"
    not_a_directory: "Not a directory: {0}"
    not_a_file: "Not a file: {0}"
    not_exists: "File does not exist: {0}"
feedback:
  api:
    emoji:
      "1": "ğŸ˜ Completely useless"
      "2": "ğŸ™ Not helpful"
      "3": "ğŸ˜ Average"
      "4": "ğŸ˜Š Very helpful"
      "5": "ğŸ¤© Extremely useful"
    error:
      invalid_feedback_type: "feedbackType must be LIKE or DISLIKE"
      invalid_rating: "Rating must be between 1-5"
      missing_params: "{0} cannot be empty"
      processing_failed: "Processing failed: {0}"
      record_not_found: "Record not found"
    impact:
      document:
        "1": "This document is not helpful, the system will significantly reduce its recommendation weight âš ï¸"
        "2": "This document is not very helpful, the system will reduce its recommendation weight ğŸ“‰"
        "3": "This document is average, the system will keep its current weight â¡ï¸"
        "4": "This document is very helpful, the system will increase its recommendation weight ğŸ“ˆ"
        "5": "This document is extremely useful! The system will prioritize recommending it ğŸš€"
      overall:
        "1": "Sorry we couldn't help you! We will focus on improvement ğŸ™"
        "2": "Thank you for your feedback! We will analyze and improve ğŸ”§"
        "3": "Thank you for your rating! We will continue to optimize ğŸ’¡"
        "4": "Thank you for your feedback! This helps us improve answer quality ğŸ‘"
        "5": "Great! This answer will be recommended to other users ğŸŒŸ"
    success:
      feedback_received: "Thank you for your feedback!"
      thank_you: "Thank you for your rating!"
image:
  log:
    get_failed: "Failed to retrieve image: {0}/{1}"
    list_failed: "Failed to retrieve document image list: {0}"
  saved: "Image saved: {0} (Document: {1})"
  service:
    saved: "Image saved: {0}"
  ppt:
    found: "PPT images found: {0}"
    processing: "Processing PPT images"
kb:
  vector_init_failed: "âŒ Vector search engine initialization failed"
kqa:
  more_docs_available: "â„¹ï¸ {0} more related documents not included in this answer"
  response_time: "\nâ±ï¸  Response time: {0}ms"
  sep: "================================================================================="
knowledge_qa_service:
  doc_item: "{0}. {1}"
  found_chunks_images: "ğŸ“¦ Found {0} chunks and {1} images"
  image_guide_1: "1. Below are image resources related to your question from the knowledge base. You can reference these images in your answer."
  image_guide_2: "2. If your answer involves content from these images (e.g., architecture diagrams, flowcharts, data charts), please reference the images using Markdown format."
  image_guide_3: "3. The reference format is provided below, please copy and use it directly."
  image_guide_4: "4. Please ensure the referenced image URL is complete and correct."
  images_in_context: "ğŸ–¼ï¸ Context contains {0} images of information"
  log:
    build_complete: "   âœ… Knowledge base build completed"
    failed_files: "      - Failed files: {0}"
    kb_ready: "   âœ… Knowledge base is ready"
    processed_files: "      - Processed files: {0}"
    total_documents: "      - Total documents: {0}"
    total_files: "      - Total files: {0}"
  more_images: "  ... {0} more images"
  question_prompt: "â“ Question: {0}"
  question_separator: "\n================================================================================\n"
  referenced_docs: "\n\n**Referenced Documents**:"
  related_image: "Related Images"
  remaining_docs: "â„¹ï¸ {0} more related documents not included in this answer"
  remaining_docs_unprocessed: "â„¹ï¸ {0} more related documents not included in this answer"
  response_time: "\nâ±ï¸  Response time: {0}ms"
  separator: "================================================================================="
  sources_label: "\nğŸ“š Data Sources (Total {0} documents):"
  too_many_docs_retrieved: "âš ï¸ Retrieved {0} documents, processing first {1} (Config: documents-per-query)"
  using_docs: "ğŸ“š Used {0} documents to generate this answer"
  using_hybrid_search: "âœ… Using hybrid search (Lucene + Vector)"
llm:
  error:
    openai_failed: "OpenAI API call failed: {0}"
    openai_http_error: "OpenAI API error: HTTP {0}, {1}"
    parse_failed: "Failed to parse OpenAI response: {0}"
  log:
    mock_init: "âœ… Mock LLM client initialized (for testing only)"
    mock_request: "Mock LLM received request, prompt length: {0}"
    mock_response: "ğŸ“ Mock LLM returned simulated answer"
    openai_error: "OpenAI API error: HTTP {0}, Body: {1}"
    openai_failed: "OpenAI API call failed"
    openai_init: "âœ… OpenAI LLM client initialized"
    openai_request: "Sending request to OpenAI: {0}"
    openai_response: "OpenAI response content: {0}"
  mock:
    default_answer: "This is a simulated answer.\n\nBased on the provided context, I understand your question. However, as a Mock LLM, I can only provide simulated responses.\n\nPlease configure a real LLM service (e.g., OpenAI) to get accurate answers.\n\n(Note: This is a simulated answer from Mock LLM)"
    marriage_answer: "According to the document content, marriage statistics include the distribution of unmarried, married, divorced, widowed and other statuses.\n\n(Note: This is a simulated answer from Mock LLM, please refer to the document content for actual data)"
    population_answer: "According to the document content, the total population of China is approximately 1.4 billion.\n\n(Note: This is a simulated answer from Mock LLM, please refer to the document content for actual data)"
log:
  admin:
    cache_cleared: "Cache cleared"
    cache_cleared_success: "Cache cleared successfully"
    clear_cache_failed: "Failed to clear cache"
    clear_cache_failed_detail: "Failed to clear cache: {0}"
    index_optimize_failed: "Index optimization failed"
    index_optimize_failed_detail: "Index optimization failed: {0}"
    index_optimized: "Index optimized"
    index_optimized_success: "Index optimized successfully"
    stats_failed: "Failed to retrieve statistics"
    stats_failed_detail: "Failed to retrieve statistics: {0}"
  app:
    started: "âœ… Application started successfully, Address: {0}"
    start_failed: "âŒ Application startup failed"
  api:
    stopped: "API server stopped"
  audit:
    init_failed: "Audit log initialization failed"
    logged: "Audit logged: {0}"
    write_failed: "Failed to write audit log"
  auth:
    logged_in: "User logged in: {0}"
    logged_out: "User logged out: {0}"
    registered: "User registered: {0}"
  chunk:
    ai_completed: "AI semantic chunking completed: {0} chars -> {1} chunks, Time taken: {2}ms"
    ai_failed: "AI semantic chunking failed, falling back to smart keyword chunking"
    ai_not_enabled: "AI chunking not enabled in configuration"
    ai_parse_failed: "Failed to parse AI semantic chunking response: {0}"
    ai_start: "Starting AI semantic chunking, content length: {0} chars"
    content_truncate: "Content too large ({0} chars), truncated to {1} chars to prevent OOM"
    coverage_low: "Low coverage, adding sequential chunking"
    delete_failed: "Failed to delete file: {0}"
    deleted_all: "All chunks of the document deleted: {0}"
    keywords_extracted: "Extracted {0} keywords from query: {1}"
    max_chunks_reached: "Maximum chunk limit ({0}) reached, stopping chunking"
    no_keywords_fallback: "No keywords found, falling back to simple chunking"
    read_meta_failed: "Failed to read chunk metadata: {0}"
    save_failed: "Failed to save document chunk: {0} (Index: {1})"
    saved: "{0} document chunks saved to document: {1}"
    simple_summary: "Simple chunking: {0} chars -> {1} chunks (size={2}, overlap={3})"
    smart_summary: "Smart keyword chunking: {0} chars -> {1} chunks, {2} keywords"
    storage:
      created: "Document chunk storage directory created: {0}"
      init_failed: "Document chunk storage initialization failed"
    truncate_warning: "Content too long ({0} chars), truncated to {1} chars for AI chunking"
  chunker:
    ai_disabled: "AI chunking not enabled, falling back to SMART_KEYWORD strategy"
    creating: "Creating document chunker: strategy={0}, chunkSize={1}, overlap={2}"
    llm_null: "LLM client is null, falling back to SMART_KEYWORD strategy"
    unknown_strategy: "Unknown chunking strategy: {0}, using SMART_KEYWORD"
  docs:
    classpath_in_jar: "Classpath path is inside JAR, writing not supported"
    classpath_load_failed: "Failed to load resource from classpath: {0} - {1}"
    classpath_not_exists: "Classpath resource does not exist: {0}"
    classpath_realpath: "Using classpath real path: {0}"
    classpath_resource_found: "âœ… Resource found from classpath: {0}"
    create:
      failed: "Failed to create document: {0}"
      success: "Document created successfully"
    create_failed: "Failed to create document directory: {0}"
    created: "Document created: {0}"
    delete:
      failed: "Failed to delete document: {0}"
      success: "Document deleted successfully"
    deleted: "Document deleted: {0}"
    directory_ready: "Document directory is ready: {0}"
    file_exists_renamed: "File already exists, renamed to: {0}"
    get:
      failed: "Failed to retrieve document: {0}"
    invalid_directory: "Invalid directory: {0}"
    list:
      exception: "Exception while listing documents: {0}"
    list.failed: "Failed to list documents: {0}"
    not_found: "Document not found: {0}"
    notfound: "Document not found: {0}"
    process_failed: "Failed to process file: {0}"
    saved: "Document saved: {0}"
    scanned_types: "Scanned file types: {0}"
    total: "Total documents: {0}"
    update:
      failed: "Failed to update document: {0}"
      success: "Document updated successfully"
    updated: "Document updated: {0}"
    upload_to_external: "Uploaded documents will be saved to external path: ./data/documents"
    using_default_path: "Using default path: ./data/documents"
    using_filesystem: "Using filesystem path: {0}"
    walk_failed: "Failed to traverse directory: {0}"
  factory:
    create_caffeine: "Creating Caffeine cache engine"
    create_filesystem: "Creating filesystem storage engine"
    create_lucene: "Creating Lucene index engine"
  feedback:
    document_failed: "Failed to process document feedback"
    document_received: "Document feedback received {0}: recordId={1}, document={2}"
    get_pending_failed: "Failed to retrieve pending QA records"
    get_record_failed: "Failed to retrieve QA record"
    get_recent_failed: "Failed to retrieve recent QA records"
    get_statistics_failed: "Failed to retrieve QA statistics"
    load_failed: "Failed to load document weights"
    overall_failed: "Failed to process overall feedback"
    overall_received: "Overall feedback received: recordId={0}, rating={1}"
    overall_rating_submitted: "Overall rating submitted {0} [{1}]: {2} stars"
    rating_failed: "Failed to process rating"
    rating_submitted: "Rating submitted {0} [{1}]: {2} - {3} stars (Impact: {4})"
    rating_updated: "ğŸ“Š Document weight updated (by rating): {0} -> Weight={1} ({2} stars, Adjustment {3}, ğŸ‘{4} ğŸ‘{5})"
    save_failed: "Failed to save document weights"
    weight_disabled: "Dynamic weight adjustment disabled"
    weight_reset: "ğŸ”„ Document weight reset: {0} -> {1}"
    weight_updated: "ğŸ“Š Document weight updated: {0} -> Weight={1} (ğŸ‘{2} ğŸ‘{3})"
    weights_cleared: "ğŸ§¹ All document weights cleared"
    weights_loaded: "ğŸ“‚ Document weights loaded: {0} documents"
    weights_saved: "ğŸ’¾ Document weights saved: {0} documents"
    weights_file_not_exists: "ğŸ“‚ Document weight file not found, using default weights"
  image:
    delete_failed: "Failed to delete file: {0}"
    deleted_all: "All images of the document deleted: {0}"
    excel:
      extracted: "Extracted {0} images from Excel"
      legacy:
        extracted: "Extracted {0} images from legacy Excel"
        processing: "Processing legacy Excel images"
      processing: "Processing Excel images"
    pdf:
      extracted: "Extracted {0} images from PDF"
      processing: "Processing PDF images"
    ppt:
      extracted: "Extracted {0} images from PPT"
    read_info_failed: "Failed to read image information: {0}"
    service:
      extracted: "Extracted {0} images"
      init: "Image service initialized"
      no_images: "No images found"
      start: "Starting image extraction"
      success: "Image processing completed"
      using_extractor: "Using image extractor: {0}"
    storage:
      created: "Image storage directory created: {0}"
      init_failed: "Image storage initialization failed"
  imageproc:
    activated: "âœ… Image processing strategy activated: {0}"
    add_ocr: "   Adding OCR strategy:"
    add_vision: "   Adding Vision LLM strategy:"
    init: "ğŸ–¼ï¸  Initializing image processing functionality..."
    language: "      - Recognition language: {0}"
    ocr_available: "   âœ… OCR strategy available"
    ocr_hint: "   ğŸ’¡ Hint: Add dependency net.sourceforge.tess4j:tess4j:5.9.0"
    ocr_unavailable: "   âš ï¸ OCR strategy unavailable: Missing tess4j dependency"
    placeholder: "   Using placeholder strategy (default)"
    strategy: "   Configured strategy: {0}"
    tessdata: "      - Tessdata path: {0}"
    vision_apikey_hint: "   ğŸ’¡ Hint: Set environment variable VISION_LLM_API_KEY or configure knowledge.qa.image-processing.vision-llm.api-key"
    vision_available: "   âœ… Vision LLM strategy available"
    vision_endpoint: "      - Endpoint: {0}"
    vision_model: "      - Model: {0}"
    vision_no_apikey: "   âš ï¸ Vision LLM enabled but API Key not configured"
    vision_unavailable: "   âš ï¸ Vision LLM strategy unavailable"
  kb:
    batch_processing: "ğŸ“¦ Batch processing: {0} documents ({1} / {2})"
    batch_commit: "ğŸ“ Committing batch index"
    content_extracted: "âœ… Content extracted: {0}"
    content_too_large: "âš ï¸  Content too large ({0} chars)"
    content_truncated: "âœ‚ï¸  Content truncated to {0} chars"
    exists: "ğŸ“š Existing knowledge base detected ({0} documents)"
    files_to_index: "ğŸ“ Files to index: {0}"
    final_batch: "ğŸ“¦ Processing final batch: {0} documents"
    first_create: "ğŸ“š Creating knowledge base for the first time"
    found_files: "âœ… Found {0} document files"
    gc_before: "ğŸ—‘ï¸  Executing GC to free memory"
    hint_put_docs: "ğŸ’¡ Hint: Place documents in {0} directory"
    images_added: "â• {0} images added to index"
    images_extracted: "ğŸ–¼ï¸  Extracted {0} images"
    incremental_done: "\nâœ… Incremental indexing completed!"
    incremental_failed: "âŒ Incremental indexing failed"
    incremental_stats: "   - Processed files: {0}/{1}, Failed: {2}, Total documents: {3}, Time taken: {4}s"
    indexation_complete: "âœ… Indexing completed: {0}"
    no_documents: "âš ï¸  No supported document files found"
    parallel_mode: "ğŸš€ Using parallel processing mode ({0} threads)"
    parallel_progress: "ğŸ“Š Parallel progress: {0}%"
    parallel_memory: "ğŸ’¾ Memory usage: {0} MB"
    processing_file: "ğŸ“„ Processing file: {0}"
    processing_start: "\nğŸ“ Starting document processing..."
    scanning: "ğŸ“‚ Scanning documents: {0}"
    serial_mode: "ğŸ“ Using serial processing mode"
    source_path: "   - Document path: {0}"
    supported_formats: "      Supported formats: {0}"
    up_to_date: "âœ… All files are up to date, no updates needed"
  llm:
    api_key_deepseek: "      - DeepSeek: export AI_API_KEY=your-deepseek-key"
    api_key_hint: "ğŸ’¡ Hint: Set environment variables:"
    api_key_missing: "âš ï¸ LLM API Key not configured"
    api_key_openai: "      - OpenAI: export OPENAI_API_KEY=your-openai-key"
    client_created: "ğŸ¤– Created {0} LLM client"
    fallback_mock: "ğŸ’¡ Falling back to Mock mode"
    model: "   - Model: {0}"
    api_url: "   - API: {0}"
    mock_created: "ğŸ¤– Created Mock LLM client (for testing only)"
    mock_hint: "ğŸ’¡ To use real LLM, configure:"
    mock_provider: "      knowledge.qa.llm.provider=openai"
    mock_apikey: "      And set corresponding API Key and URL"
    mock_warning: "âš ï¸ Mock mode will return fixed simulated answers"
  memory:
    usage: "Phase {0} - Memory used: {1} MB / Max: {2} MB ({3}%)"
    warning: "Phase {0} - High memory usage: {1}%"
  optimization:
    chunker:
      batch_completed: "Batch chunking completed: {0} documents -> {1} chunks"
      chunked: "Document {0} chunked into {1} parts"
      initialized: "DocumentChunker initialized - chunkSize: {0}, overlap: {1}, smartSplit: {2}, maxContentLength: {3}, maxChunks: {4}"
    context:
      built: "Smart context built: {0} chars from {1} documents ({2}% of max)"
      initialized_with_chunker: "SmartContextBuilder initialized (with chunker): strategy={0}, maxContext={1} chars, maxDoc={2} chars, storage={3}"
      remaining_chars: "\n[... {0} more chars not displayed, content extracted by keyword priority]"
    memory:
      gc_freed: "GC completed, approximately {0}MB memory freed"
      gc_no_freed: "GC completed, no significant memory freed"
      suggest_gc: "Suggest executing garbage collection, current memory usage: {0}MB"
  qa:
    archive:
      init: "Initializing QA archiving service"
    archive_failed: "QA archiving failed"
    archived: "High-rated QA archived: rating={0}, Path={1}"
    document_feedback: "Document feedback {0} [{1}]: {2} - {3}"
    feedback_applied: "Feedback applied to document weight: {0}"
    feedback_pending: "Feedback pending review: {0}"
    find_failed: "Failed to find record: {0}"
    load_failed: "Failed to load QA records: {0}"
    marked_as_quality: "QA record marked as high-quality content: [{0}]"
    overall_rating_submitted: "Overall rating submitted {0} [{1}]: {2} stars"
    pending_failed: "Failed to retrieve pending review records"
    rating_applied: "Star rating applied to document weight: {0} ({1} stars -> Adjustment {2})"
    rating_pending: "Star rating pending review: {0} ({1} stars)"
    rating_submitted: "Star rating submitted {0} [{1}]: {2} - {3} stars (Weight adjustment: {4})"
    recent_failed: "Failed to retrieve recent records"
    record_notfound: "Record not found: {0}"
    record_save_failed: "Failed to save QA record"
    record_saved: "QA record saved: {0} - {1}"
    record_update_failed: "Failed to update QA record: {0}"
    record_updated: "QA record updated: {0}"
    records_dir: "QA records storage directory: {0}"
    records_dir_failed: "Failed to create QA records directory: {0}"
    stats_failed: "Failed to calculate QA statistics"
    user_feedback: "User feedback [{0}]: Rating={1}, Content={2}"
  kqa:
    answer_header: "\nğŸ’¡ Answer:"
    build_failed: "Knowledge base build failed: {0}"
    docs_dir_missing: "âš ï¸ Document directory does not exist: {0}"
    incremental_complete: "Incremental indexing completed"
    incremental_error: "Incremental indexing error"
    incremental_failed: "Incremental indexing failed: {0}"
    incremental_mode: "   ğŸ”„ Starting incremental knowledge base indexing..."
    init_done: "âœ… Knowledge base Q&A system initialized successfully!"
    init_failed: "âŒ Knowledge Q&A service initialization failed"
    init_kb: "Initializing knowledge base"
    init_start: "ğŸš€ Initializing knowledge Q&A service"
    kb_not_initialized: "Knowledge base not initialized"
    load_chunks_images_failed: "Failed to load chunks/images information"
    more_docs_available: "â„¹ï¸ {0} more related documents not included in this answer"
    rebuild_complete: "Knowledge base rebuilt successfully"
    rebuild_error: "Knowledge base rebuild error"
    rebuild_failed: "Knowledge base rebuild failed: {0}"
    rebuild_mode: "   ğŸš€ Starting full knowledge base rebuild..."
    recover_failed: "Knowledge base recovery failed"
    recover_kb: "Recovering knowledge base"
    reinit_complete: "Knowledge base reinitialized successfully"
    reinit_kb: "Reinitializing knowledge base"
    scanned_files_count: "ğŸ“‚ Filesystem scan completed, found {0} supported documents"
    scan_failed: "âŒ Filesystem scan failed"
    source_path: "   - Source path: {0}"
    sources_header: "\nğŸ“š Data Sources (Total {0} documents):"
    step: "\nğŸš€ Step {0}: {1}"
    storage_path: "   - Storage path: {0}"
    system_not_initialized: "Knowledge Q&A system not initialized"
    used_docs_count: "ğŸ“š Used {0} documents to generate this answer"
  rag:
    batch_indexed: "Batch indexed {0} documents"
    cache_hit: "Query result from cache: {0}"
    close_error: "Error while closing Local File RAG"
    closing: "Closing Local File RAG..."
    delete_failed: "Failed to delete document: {0}"
    deleted_count: "{0} documents deleted"
    deleting_all: "Deleting all documents..."
    doc_deleted: "Document deleted: {0}"
    doc_indexed: "Document indexed successfully: {0}"
    doc_updated: "Document updated: {0}"
    enable_cache: "Cache enabled: {0}"
    enable_compression: "Compression enabled: {0}"
    found_to_delete: "Found {0} documents to delete"
    init_done: "Local File RAG initialized successfully"
    load_content_failed: "Failed to load document content: {0}"
    loaded_content: "Document content loaded: {0}, Length: {1}"
    no_documents: "No documents to delete"
    optimized: "Index optimization completed"
    optimizing: "Optimizing index..."
    search_completed: "Search completed, Time taken: {0}ms, Found {1} results"
    simple_init: "Initializing simple RAG service..."
    simple_init_done: "Simple RAG service initialized successfully"
    storage: "Storage path: {0}"
  session:
    create: "Creating session: {0}"
  storage:
    ai_image_analyzer_init: "Initializing AI image analyzer: enabled={0}, model={1}"
    chunk_storage_init: "Initializing document chunk storage service, Path: {0}"
    document_image_extraction_init: "Initializing document image extraction service: AI analysis={0}"
    image_storage_init: "Initializing image storage service, Path: {0}"
  tika:
    active_image_strategy: "Active image strategy: {0}"
    detected_mime: "Detected MIME type: {0}"
    extract_image_metadata: "Extracting image metadata: {0}"
    include_image_placeholders: "Include image placeholders: {0}"
    init: "Initializing Apache Tika"
    max_content: "Max content length: {0}"
    ocr_disabled: "OCR disabled"
    office_done: "Office document processing completed"
    office_pptx: "Processing Office PPTX document"
    office_xlsx: "Processing Office XLSX document"
    parsed_file: "File parsed: {0}"
model:
  check: "ğŸ” Checking model files..."
  dir_and_file: "ğŸ“‚ Model directory and files are ready"
  found: "âœ… Model found: {0}"
  passed: "âœ… Model check passed"
  sep: "===================================================="
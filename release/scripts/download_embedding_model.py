#!/usr/bin/env python3
"""
ÂõΩ‰∫ßÂêëÈáèÂµåÂÖ•Ê®°Âûã‰∏ãËΩΩËÑöÊú¨
ÊîØÊåÅ BGE„ÄÅText2Vec Á≠âÂõΩ‰∫ßÊ®°Âûã

‰ΩøÁî®ÊñπÊ≥ïÔºö
    python scripts\download_embedding_model.py --model bge-m3
    python scripts\download_embedding_model.py --model bge-base-zh --mirror
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path

def install_package(package_name):
    """ÂÆâË£Ö Python ÂåÖ"""
    print(f"üì• ÂÆâË£Ö {package_name}...")
    try:
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--upgrade", package_name],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE
        )
        print(f"‚úÖ {package_name} ÂÆâË£ÖÊàêÂäü")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå {package_name} ÂÆâË£ÖÂ§±Ë¥•: {e.stderr.decode() if e.stderr else str(e)}")
        return False

def check_dependencies(use_mirror=False):
    """Ê£ÄÊü•Âπ∂Ëá™Âä®ÂÆâË£ÖÊâÄÊúâÂøÖÈúÄÁöÑ‰æùËµñ"""
    print("=" * 70)
    print("üì¶ Ê£ÄÊü•Âπ∂ÂÆâË£Ö‰æùËµñ...")
    print("=" * 70)

    required_packages = {
        "sentence_transformers": "sentence-transformers>=2.0.0",
        "torch": "torch>=2.0.0",
        "transformers": "transformers>=4.30.0",
        "optimum": "optimum[onnxruntime]>=1.14.0",
        "onnxruntime": "onnxruntime>=1.15.0",
        "onnxscript": "onnxscript>=0.1.0"
    }

    # Â¶ÇÊûú‰ΩøÁî®ÈïúÂÉèÔºåÊ∑ªÂä† modelscope
    if use_mirror:
        required_packages["modelscope"] = "modelscope>=1.0.0"

    installed_packages = []
    failed_packages = []

    for package_name, package_spec in required_packages.items():
        try:
            # Â∞ùËØïÂØºÂÖ•ÂåÖ
            __import__(package_name)
            print(f"‚úÖ {package_name} Â∑≤ÂÆâË£Ö")
            installed_packages.append(package_name)
        except ImportError:
            print(f"‚ö†Ô∏è  {package_name} Êú™ÂÆâË£ÖÔºåÂºÄÂßãÂÆâË£Ö...")
            if install_package(package_spec):
                installed_packages.append(package_name)
            else:
                failed_packages.append(package_name)

    print()
    if failed_packages:
        print(f"‚ùå ‰ª•‰∏ã‰æùËµñÂÆâË£ÖÂ§±Ë¥•: {', '.join(failed_packages)}")
        print("\nËØ∑ÊâãÂä®ÂÆâË£Ö:")
        print(f"pip install {' '.join([required_packages[p] for p in failed_packages])}")
        return False

    print(f"‚úÖ ÊâÄÊúâ‰æùËµñÂ∑≤Â∞±Áª™ ({len(installed_packages)}/{len(required_packages)})")
    print("=" * 70)
    print()
    return True

def download_model_huggingface(model_name, output_dir):
    """‰ªé Hugging Face ‰∏ãËΩΩÊ®°Âûã"""
    from sentence_transformers import SentenceTransformer

    print(f"üì• ‰ªé Hugging Face ‰∏ãËΩΩÊ®°Âûã: {model_name}")

    try:
        # ‰∏ãËΩΩÊ®°Âûã
        model = SentenceTransformer(model_name)

        # ‰øùÂ≠òÊ®°Âûã
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        model.save(str(output_path))

        print(f"‚úÖ Ê®°Âûã‰øùÂ≠òÂà∞: {output_path}")

        # ÊòæÁ§∫Ê®°Âûã‰ø°ÊÅØ
        print("\nüìä Ê®°Âûã‰ø°ÊÅØ:")
        print(f"  - Áª¥Â∫¶: {model.get_sentence_embedding_dimension()}")
        print(f"  - ÊúÄÂ§ßÈïøÂ∫¶: {model.max_seq_length}")

        # ÊµãËØï
        print("\nüß™ ÊµãËØïÊ®°Âûã...")
        test_text = "ËøôÊòØ‰∏Ä‰∏™ÊµãËØïÂè•Â≠ê"
        embedding = model.encode(test_text)
        print(f"‚úÖ Ê®°ÂûãÂ∑•‰ΩúÊ≠£Â∏∏")
        print(f"  ËæìÂÖ•: {test_text}")
        print(f"  ËæìÂá∫Áª¥Â∫¶: {len(embedding)}")

        return True

    except Exception as e:
        print(f"‚ùå ‰∏ãËΩΩÂ§±Ë¥•: {e}")
        return False

def download_model_modelscope(model_name, output_dir):
    """‰ªéÈ≠îÊê≠Á§æÂå∫‰∏ãËΩΩÊ®°Âûã"""
    try:
        from modelscope import snapshot_download
        from sentence_transformers import SentenceTransformer
    except ImportError as e:
        print(f"‚ùå Áº∫Â∞ë‰æùËµñ: {e}")
        print("ÂÆâË£Ö: pip install modelscope sentence-transformers")
        return False

    print(f"üì• ‰ªéÈ≠îÊê≠Á§æÂå∫‰∏ãËΩΩÊ®°Âûã: {model_name}")

    try:
        # 1. ‰ΩøÁî®È≠îÊê≠Á§æÂå∫‰∏ãËΩΩÊ®°ÂûãÂà∞‰∏¥Êó∂ÁõÆÂΩï
        temp_dir = snapshot_download(model_name)
        print(f"‚úÖ Ê®°Âûã‰∏ãËΩΩÂà∞ÁºìÂ≠ò: {temp_dir}")

        # 2. ‰ΩøÁî® sentence-transformers Âä†ËΩΩÂπ∂‰øùÂ≠òÂà∞ÁõÆÊ†áÁõÆÂΩï
        print(f"üì¶ ËΩ¨Êç¢Âπ∂‰øùÂ≠òÊ®°ÂûãÂà∞: {output_dir}")
        model = SentenceTransformer(temp_dir)

        # ‰øùÂ≠òÂà∞ÁõÆÊ†áÁõÆÂΩï
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        model.save(str(output_path))

        print(f"‚úÖ Ê®°Âûã‰øùÂ≠òÂà∞: {output_path}")

        # ÊòæÁ§∫Ê®°Âûã‰ø°ÊÅØ
        print("\nüìä Ê®°Âûã‰ø°ÊÅØ:")
        print(f"  - Áª¥Â∫¶: {model.get_sentence_embedding_dimension()}")
        print(f"  - ÊúÄÂ§ßÈïøÂ∫¶: {model.max_seq_length}")

        # ÊµãËØï
        print("\nüß™ ÊµãËØïÊ®°Âûã...")
        test_text = "ËøôÊòØ‰∏Ä‰∏™ÊµãËØïÂè•Â≠ê"
        embedding = model.encode(test_text)
        print(f"‚úÖ Ê®°ÂûãÂ∑•‰ΩúÊ≠£Â∏∏")
        print(f"  ËæìÂÖ•: {test_text}")
        print(f"  ËæìÂá∫Áª¥Â∫¶: {len(embedding)}")

        return True

    except Exception as e:
        print(f"‚ùå ‰∏ãËΩΩÂ§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()
        return False


def merge_onnx_external_data(source_onnx_path, target_onnx_path):
    """
    Â∞Ü ONNX Ê®°ÂûãÁöÑÂ§ñÈÉ®Êï∞ÊçÆÂêàÂπ∂Âà∞Âçï‰∏™Êñá‰ª∂‰∏≠

    Args:
        source_onnx_path: Ê∫ê ONNX Êñá‰ª∂Ë∑ØÂæÑÔºàÂ∏¶ÊúâÂ§ñÈÉ®Êï∞ÊçÆÔºâ
        target_onnx_path: ÁõÆÊ†á ONNX Êñá‰ª∂Ë∑ØÂæÑÔºàÂêàÂπ∂ÂêéÁöÑÂçïÊñá‰ª∂Ôºâ

    Returns:
        Path: ÊàêÂäüËøîÂõûÁõÆÊ†áÊñá‰ª∂Ë∑ØÂæÑÔºåÂ§±Ë¥•ËøîÂõû None
    """
    try:
        import onnx

        source_path = Path(source_onnx_path)
        target_path = Path(target_onnx_path)

        # Ê£ÄÊü•Â§ñÈÉ®Êï∞ÊçÆÂ§ßÂ∞è
        external_data_path = source_path.parent / "model.onnx_data"
        total_size_mb = source_path.stat().st_size / (1024 * 1024)

        if external_data_path.exists():
            external_size_mb = external_data_path.stat().st_size / (1024 * 1024)
            total_size_mb += external_size_mb
            print(f"  Â§ñÈÉ®Êï∞ÊçÆÂ§ßÂ∞è: {external_size_mb:.1f} MB")
            print(f"  ÊÄªÊ®°ÂûãÂ§ßÂ∞è: {total_size_mb:.1f} MB")

        # Protobuf Êúâ 2GB ÈôêÂà∂ÔºåË∂ÖËøá 1.9GB ÁöÑÊ®°Âûã‰∏çÂª∫ËÆÆÂêàÂπ∂
        if total_size_mb > 1900:
            print(f"  ‚ö†Ô∏è Ê®°ÂûãË∂ÖËøá 1.9GBÔºåÊó†Ê≥ïÂêàÂπ∂‰∏∫ÂçïÊñá‰ª∂ÔºàProtobuf 2GB ÈôêÂà∂Ôºâ")
            print(f"  Â∞Ü‰øùÁïôÂàÜÁ¶ªÁöÑ model.onnx Âíå model.onnx_data Êñá‰ª∂")
            return None

        print(f"  Âä†ËΩΩÊ®°Âûã: {source_path.name}")

        # Âä†ËΩΩÊ®°ÂûãÔºàÂåÖÊã¨Â§ñÈÉ®Êï∞ÊçÆÔºâ
        model = onnx.load(str(source_path), load_external_data=True)

        # ‰øùÂ≠ò‰∏∫Âçï‰∏™Êñá‰ª∂Ôºà‰∏ç‰ΩøÁî®Â§ñÈÉ®Êï∞ÊçÆÔºâ
        print(f"  ÂêàÂπ∂Âà∞ÂçïÊñá‰ª∂: {target_path.name}")

        # Á°Æ‰øùÁõÆÊ†áÁõÆÂΩïÂ≠òÂú®
        target_path.parent.mkdir(parents=True, exist_ok=True)

        # ‰øùÂ≠òÊ®°ÂûãÔºåÊâÄÊúâÊï∞ÊçÆÂÜÖËÅî
        onnx.save_model(
            model,
            str(target_path),
            save_as_external_data=False  # ‰∏ç‰ΩøÁî®Â§ñÈÉ®Êï∞ÊçÆÔºåÂÖ®ÈÉ®ÂÜÖËÅî
        )

        return target_path

    except ImportError:
        print("  ‚ö†Ô∏è ÈúÄË¶ÅÂÆâË£Ö onnx ÂåÖ: pip install onnx")
        return None
    except Exception as e:
        error_msg = str(e)
        if "2GB" in error_msg or "protobuf" in error_msg.lower():
            print(f"  ‚ö†Ô∏è Ê®°ÂûãÂ§™Â§ßÔºåÊó†Ê≥ïÂêàÂπ∂‰∏∫ÂçïÊñá‰ª∂ÔºàProtobuf 2GB ÈôêÂà∂Ôºâ")
        else:
            print(f"  ‚ö†Ô∏è ÂêàÂπ∂Â§±Ë¥•: {e}")
        return None


def convert_to_onnx(model_path):
    """
    Â∞Ü Sentence-Transformers Ê®°ÂûãËΩ¨Êç¢‰∏∫ ONNX Ê†ºÂºè

    Args:
        model_path: Ê®°ÂûãË∑ØÂæÑ

    Returns:
        bool: ËΩ¨Êç¢ÊòØÂê¶ÊàêÂäü
    """
    print("\nüîÑ ËΩ¨Êç¢‰∏∫ ONNX Ê†ºÂºè...")

    try:
        from sentence_transformers import SentenceTransformer
        import torch
        import shutil

        output_dir = str(Path(model_path).parent / (Path(model_path).name + "-onnx"))
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # ÊñπÊ≥ï1: ‰ΩøÁî® optimum ORTModelForFeatureExtractionÔºàÊúÄÂèØÈù†Ôºâ
        print("üí° ÊñπÊ≥ï1: Â∞ùËØï‰ΩøÁî® optimum ORTModelForFeatureExtraction...")
        ort_export_success = False

        try:
            from optimum.onnxruntime import ORTModelForFeatureExtraction

            ort_model = ORTModelForFeatureExtraction.from_pretrained(
                model_path,
                export=True
            )
            ort_model.save_pretrained(output_dir)
            print("‚úÖ ORTModelForFeatureExtraction ËΩ¨Êç¢ÊàêÂäü")
            ort_export_success = True

        except Exception as e:
            print(f"  ‚ö†Ô∏è ORTModelForFeatureExtraction Â§±Ë¥•: {str(e)[:150]}")

        # ÊñπÊ≥ï2: ‰ΩøÁî® optimum-cli
        if not ort_export_success:
            print("\nüí° ÊñπÊ≥ï2: Â∞ùËØï‰ΩøÁî® optimum-cli...")

            result = subprocess.run([
                sys.executable, "-m", "optimum.exporters.onnx",
                "--model", str(model_path),
                output_dir
            ], capture_output=True, text=True)

            if result.returncode == 0:
                print("‚úÖ optimum-cli ËΩ¨Êç¢ÊàêÂäü")
                ort_export_success = True
            else:
                print(f"  ‚ö†Ô∏è optimum-cli Â§±Ë¥•: {result.stderr[:200]}")

        # ÊñπÊ≥ï3: ‰ΩøÁî® torch.onnx.export
        if not ort_export_success:
            print("\nüí° ÊñπÊ≥ï3: ‰ΩøÁî® torch.onnx.export...")

            model = SentenceTransformer(str(model_path))

            if len(model) > 0 and hasattr(model[0], 'auto_model'):
                transformer_model = model[0].auto_model
                tokenizer = model[0].tokenizer

                # ÂàõÂª∫Á§∫‰æãËæìÂÖ•
                dummy_text = "This is a sample sentence"
                encoded = tokenizer(
                    dummy_text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )

                onnx_path = Path(output_dir) / "model.onnx"
                opset_versions = [11, 12, 13, 14]
                export_success = False

                transformer_model.eval()

                for opset in opset_versions:
                    try:
                        print(f"  Â∞ùËØï opset_version={opset}...")
                        with torch.no_grad():
                            torch.onnx.export(
                                transformer_model,
                                (encoded['input_ids'], encoded['attention_mask']),
                                str(onnx_path),
                                input_names=['input_ids', 'attention_mask'],
                                output_names=['last_hidden_state'],
                                dynamic_axes={
                                    'input_ids': {0: 'batch', 1: 'sequence'},
                                    'attention_mask': {0: 'batch', 1: 'sequence'},
                                    'last_hidden_state': {0: 'batch', 1: 'sequence'}
                                },
                                opset_version=opset,
                                do_constant_folding=True,
                                export_params=True
                            )

                        if onnx_path.exists():
                            size_mb = onnx_path.stat().st_size / (1024 * 1024)
                            if size_mb < 10:
                                print(f"  ‚ö†Ô∏è opset={opset}: Êñá‰ª∂Â§™Â∞è ({size_mb:.1f}MB)")
                                onnx_path.unlink()
                                continue

                        print(f"‚úÖ torch.onnx.export ËΩ¨Êç¢ÊàêÂäü (opset={opset})")
                        export_success = True
                        break
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è opset={opset} Â§±Ë¥•: {str(e)[:100]}")
                        if onnx_path.exists():
                            onnx_path.unlink()
                        continue

                if not export_success:
                    print("‚ùå ÊâÄÊúâËΩ¨Êç¢ÊñπÊ≥ïÈÉΩÂ§±Ë¥•")
                    return False

        # Â§çÂà∂ ONNX Êñá‰ª∂Âà∞ÂéüÁõÆÂΩï
        print("\nüìã Â§çÂà∂ ONNX Êñá‰ª∂Âà∞Ê®°ÂûãÁõÆÂΩï...")

        # ÈÄíÂΩíÊêúÁ¥¢ model.onnx Âíå model.onnx_data Êñá‰ª∂ÔºàÂèØËÉΩÂú®Â≠êÁõÆÂΩï‰∏≠Ôºâ
        output_path = Path(output_dir)
        onnx_files = list(output_path.rglob("model.onnx"))
        onnx_data_files = list(output_path.rglob("model.onnx_data"))

        print(f"  ÊêúÁ¥¢Âà∞ÁöÑ ONNX Êñá‰ª∂: {[str(f.relative_to(output_path)) for f in onnx_files]}")
        if onnx_data_files:
            print(f"  ÊêúÁ¥¢Âà∞ÁöÑÊùÉÈáçÊñá‰ª∂: {[str(f.relative_to(output_path)) for f in onnx_data_files]}")

        if onnx_files:
            # ‰ºòÂÖàÈÄâÊã©ÊúÄÂ§ßÁöÑ model.onnx Êñá‰ª∂ÔºàÊõ¥ÂèØËÉΩÊòØÂÆåÊï¥ÁöÑÔºâ
            onnx_file = max(onnx_files, key=lambda f: f.stat().st_size)
            onnx_size_mb = onnx_file.stat().st_size / (1024 * 1024)

            # Ê£ÄÊü•ÊòØÂê¶ÊúâÂ§ñÈÉ®Êï∞ÊçÆÊñá‰ª∂ÈúÄË¶ÅÂêàÂπ∂
            onnx_data = onnx_file.parent / "model.onnx_data"
            has_external_data = onnx_data.exists() or onnx_data_files

            if has_external_data:
                # Â¶ÇÊûúÊúâÂ§ñÈÉ®Êï∞ÊçÆÔºåÂÖàÂêàÂπ∂ÂÜçÂ§çÂà∂
                print("\nüîß ÂêàÂπ∂Â§ñÈÉ®Êï∞ÊçÆÂà∞ ONNX Êñá‰ª∂...")
                merged_onnx_path = merge_onnx_external_data(onnx_file, Path(model_path) / "model.onnx")
                if merged_onnx_path:
                    merged_size_mb = merged_onnx_path.stat().st_size / (1024 * 1024)
                    print(f"‚úÖ Â∑≤ÂêàÂπ∂: model.onnx ({merged_size_mb:.1f} MB) - ÂåÖÂê´ÊâÄÊúâÊùÉÈáçÊï∞ÊçÆ")

                    # Âà†Èô§ÊóßÁöÑ model.onnx_data Êñá‰ª∂ÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
                    old_data_file = Path(model_path) / "model.onnx_data"
                    if old_data_file.exists():
                        old_data_file.unlink()
                        print(f"üßπ Â∑≤Âà†Èô§ÊóßÁöÑ model.onnx_data Êñá‰ª∂")
                else:
                    # ÂêàÂπ∂Â§±Ë¥•ÔºåÂõûÈÄÄÂà∞Â§çÂà∂‰∏§‰∏™Êñá‰ª∂
                    print("‚ö†Ô∏è  ÂêàÂπ∂Â§±Ë¥•ÔºåÂ∞ÜÂàÜÂà´Â§çÂà∂ model.onnx Âíå model.onnx_data")
                    shutil.copy2(onnx_file, Path(model_path) / "model.onnx")
                    print(f"‚úÖ Â∑≤Â§çÂà∂: model.onnx ({onnx_size_mb:.1f} MB)")

                    if onnx_data.exists():
                        data_size_mb = onnx_data.stat().st_size / (1024 * 1024)
                        shutil.copy2(onnx_data, Path(model_path) / "model.onnx_data")
                        print(f"‚úÖ Â∑≤Â§çÂà∂: model.onnx_data ({data_size_mb:.1f} MB)")
                    elif onnx_data_files:
                        largest_data = max(onnx_data_files, key=lambda f: f.stat().st_size)
                        data_size_mb = largest_data.stat().st_size / (1024 * 1024)
                        shutil.copy2(largest_data, Path(model_path) / "model.onnx_data")
                        print(f"‚úÖ Â∑≤Â§çÂà∂: model.onnx_data ({data_size_mb:.1f} MB)")
            else:
                # Ê≤°ÊúâÂ§ñÈÉ®Êï∞ÊçÆÔºåÁõ¥Êé•Â§çÂà∂
                shutil.copy2(onnx_file, Path(model_path) / "model.onnx")
                print(f"‚úÖ Â∑≤Â§çÂà∂: model.onnx ({onnx_size_mb:.1f} MB)")

                if onnx_size_mb < 10:
                    print(f"‚ö†Ô∏è  Ë≠¶Âëä: model.onnx ‰ªÖ {onnx_size_mb:.1f}MBÔºåÊú™ÊâæÂà∞ model.onnx_data Êñá‰ª∂!")
                    print(f"   ËøôÂØπ‰∫éÂµåÂÖ•Ê®°ÂûãÊù•ËØ¥Â§™Â∞è‰∫ÜÔºåÊ®°ÂûãÂèØËÉΩ‰∏çÂÆåÊï¥")
        else:
            print("‚ùå Êú™ÊâæÂà∞ ONNX Êñá‰ª∂")
            return False

        # Ê∏ÖÁêÜ‰∏¥Êó∂ÁõÆÂΩï
        print("\nüßπ Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂...")
        try:
            shutil.rmtree(output_dir)
            print(f"‚úÖ Â∑≤Âà†Èô§‰∏¥Êó∂ÁõÆÂΩï: {Path(output_dir).name}")
        except Exception as e:
            print(f"‚ö†Ô∏è Ê∏ÖÁêÜ‰∏¥Êó∂ÁõÆÂΩïÂ§±Ë¥•: {e}")

        # È™åËØÅ ONNX Ê®°Âûã
        print("\n" + "=" * 60)
        print("üîç È™åËØÅ ONNX Ê®°ÂûãÂÆåÊï¥ÊÄß...")
        print("=" * 60)

        onnx_model_path = Path(model_path) / "model.onnx"
        onnx_data_path = Path(model_path) / "model.onnx_data"
        errors = []
        warnings = []

        # 1. Ê£ÄÊü• model.onnx Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
        if not onnx_model_path.exists():
            errors.append("ONNX Ê®°ÂûãÊñá‰ª∂ (model.onnx) ‰∏çÂ≠òÂú®")
            print("‚ùå model.onnx ‰∏çÂ≠òÂú®")
        else:
            file_size = onnx_model_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            print(f"‚úÖ model.onnx ({file_size_mb:.2f} MB)")

            if file_size < 1024:  # Â∞è‰∫é 1KB
                errors.append(f"model.onnx Êñá‰ª∂Â§™Â∞è ({file_size} bytes)ÔºåÂèØËÉΩÂ∑≤ÊçüÂùè")

        # 2. Ê£ÄÊü• model.onnx_data Êñá‰ª∂ÔºàÂ§ñÈÉ®ÊùÉÈáçÊï∞ÊçÆÔºâ
        if onnx_data_path.exists():
            data_size = onnx_data_path.stat().st_size
            data_size_mb = data_size / (1024 * 1024)
            print(f"‚úÖ model.onnx_data ({data_size_mb:.2f} MB)")
        else:
            # Ê£ÄÊü• model.onnx ÊòØÂê¶ÂºïÁî®‰∫ÜÂ§ñÈÉ®Êï∞ÊçÆÊñá‰ª∂
            if onnx_model_path.exists():
                file_size_mb = onnx_model_path.stat().st_size / (1024 * 1024)

                # Ê£ÄÊü•Êñá‰ª∂ÂÜÖÂÆπÊòØÂê¶ÂºïÁî®‰∫ÜÂ§ñÈÉ®Êï∞ÊçÆ
                has_external_ref = False
                try:
                    with open(onnx_model_path, 'rb') as f:
                        content = f.read(100000)  # ËØªÂèñÂâç100KBÊ£ÄÊü•
                        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂ§ñÈÉ®Êï∞ÊçÆÂºïÁî®
                        if b'model.onnx_data' in content or b'onnx_data' in content or b'external_data' in content:
                            has_external_ref = True
                except Exception as e:
                    warnings.append(f"Êó†Ê≥ïÊ£ÄÊü• model.onnx ÂÜÖÂÆπ: {e}")

                if has_external_ref:
                    errors.append(
                        "model.onnx ÂºïÁî®‰∫ÜÂ§ñÈÉ®Êï∞ÊçÆÊñá‰ª∂ model.onnx_dataÔºå‰ΩÜËØ•Êñá‰ª∂Áº∫Â§±ÔºÅ\n"
                        "   Ëøô‰ºöÂØºËá¥Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•ÔºàUnsupported model IR version Êàñ file not found ÈîôËØØÔºâ"
                    )
                    print("‚ùå model.onnx_data Áº∫Â§±Ôºàmodel.onnx ÈúÄË¶ÅÊ≠§Êñá‰ª∂ÔºÅÔºâ")
                elif file_size_mb < 10:
                    # Â∞è‰∫é 10MB ÁöÑÂµåÂÖ•Ê®°ÂûãÂá†‰πéËÇØÂÆöÊòØ‰∏çÂÆåÊï¥ÁöÑ
                    # BGE-base-zh Á∫¶ 400MB, BGE-m3 Á∫¶ 2GB
                    errors.append(
                        f"‚ùå model.onnx ‰ªÖ {file_size_mb:.2f}MBÔºåËøôÂØπ‰∫éÂµåÂÖ•Ê®°ÂûãÊù•ËØ¥Â§™Â∞è‰∫ÜÔºÅ\n"
                        f"   È¢ÑÊúüÂ§ßÂ∞è: BGE-base-zh ~400MB, BGE-m3 ~2GB\n"
                        f"   ÈóÆÈ¢ò: model.onnx_data ÊùÉÈáçÊñá‰ª∂Áº∫Â§±ÔºåÊ®°Âûã‰∏çÂÆåÊï¥"
                    )
                    print(f"‚ùå model.onnx ‰ªÖ {file_size_mb:.2f}MB - Ê®°Âûã‰∏çÂÆåÊï¥ÔºåÁº∫Â∞ëÊùÉÈáçÊï∞ÊçÆÔºÅ")
                elif file_size_mb < 100:
                    # 10-100MB ÁöÑÊ®°ÂûãÂèØËÉΩÊúâÈóÆÈ¢ò
                    warnings.append(
                        f"model.onnx ‰ªÖ {file_size_mb:.1f}MBÔºåÂèØËÉΩÁº∫Â∞ë model.onnx_data Êñá‰ª∂„ÄÇ\n"
                        "   Â¶ÇÊûúÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•ÔºåËØ∑Â∞ùËØïÈáçÊñ∞‰∏ãËΩΩ„ÄÇ"
                    )
                    print(f"‚ö†Ô∏è  model.onnx_data ‰∏çÂ≠òÂú®ÔºàÊ®°ÂûãÂèØËÉΩ‰∏çÂÆåÊï¥Ôºâ")
                else:
                    print(f"‚ÑπÔ∏è  model.onnx_data ‰∏çÂ≠òÂú®ÔºàÊùÉÈáçÂ∑≤ÂÜÖËÅîÂú® model.onnx ‰∏≠Ôºâ")

        # 3. Ê£ÄÊü•ÂÖ∂‰ªñÂøÖÈúÄÊñá‰ª∂
        required_files = ["tokenizer.json", "vocab.txt"]
        found_tokenizer = False
        for req_file in required_files:
            req_path = Path(model_path) / req_file
            if req_path.exists():
                found_tokenizer = True
                print(f"‚úÖ {req_file}")

        if not found_tokenizer:
            warnings.append("Êú™ÊâæÂà∞ tokenizer.json Êàñ vocab.txtÔºåtokenizer ÂèØËÉΩÊó†Ê≥ïÊ≠£Â∏∏Â∑•‰Ωú")
            print("‚ö†Ô∏è  Êú™ÊâæÂà∞ tokenizer Êñá‰ª∂")

        # 4. ‰ΩøÁî® ONNX Runtime È™åËØÅ
        if errors:
            print("\n‚ùå Ë∑≥Ëøá ONNX Runtime È™åËØÅÔºàÂ≠òÂú®‰∏•ÈáçÈîôËØØÔºâ")
        else:
            print("\nüß™ ‰ΩøÁî® ONNX Runtime Âä†ËΩΩÈ™åËØÅ...")
            try:
                import onnxruntime as ort

                sess_options = ort.SessionOptions()
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL

                session = ort.InferenceSession(
                    str(onnx_model_path),
                    sess_options=sess_options,
                    providers=['CPUExecutionProvider']
                )
                print("‚úÖ ONNX Runtime Âä†ËΩΩÊàêÂäü")

                print("\nüìã Ê®°ÂûãÁªìÊûÑ:")
                print("  ËæìÂÖ•:")
                for input_meta in session.get_inputs():
                    print(f"    - {input_meta.name}: {input_meta.shape}")
                print("  ËæìÂá∫:")
                for output_meta in session.get_outputs():
                    print(f"    - {output_meta.name}: {output_meta.shape}")

            except Exception as e:
                error_msg = str(e)
                if "model.onnx_data" in error_msg or "external data" in error_msg.lower() or "file_size" in error_msg.lower():
                    errors.append(
                        f"ONNX Runtime Âä†ËΩΩÂ§±Ë¥•: Áº∫Â§±Â§ñÈÉ®Êï∞ÊçÆÊñá‰ª∂ model.onnx_data\n"
                        f"   ÂéüÂßãÈîôËØØ: {error_msg[:200]}"
                    )
                elif "IR version" in error_msg:
                    errors.append(
                        f"ONNX Runtime ÁâàÊú¨‰∏çÂÖºÂÆπ: {error_msg[:200]}\n"
                        f"   Âª∫ËÆÆÂçáÁ∫ß ONNX Runtime: pip install --upgrade onnxruntime"
                    )
                else:
                    warnings.append(f"ONNX Runtime È™åËØÅÂ§±Ë¥•: {error_msg[:200]}")
                print(f"‚ö†Ô∏è  È™åËØÅË≠¶Âëä: {error_msg[:150]}")

        # 5. ËæìÂá∫È™åËØÅÁªìÊûú
        print("\n" + "=" * 60)
        if errors:
            print("‚ùå Ê®°ÂûãÈ™åËØÅÂ§±Ë¥•!")
            print("\nÈîôËØØÂàóË°®:")
            for i, error in enumerate(errors, 1):
                print(f"  {i}. {error}")
            print("\nüí° ‰øÆÂ§çÂª∫ËÆÆ:")
            print("  1. Âà†Èô§Ê®°ÂûãÁõÆÂΩïÔºåÈáçÊñ∞ËøêË°å‰∏ãËΩΩËÑöÊú¨")
            print("  2. Á°Æ‰øùÁΩëÁªúËøûÊé•Á®≥ÂÆöÔºåÁ£ÅÁõòÁ©∫Èó¥ÂÖÖË∂≥")
            print("  3. ‰ΩøÁî® --mirror ÂèÇÊï∞Â∞ùËØïÂõΩÂÜÖÈïúÂÉè")
            print("  4. Â¶ÇÊûúÈóÆÈ¢òÊåÅÁª≠ÔºåÂ∞ùËØïËæÉÂ∞èÁöÑÊ®°ÂûãÔºàÂ¶Ç bge-base-zhÔºâ")
            return False
        elif warnings:
            print("‚ö†Ô∏è  Ê®°ÂûãÈ™åËØÅÈÄöËøáÔºàÊúâË≠¶ÂëäÔºâ")
            print("\nË≠¶ÂëäÂàóË°®:")
            for i, warning in enumerate(warnings, 1):
                print(f"  {i}. {warning}")
            print("\nÊ®°ÂûãÂèØËÉΩÂèØÁî®Ôºå‰ΩÜÂ¶ÇÊûúÈÅáÂà∞ÈóÆÈ¢òËØ∑ÂèÇËÄÉ‰∏äËø∞Ë≠¶Âëä„ÄÇ")
            return True
        else:
            print("‚úÖ Ê®°ÂûãÈ™åËØÅÂÆåÂÖ®ÈÄöËøá!")
            return True

    except Exception as e:
        print(f"‚ùå ËΩ¨Êç¢Â§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(
        description="‰∏ãËΩΩÂõΩ‰∫ßÂêëÈáèÂµåÂÖ•Ê®°Âûã"
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        choices=["bge-m3", "bge-large-zh", "bge-base-zh", "text2vec-base", "text2vec-large"],
        help="ÈÄâÊã©Ê®°Âûã"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./models",
        help="ËæìÂá∫ÁõÆÂΩïÔºàÈªòËÆ§Ôºö./modelsÔºâ"
    )
    parser.add_argument(
        "--mirror",
        action="store_true",
        help="‰ΩøÁî®È≠îÊê≠Á§æÂå∫ÈïúÂÉèÔºàÂõΩÂÜÖÂø´Ôºâ"
    )
    parser.add_argument(
        "--convert-onnx",
        action="store_true",
        default=True,
        help="Ëá™Âä®ËΩ¨Êç¢‰∏∫ ONNX Ê†ºÂºèÔºàÈªòËÆ§ÂêØÁî®Ôºâ"
    )
    parser.add_argument(
        "--no-convert-onnx",
        dest="convert_onnx",
        action="store_false",
        help="‰∏çËΩ¨Êç¢‰∏∫ ONNX Ê†ºÂºè"
    )

    args = parser.parse_args()

    # Ê®°ÂûãÊò†Â∞Ñ
    model_map_hf = {
        "bge-m3": "BAAI/bge-m3",
        "bge-large-zh": "BAAI/bge-large-zh",
        "bge-base-zh": "BAAI/bge-base-zh-v1.5",
        "text2vec-base": "shibing624/text2vec-base-chinese",
        "text2vec-large": "GanymedeNil/text2vec-large-chinese"
    }

    model_map_ms = {
        "bge-m3": "Xorbits/bge-m3",
        "bge-large-zh": "AI-ModelScope/bge-large-zh",
        "bge-base-zh": "AI-ModelScope/bge-base-zh-v1.5",
        "text2vec-base": "damo/nlp_corom_sentence-embedding_chinese-base",
        "text2vec-large": "damo/nlp_corom_sentence-embedding_chinese-large"
    }

    print("=" * 70)
    print("üá®üá≥ ÂõΩ‰∫ßÂêëÈáèÂµåÂÖ•Ê®°Âûã‰∏ãËΩΩÂ∑•ÂÖ∑")
    print("=" * 70)
    print()

    # ËÆæÁΩÆÈïúÂÉè
    if args.mirror:
        print("üåè ‰ΩøÁî®È≠îÊê≠Á§æÂå∫ÈïúÂÉè...")
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

    # Ê£ÄÊü•Âπ∂ÂÆâË£Ö‰æùËµñ
    if not check_dependencies(use_mirror=args.mirror):
        sys.exit(1)

    # Á°ÆÂÆöËæìÂá∫Ë∑ØÂæÑ
    model_output = Path(args.output) / args.model

    # ‰∏ãËΩΩÊ®°Âûã
    if args.mirror:
        model_name = model_map_ms.get(args.model)
        success = download_model_modelscope(model_name, str(model_output))
    else:
        model_name = model_map_hf.get(args.model)
        success = download_model_huggingface(model_name, str(model_output))

    if success:
        # Ëá™Âä®ËΩ¨Êç¢‰∏∫ ONNXÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
        if args.convert_onnx:
            onnx_success = convert_to_onnx(str(model_output))
            if not onnx_success:
                print("\n‚ö†Ô∏è ONNX ËΩ¨Êç¢Â§±Ë¥•Ôºå‰ΩÜ PyTorch Ê®°ÂûãÂ∑≤‰∏ãËΩΩ")
                print("üí° ÂèØ‰ª•Á®çÂêéÊâãÂä®ËΩ¨Êç¢:")
                print(f"   python {sys.argv[0]} --model {args.model} --convert-onnx")

        print("\n" + "=" * 70)
        print("üéâ ÂÆåÊàêÔºÅ")
        print("=" * 70)
        print()
        print("üìù ‰∏ã‰∏ÄÊ≠•Ôºö")
        print("1. Êõ¥Êñ∞ application.yml ÈÖçÁΩÆ")
        print(f"   model:")
        print(f"     name: {args.model}")
        print(f"     path: {model_output}/model.onnx")
        print()
        print("2. ÈáçÂª∫ÂêëÈáèÁ¥¢Âºï")
        print("   ËÆøÈóÆ: http://localhost:8080")
        print("   ÁÇπÂáª: ÈáçÂª∫Á¥¢Âºï")
        print()
        print("3. ÊµãËØïÊ£ÄÁ¥¢ÊïàÊûú")
        print("   ÂØπÊØîÊñ∞ÊóßÊ®°ÂûãÁöÑÊ£ÄÁ¥¢ÂáÜÁ°ÆÁéá")
    else:
        print("\n‚ùå Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•")
        print("\nüí° ÊïÖÈöúÊéíÊü•:")
        print("1. Ê£ÄÊü•ÁΩëÁªúËøûÊé•")
        print("2. Â∞ùËØï‰ΩøÁî®ÈïúÂÉè: --mirror")
        print("3. ÊâãÂä®‰∏ãËΩΩ:")
        if args.mirror:
            print(f"   ËÆøÈóÆ: https://modelscope.cn/models/{model_map_ms.get(args.model)}")
        else:
            print(f"   ËÆøÈóÆ: https://huggingface.co/{model_map_hf.get(args.model)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
